--- a/drivers/net/ethernet/ibm/emac/core.c	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/core.c	2019-04-07 12:54:37.696842409 +0200
@@ -24,27 +24,19 @@
  *
  */
 
-#include <linux/module.h>
 #include <linux/sched.h>
 #include <linux/string.h>
 #include <linux/errno.h>
-#include <linux/delay.h>
 #include <linux/types.h>
 #include <linux/pci.h>
-#include <linux/etherdevice.h>
-#include <linux/skbuff.h>
 #include <linux/crc32.h>
 #include <linux/ethtool.h>
 #include <linux/mii.h>
 #include <linux/bitops.h>
 #include <linux/workqueue.h>
-#include <linux/of.h>
-#include <linux/of_address.h>
-#include <linux/of_irq.h>
-#include <linux/of_net.h>
-#include <linux/of_mdio.h>
-#include <linux/slab.h>
 
+#include <net/tcp.h>
+#include <net/pkt_sched.h>
 #include <asm/processor.h>
 #include <asm/io.h>
 #include <asm/dma.h>
@@ -76,18 +68,9 @@
 #define DRV_DESC        "PPC 4xx OCP EMAC driver"
 
 MODULE_DESCRIPTION(DRV_DESC);
-MODULE_AUTHOR
-    ("Eugene Surovegin <eugene.surovegin@zultys.com> or <ebs@ebshome.net>");
+MODULE_AUTHOR("Eugene Surovegin <eugene.surovegin@zultys.com> or <ebs@ebshome.net>");
 MODULE_LICENSE("GPL");
 
-/* minimum number of free TX descriptors required to wake up TX process */
-#define EMAC_TX_WAKEUP_THRESH		(NUM_TX_BUFF / 4)
-
-/* If packet size is less than this number, we allocate small skb and copy packet
- * contents into it instead of just sending original big skb up
- */
-#define EMAC_RX_COPY_THRESH		CONFIG_IBM_EMAC_RX_COPY_THRESHOLD
-
 /* Since multiple EMACs share MDIO lines in various ways, we need
  * to avoid re-using the same PHY ID in cases where the arch didn't
  * setup precise phy_map entries
@@ -96,6 +79,7 @@
  * EMAC "sets" (multiple ASICs containing several EMACs) though we can
  * probably require in that case to have explicit PHY IDs in the device-tree
  */
+//static int cnt1=0, cnt2=0, cnt3=0;
 static u32 busy_phy_map;
 static DEFINE_MUTEX(emac_phy_map_lock);
 
@@ -123,15 +107,13 @@
 #define EMAC_PROBE_DEP_TIMEOUT	(HZ * 5)
 
 /* I don't want to litter system log with timeout errors
- * when we have brain-damaged PHY.
- */
-static inline void emac_report_timeout_error(struct emac_instance *dev,
-					     const char *error)
-{
+ * when we have brain-damaged PHY */
+static inline void emac_report_timeout_error(struct emac_instance *dev, const char *error) {
 	if (emac_has_feature(dev, EMAC_FTR_440GX_PHY_CLK_FIX |
 				  EMAC_FTR_460EX_PHY_CLK_FIX |
 				  EMAC_FTR_440EP_PHY_CLK_FIX))
-		DBG(dev, "%s" NL, error);
+		//DBG(dev, "%s" NL, error);
+		printk(KERN_ERR "%pOF: %s\n", dev->ofdev->dev.of_node, error); //ECO
 	else if (net_ratelimit())
 		printk(KERN_ERR "%pOF: %s\n", dev->ofdev->dev.of_node, error);
 }
@@ -140,39 +122,33 @@
  * 440EP/440GR has more sane SDR0_MFR register implementation than 440GX,
  * which allows controlling each EMAC clock
  */
-static inline void emac_rx_clk_tx(struct emac_instance *dev)
-{
+static inline void emac_rx_clk_tx(struct emac_instance *dev) {
 #ifdef CONFIG_PPC_DCR_NATIVE
 	if (emac_has_feature(dev, EMAC_FTR_440EP_PHY_CLK_FIX))
-		dcri_clrset(SDR0, SDR0_MFR,
-			    0, SDR0_MFR_ECS >> dev->cell_index);
+		dcri_clrset(SDR0, SDR0_MFR, 0, SDR0_MFR_ECS >> dev->cell_index);
 #endif
 }
 
-static inline void emac_rx_clk_default(struct emac_instance *dev)
-{
+static inline void emac_rx_clk_default(struct emac_instance *dev) {
 #ifdef CONFIG_PPC_DCR_NATIVE
 	if (emac_has_feature(dev, EMAC_FTR_440EP_PHY_CLK_FIX))
-		dcri_clrset(SDR0, SDR0_MFR,
-			    SDR0_MFR_ECS >> dev->cell_index, 0);
+		dcri_clrset(SDR0, SDR0_MFR, SDR0_MFR_ECS >> dev->cell_index, 0);
 #endif
 }
 
 /* PHY polling intervals */
 #define PHY_POLL_LINK_ON	HZ
 #define PHY_POLL_LINK_OFF	(HZ / 5)
-
+//#define PHY_POLL_LINK_ON	HZ / 10
+//#define PHY_POLL_LINK_OFF	(HZ / 50)
 /* Graceful stop timeouts in us.
- * We should allow up to 1 frame time (full-duplex, ignoring collisions)
- */
+ * We should allow up to 1 frame time (full-duplex, ignoring collisions) */
 #define STOP_TIMEOUT_10		1230
 #define STOP_TIMEOUT_100	124
 #define STOP_TIMEOUT_1000	13
 #define STOP_TIMEOUT_1000_JUMBO	73
 
-static unsigned char default_mcast_addr[] = {
-	0x01, 0x80, 0xC2, 0x00, 0x00, 0x01
-};
+static unsigned char default_mcast_addr[] = { 0x01, 0x80, 0xC2, 0x00, 0x00, 0x01 };
 
 /* Please, keep in sync with struct ibm_emac_stats/ibm_emac_error_stats */
 static const char emac_stats_keys[EMAC_ETHTOOL_STATS_COUNT][ETH_GSTRING_LEN] = {
@@ -194,44 +170,34 @@
 };
 
 static irqreturn_t emac_irq(int irq, void *dev_instance);
+static irqreturn_t wol_irq(int irq, void *dev_instance);
 static void emac_clean_tx_ring(struct emac_instance *dev);
 static void __emac_set_multicast_list(struct emac_instance *dev);
 
-static inline int emac_phy_supports_gige(int phy_mode)
-{
-	return  phy_interface_mode_is_rgmii(phy_mode) ||
-		phy_mode == PHY_INTERFACE_MODE_GMII ||
-		phy_mode == PHY_INTERFACE_MODE_SGMII ||
-		phy_mode == PHY_INTERFACE_MODE_TBI ||
-		phy_mode == PHY_INTERFACE_MODE_RTBI;
+static inline int emac_phy_supports_gige(int phy_mode) {
+	return phy_interface_mode_is_rgmii(phy_mode) || phy_mode == PHY_INTERFACE_MODE_GMII ||
+		phy_mode == PHY_INTERFACE_MODE_SGMII || phy_mode == PHY_INTERFACE_MODE_TBI || phy_mode == PHY_INTERFACE_MODE_RTBI;
 }
 
-static inline int emac_phy_gpcs(int phy_mode)
-{
+static inline int emac_phy_gpcs(int phy_mode) {
 	return  phy_mode == PHY_INTERFACE_MODE_SGMII ||
-		phy_mode == PHY_INTERFACE_MODE_TBI ||
+		phy_mode == PHY_INTERFACE_MODE_TBI || 
 		phy_mode == PHY_INTERFACE_MODE_RTBI;
 }
 
-static inline void emac_tx_enable(struct emac_instance *dev)
-{
+static __always_inline void emac_tx_enable(struct emac_instance *dev) {
 	struct emac_regs __iomem *p = dev->emacp;
-	u32 r;
-
+	u32 r = in_be32(&p->mr0);
 	DBG(dev, "tx_enable" NL);
-
-	r = in_be32(&p->mr0);
-	if (!(r & EMAC_MR0_TXE))
-		out_be32(&p->mr0, r | EMAC_MR0_TXE);
+	if (!(r & EMAC_MR0_TXE)) out_be32(&p->mr0, r | EMAC_MR0_TXE);
 }
 
-static void emac_tx_disable(struct emac_instance *dev)
-{
+static void emac_tx_disable(struct emac_instance *dev) {
 	struct emac_regs __iomem *p = dev->emacp;
 	u32 r;
 
 	DBG(dev, "tx_disable" NL);
-
+ 
 	r = in_be32(&p->mr0);
 	if (r & EMAC_MR0_TXE) {
 		int n = dev->stop_timeout;
@@ -245,57 +211,68 @@
 	}
 }
 
-static void emac_rx_enable(struct emac_instance *dev)
-{
+#ifdef CONFIG_IBM_EMAC_MASK_CEXT
+
+/* some code duplication here to avoid function calls */
+static inline void emac_start_idlemode(struct emac_instance *dev) {
+	u32 perclk;
+	//printk("ibmnewemac: start_idle\n");
+	DBG(dev, "start_idlemode" NL);
+
+	//emac_spin_delay(TX_FIFO_SYNC_USEC);	/* Wait for TX FIFO to Sync */
+	perclk = mfdcri(SDR0, SDR0_PERCLK);		/* Disable Ethernet Clock */
+	mtdcri(SDR0, SDR0_PERCLK, perclk | 0x88000000);
+	perclk = mfdcri(SDR0, SDR0_PERCLK);		/* Write0 to set rising clock edge next time*/
+	mtdcri(SDR0, SDR0_PERCLK, perclk & 0x7fffffff); 
+	//perclk = mfdcri(SDR0, SDR0_PERCLK);
+	//printk("%s:%d - Ethernet TX Clock Disabled perclk=0x%08lx\n", __FUNCTION__, __LINE__, perclk);
+}
+
+static inline void emac_exit_idlemode(struct emac_instance *dev) {
+	u32 perclk;
+	DBG(dev, "exit_idlemode" NL);
+
+	perclk = mfdcri(SDR0, SDR0_PERCLK);		/* Enable Ethernet Clock */
+	mtdcri(SDR0, SDR0_PERCLK, (perclk & 0xF7ffffff) | 0x80000000);
+	perclk = mfdcri(SDR0, SDR0_PERCLK);
+	mtdcri(SDR0, SDR0_PERCLK, perclk & 0x7fffffff);	/* Write0 to set rising clock edge next time*/
+	//perclk = mfdcri(SDR0, SDR0_PERCLK);
+	//printk("%s:%d - Ethernet TX Clock Enabled perclk=0x%08lx\n", __FUNCTION__, __LINE__, perclk);
+}
+#endif
+
+inline void emac_rx_enable(struct emac_instance *dev) {
 	struct emac_regs __iomem *p = dev->emacp;
-	u32 r;
+	u32 r = in_be32(&p->mr0);
 
-	if (unlikely(test_bit(MAL_COMMAC_RX_STOPPED, &dev->commac.flags)))
-		goto out;
+	if (unlikely(test_bit(MAL_COMMAC_RX_STOPPED, &dev->commac.flags))) return;
 
 	DBG(dev, "rx_enable" NL);
-
-	r = in_be32(&p->mr0);
 	if (!(r & EMAC_MR0_RXE)) {
-		if (unlikely(!(r & EMAC_MR0_RXI))) {
-			/* Wait if previous async disable is still in progress */
+		if (unlikely(!(r & EMAC_MR0_RXI))) { // Wait if previous async disable is still in progress
 			int n = dev->stop_timeout;
-			while (!(r = in_be32(&p->mr0) & EMAC_MR0_RXI) && n) {
-				udelay(1);
-				--n;
-			}
-			if (unlikely(!n))
-				emac_report_timeout_error(dev,
-							  "RX disable timeout");
+			while (!(r = in_be32(&p->mr0) & EMAC_MR0_RXI) && n--) udelay(1);
+			if (unlikely(!n)) emac_report_timeout_error(dev, "RX disable timeout");
 		}
 		out_be32(&p->mr0, r | EMAC_MR0_RXE);
 	}
- out:
-	;
 }
+EXPORT_SYMBOL_GPL(emac_rx_enable);
 
-static void emac_rx_disable(struct emac_instance *dev)
-{
+static __always_inline void emac_rx_disable(struct emac_instance *dev) {
 	struct emac_regs __iomem *p = dev->emacp;
-	u32 r;
+	u32 r = in_be32(&p->mr0);
 
 	DBG(dev, "rx_disable" NL);
-
-	r = in_be32(&p->mr0);
 	if (r & EMAC_MR0_RXE) {
 		int n = dev->stop_timeout;
 		out_be32(&p->mr0, r & ~EMAC_MR0_RXE);
-		while (!(in_be32(&p->mr0) & EMAC_MR0_RXI) && n) {
-			udelay(1);
-			--n;
-		}
-		if (unlikely(!n))
-			emac_report_timeout_error(dev, "RX disable timeout");
+		while (!(in_be32(&p->mr0) & EMAC_MR0_RXI) && n--) udelay(1);
+		if (unlikely(!n)) emac_report_timeout_error(dev, "RX disable timeout");
 	}
 }
 
-static inline void emac_netif_stop(struct emac_instance *dev)
-{
+static inline void emac_netif_stop(struct emac_instance *dev) {
 	netif_tx_lock_bh(dev->ndev);
 	netif_addr_lock(dev->ndev);
 	dev->no_mcast = 1;
@@ -306,8 +283,7 @@
 	netif_tx_disable(dev->ndev);
 }
 
-static inline void emac_netif_start(struct emac_instance *dev)
-{
+static inline void emac_netif_start(struct emac_instance *dev) {
 	netif_tx_lock_bh(dev->ndev);
 	netif_addr_lock(dev->ndev);
 	dev->no_mcast = 0;
@@ -326,20 +302,15 @@
 	mal_poll_enable(dev->mal, &dev->commac);
 }
 
-static inline void emac_rx_disable_async(struct emac_instance *dev)
-{
+static inline void emac_rx_disable_async(struct emac_instance *dev) {
 	struct emac_regs __iomem *p = dev->emacp;
-	u32 r;
-
+	u32 r = in_be32(&p->mr0);
 	DBG(dev, "rx_disable_async" NL);
 
-	r = in_be32(&p->mr0);
-	if (r & EMAC_MR0_RXE)
-		out_be32(&p->mr0, r & ~EMAC_MR0_RXE);
+	if (r & EMAC_MR0_RXE) out_be32(&p->mr0, r & ~EMAC_MR0_RXE);
 }
 
-static int emac_reset(struct emac_instance *dev)
-{
+static int emac_reset(struct emac_instance *dev) {
 	struct emac_regs __iomem *p = dev->emacp;
 	int n = 20;
 	bool __maybe_unused try_internal_clock = false;
@@ -347,9 +318,14 @@
 	DBG(dev, "reset" NL);
 
 	if (!dev->reset_failed) {
-		/* 40x erratum suggests stopping RX channel before reset,
-		 * we stop TX as well
-		 */
+#ifdef CONFIG_IBM_EMAC_MASK_CEXT
+	if (atomic_read(&dev->mask_cext_enable))
+		if (atomic_read(&dev->idle_mode)) {
+			emac_exit_idlemode(dev);
+			atomic_set(&dev->idle_mode, 0);
+		}
+#endif
+		/* 40x erratum suggests stopping RX channel before reset, we stop TX as well */
 		emac_rx_disable(dev);
 		emac_tx_disable(dev);
 	}
@@ -374,22 +350,18 @@
 	 * the first reset fails.
 	 */
 	if (emac_has_feature(dev, EMAC_FTR_460EX_PHY_CLK_FIX)) {
-		if (try_internal_clock || (dev->phy_address == 0xffffffff &&
-					   dev->phy_map == 0xffffffff)) {
+		if (try_internal_clock || (dev->phy_address == 0xffffffff && dev->phy_map == 0xffffffff))
 			/* No PHY: select internal loop clock before reset */
-			dcri_clrset(SDR0, SDR0_ETH_CFG,
-				    0, SDR0_ETH_CFG_ECS << dev->cell_index);
-		} else {
+			dcri_clrset(SDR0, SDR0_ETH_CFG, 0, SDR0_ETH_CFG_ECS << dev->cell_index);
+		else
 			/* PHY present: select external clock before reset */
-			dcri_clrset(SDR0, SDR0_ETH_CFG,
-				    SDR0_ETH_CFG_ECS << dev->cell_index, 0);
-		}
+			dcri_clrset(SDR0, SDR0_ETH_CFG, SDR0_ETH_CFG_ECS << dev->cell_index, 0);
 	}
 #endif
 
-	out_be32(&p->mr0, EMAC_MR0_SRST);
+	out_be32(&p->mr0, EMAC_MR0_SRST);	// Soft Reset
 	while ((in_be32(&p->mr0) & EMAC_MR0_SRST) && n)
-		--n;
+		--n;	// Poll until 0
 
 #ifdef CONFIG_PPC_DCR_NATIVE
 	if (emac_has_feature(dev, EMAC_FTR_460EX_PHY_CLK_FIX)) {
@@ -400,11 +372,9 @@
 			goto do_retry;
 		}
 
-		if (try_internal_clock || (dev->phy_address == 0xffffffff &&
-					   dev->phy_map == 0xffffffff)) {
+		if (try_internal_clock || (dev->phy_address == 0xffffffff && dev->phy_map == 0xffffffff)) {
 			/* No PHY: restore external clock source after reset */
-			dcri_clrset(SDR0, SDR0_ETH_CFG,
-				    SDR0_ETH_CFG_ECS << dev->cell_index, 0);
+			dcri_clrset(SDR0, SDR0_ETH_CFG, SDR0_ETH_CFG_ECS << dev->cell_index, 0);
 		}
 	}
 #endif
@@ -419,164 +389,106 @@
 	}
 }
 
-static void emac_hash_mc(struct emac_instance *dev)
-{
+static void emac_hash_mc(struct emac_instance *dev) {
 	const int regs = EMAC_XAHT_REGS(dev);
-	u32 *gaht_base = emac_gaht_base(dev);
-	u32 gaht_temp[regs];
+	u32 *gaht_base = emac_gaht_base(dev), gaht_temp[EMAC_XAHT_MAX_REGS];
 	struct netdev_hw_addr *ha;
 	int i;
 
 	DBG(dev, "hash_mc %d" NL, netdev_mc_count(dev->ndev));
-
 	memset(gaht_temp, 0, sizeof (gaht_temp));
-
 	netdev_for_each_mc_addr(ha, dev->ndev) {
-		int slot, reg, mask;
+		int slot, reg;
 		DBG2(dev, "mc %pM" NL, ha->addr);
-
-		slot = EMAC_XAHT_CRC_TO_SLOT(dev,
-					     ether_crc(ETH_ALEN, ha->addr));
+		slot = EMAC_XAHT_CRC_TO_SLOT(dev, ether_crc(ETH_ALEN, ha->addr));
 		reg = EMAC_XAHT_SLOT_TO_REG(dev, slot);
-		mask = EMAC_XAHT_SLOT_TO_MASK(dev, slot);
-
-		gaht_temp[reg] |= mask;
+		gaht_temp[reg] |= EMAC_XAHT_SLOT_TO_MASK(dev, slot);
 	}
-
-	for (i = 0; i < regs; i++)
-		out_be32(gaht_base + i, gaht_temp[i]);
+	for (i = 0; i < regs; i++) out_be32(gaht_base + i, gaht_temp[i]);
 }
 
-static inline u32 emac_iff2rmr(struct net_device *ndev)
-{
+static inline u32 emac_iff2rmr(struct net_device *ndev) {
 	struct emac_instance *dev = netdev_priv(ndev);
-	u32 r;
+	u32 r = EMAC_RMR_SP | EMAC_RMR_SFCS | EMAC_RMR_IAE | EMAC_RMR_BAE;	// EMAC_RMR_RFP (Allow Receive Packets with a FCS Error) (ECO)
 
-	r = EMAC_RMR_SP | EMAC_RMR_SFCS | EMAC_RMR_IAE | EMAC_RMR_BAE;
+	if (likely(emac_has_feature(dev, EMAC_FTR_EMAC4))) r |= EMAC4_RMR_BASE;
+	else r |= EMAC_RMR_BASE;
 
-	if (emac_has_feature(dev, EMAC_FTR_EMAC4))
-	    r |= EMAC4_RMR_BASE;
-	else
-	    r |= EMAC_RMR_BASE;
-
-	if (ndev->flags & IFF_PROMISC)
-		r |= EMAC_RMR_PME;
-	else if (ndev->flags & IFF_ALLMULTI ||
-			 (netdev_mc_count(ndev) > EMAC_XAHT_SLOTS(dev)))
+	if (ndev->flags & IFF_PROMISC) r |= EMAC_RMR_PME;
+	else if (ndev->flags & IFF_ALLMULTI || (netdev_mc_count(ndev) > EMAC_XAHT_SLOTS(dev)))
 		r |= EMAC_RMR_PMME;
-	else if (!netdev_mc_empty(ndev))
-		r |= EMAC_RMR_MAE;
+	else if (!netdev_mc_empty(ndev)) r |= EMAC_RMR_MAE;
 
-	if (emac_has_feature(dev, EMAC_APM821XX_REQ_JUMBO_FRAME_SIZE)) {
+	/* When Jumbo Frame is not enabled, MJS field has no effect. So setting MJS when Jumbo Frame
+	 * is disabled should not cause any issue */
+	if (emac_has_feature(dev, EMAC_APM821XX_REQ_JUMBO_FRAME_SIZE)) { //ECO
 		r &= ~EMAC4_RMR_MJS_MASK;
 		r |= EMAC4_RMR_MJS(ndev->mtu);
+		DBG(dev, "emac_iff2rmr: EMAC_RMR = 0x%08x" NL, r);
 	}
-
 	return r;
 }
 
-static u32 __emac_calc_base_mr1(struct emac_instance *dev, int tx_size, int rx_size)
-{
+static u32 __emac_calc_base_mr1(struct emac_instance *dev, int tx_size, int rx_size) {
 	u32 ret = EMAC_MR1_VLE | EMAC_MR1_IST | EMAC_MR1_TR0_MULT;
 
 	DBG2(dev, "__emac_calc_base_mr1" NL);
-
 	switch(tx_size) {
-	case 2048:
-		ret |= EMAC_MR1_TFS_2K;
-		break;
-	default:
-		printk(KERN_WARNING "%s: Unknown Tx FIFO size %d\n",
-		       dev->ndev->name, tx_size);
+		case 2048: ret |= EMAC_MR1_TFS_2K; break;
+		default:
+			printk(KERN_WARNING "%s: Unknown Tx FIFO size %d\n", dev->ndev->name, tx_size);
 	}
 
 	switch(rx_size) {
-	case 16384:
-		ret |= EMAC_MR1_RFS_16K;
-		break;
-	case 4096:
-		ret |= EMAC_MR1_RFS_4K;
-		break;
-	default:
-		printk(KERN_WARNING "%s: Unknown Rx FIFO size %d\n",
-		       dev->ndev->name, rx_size);
+		case 16384:	ret |= EMAC_MR1_RFS_16K; break;
+		case 4096:	ret |= EMAC_MR1_RFS_4K; break;
+		default:
+			printk(KERN_WARNING "%s: Unknown Rx FIFO size %d\n", dev->ndev->name, rx_size);
 	}
-
 	return ret;
 }
 
-static u32 __emac4_calc_base_mr1(struct emac_instance *dev, int tx_size, int rx_size)
-{
-	u32 ret = EMAC_MR1_VLE | EMAC_MR1_IST | EMAC4_MR1_TR |
-		EMAC4_MR1_OBCI(dev->opb_bus_freq / 1000000);
+static u32 __emac4_calc_base_mr1(struct emac_instance *dev, int tx_size, int rx_size) {
+	u32 ret = EMAC_MR1_VLE| EMAC_MR1_IST| EMAC4_MR1_TR| EMAC4_MR1_OBCI(dev->opb_bus_freq / 1000000);
 
 	DBG2(dev, "__emac4_calc_base_mr1" NL);
-
 	switch(tx_size) {
-	case 16384:
-		ret |= EMAC4_MR1_TFS_16K;
-		break;
-	case 8192:
-		ret |= EMAC4_MR1_TFS_8K;
-		break;
-	case 4096:
-		ret |= EMAC4_MR1_TFS_4K;
-		break;
-	case 2048:
-		ret |= EMAC4_MR1_TFS_2K;
-		break;
-	default:
-		printk(KERN_WARNING "%s: Unknown Tx FIFO size %d\n",
-		       dev->ndev->name, tx_size);
+		case 16384:	ret |= EMAC4_MR1_TFS_16K; break;
+		case 8192:	ret |= EMAC4_MR1_TFS_8K; break;
+		case 4096:	ret |= EMAC4_MR1_TFS_4K; break;
+		case 2048:	ret |= EMAC4_MR1_TFS_2K; break;
+		default:
+			printk(KERN_WARNING "%s: Unknown Tx FIFO size %d\n", dev->ndev->name, tx_size);
 	}
 
 	switch(rx_size) {
-	case 16384:
-		ret |= EMAC4_MR1_RFS_16K;
-		break;
-	case 8192:
-		ret |= EMAC4_MR1_RFS_8K;
-		break;
-	case 4096:
-		ret |= EMAC4_MR1_RFS_4K;
-		break;
-	case 2048:
-		ret |= EMAC4_MR1_RFS_2K;
-		break;
-	default:
-		printk(KERN_WARNING "%s: Unknown Rx FIFO size %d\n",
-		       dev->ndev->name, rx_size);
+		case 16384:	ret |= EMAC4_MR1_RFS_16K; break;
+		case 8192:	ret |= EMAC4_MR1_RFS_8K; break;
+		case 4096:	ret |= EMAC4_MR1_RFS_4K; break;
+		case 2048:	ret |= EMAC4_MR1_RFS_2K; break;
+		default:
+			printk(KERN_WARNING "%s: Unknown Rx FIFO size %d\n", dev->ndev->name, rx_size);
 	}
-
 	return ret;
 }
 
-static u32 emac_calc_base_mr1(struct emac_instance *dev, int tx_size, int rx_size)
-{
+static inline u32 emac_calc_base_mr1(struct emac_instance *dev, int tx_size, int rx_size) {
 	return emac_has_feature(dev, EMAC_FTR_EMAC4) ?
 		__emac4_calc_base_mr1(dev, tx_size, rx_size) :
 		__emac_calc_base_mr1(dev, tx_size, rx_size);
 }
 
-static inline u32 emac_calc_trtr(struct emac_instance *dev, unsigned int size)
-{
-	if (emac_has_feature(dev, EMAC_FTR_EMAC4))
-		return ((size >> 6) - 1) << EMAC_TRTR_SHIFT_EMAC4;
-	else
-		return ((size >> 6) - 1) << EMAC_TRTR_SHIFT;
+static inline u32 emac_calc_trtr(struct emac_instance *dev, unsigned int size) {
+	return (likely(emac_has_feature(dev, EMAC_FTR_EMAC4))) ? 
+		((size >> 6) -1) << EMAC_TRTR_SHIFT_EMAC4 : ((size >> 6) -1) << EMAC_TRTR_SHIFT;
 }
 
-static inline u32 emac_calc_rwmr(struct emac_instance *dev,
-				 unsigned int low, unsigned int high)
-{
-	if (emac_has_feature(dev, EMAC_FTR_EMAC4))
-		return (low << 22) | ( (high & 0x3ff) << 6);
-	else
-		return (low << 23) | ( (high & 0x1ff) << 7);
+static inline u32 emac_calc_rwmr(struct emac_instance *dev, u32 low, u32 high) {
+	return (likely(emac_has_feature(dev, EMAC_FTR_EMAC4))) ? 
+		(low << 22) | ( (high & 0x3ff) << 6) : (low << 23) | ( (high & 0x1ff) << 7);
 }
 
-static int emac_configure(struct emac_instance *dev)
-{
+static int emac_configure(struct emac_instance *dev) {
 	struct emac_regs __iomem *p = dev->emacp;
 	struct net_device *ndev = dev->ndev;
 	int tx_size, rx_size, link = netif_carrier_ok(dev->ndev);
@@ -585,14 +497,11 @@
 	DBG(dev, "configure" NL);
 
 	if (!link) {
-		out_be32(&p->mr1, in_be32(&p->mr1)
-			 | EMAC_MR1_FDE | EMAC_MR1_ILE);
+		out_be32(&p->mr1, in_be32(&p->mr1) | EMAC_MR1_FDE | EMAC_MR1_ILE);
 		udelay(100);
-	} else if (emac_reset(dev) < 0)
-		return -ETIMEDOUT;
+	} else if (emac_reset(dev) < 0)	return -ETIMEDOUT;
 
-	if (emac_has_feature(dev, EMAC_FTR_HAS_TAH))
-		tah_reset(dev->tah_dev);
+	if (emac_has_feature(dev, EMAC_FTR_HAS_TAH)) tah_reset(dev->tah_dev);
 
 	DBG(dev, " link = %d duplex = %d, pause = %d, asym_pause = %d\n",
 	    link, dev->phy.duplex, dev->phy.pause, dev->phy.asym_pause);
@@ -602,12 +511,16 @@
 	rx_size = dev->rx_fifo_size;
 
 	/* No link, force loopback */
-	if (!link)
-		mr1 = EMAC_MR1_FDE | EMAC_MR1_ILE;
+	if (!link) mr1 = EMAC_MR1_FDE | EMAC_MR1_ILE;
 
 	/* Check for full duplex */
 	else if (dev->phy.duplex == DUPLEX_FULL)
-		mr1 |= EMAC_MR1_FDE | EMAC_MR1_MWSW_001;
+
+#if !defined(CONFIG_IBM_EMAC_MASK_CEXT)
+		mr1 |= EMAC_MR1_FDE | EMAC_MR1_MWSW_001;  // EMAC4_MR1_MWSW_001 is unstable
+#else
+		mr1 |= EMAC_MR1_FDE;
+#endif
 
 	/* Adjust fifo sizes, mr1 and timeouts based on link speed */
 	dev->stop_timeout = STOP_TIMEOUT_10;
@@ -630,37 +543,29 @@
 		rx_size = dev->rx_fifo_size_gige;
 
 		if (dev->ndev->mtu > ETH_DATA_LEN) {
-			if (emac_has_feature(dev, EMAC_FTR_EMAC4))
-				mr1 |= EMAC4_MR1_JPSM;
-			else
-				mr1 |= EMAC_MR1_JPSM;
+			if (likely(emac_has_feature(dev, EMAC_FTR_EMAC4))) mr1 |= EMAC4_MR1_JPSM;
+			else mr1 |= EMAC_MR1_JPSM;
 			dev->stop_timeout = STOP_TIMEOUT_1000_JUMBO;
-		} else
-			dev->stop_timeout = STOP_TIMEOUT_1000;
+		} else dev->stop_timeout = STOP_TIMEOUT_1000;
 		break;
 	case SPEED_100:
 		mr1 |= EMAC_MR1_MF_100;
 		dev->stop_timeout = STOP_TIMEOUT_100;
 		break;
-	default: /* make gcc happy */
-		break;
+	default: break;
 	}
 
 	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII))
-		rgmii_set_speed(dev->rgmii_dev, dev->rgmii_port,
-				dev->phy.speed);
+		rgmii_set_speed(dev->rgmii_dev, dev->rgmii_port, dev->phy.speed);
 	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII))
 		zmii_set_speed(dev->zmii_dev, dev->zmii_port, dev->phy.speed);
 
 	/* on 40x erratum forces us to NOT use integrated flow control,
-	 * let's hope it works on 44x ;)
-	 */
+	 * let's hope it works on 44x ;) */
 	if (!emac_has_feature(dev, EMAC_FTR_NO_FLOW_CONTROL_40x) &&
 	    dev->phy.duplex == DUPLEX_FULL) {
-		if (dev->phy.pause)
-			mr1 |= EMAC_MR1_EIFC | EMAC_MR1_APP;
-		else if (dev->phy.asym_pause)
-			mr1 |= EMAC_MR1_APP;
+ 		if (dev->phy.pause)	mr1 |= EMAC_MR1_EIFC | EMAC_MR1_APP;
+		else if (dev->phy.asym_pause) mr1 |= EMAC_MR1_APP;
 	}
 
 	/* Add base settings & fifo sizes & program MR1 */
@@ -669,25 +574,23 @@
 
 	/* Set individual MAC address */
 	out_be32(&p->iahr, (ndev->dev_addr[0] << 8) | ndev->dev_addr[1]);
-	out_be32(&p->ialr, (ndev->dev_addr[2] << 24) |
-		 (ndev->dev_addr[3] << 16) | (ndev->dev_addr[4] << 8) |
-		 ndev->dev_addr[5]);
+	out_be32(&p->ialr, (ndev->dev_addr[2] << 24) | (ndev->dev_addr[3] << 16) |
+		 (ndev->dev_addr[4] << 8) | ndev->dev_addr[5]);
 
 	/* VLAN Tag Protocol ID */
 	out_be32(&p->vtpid, 0x8100);
 
 	/* Receive mode register */
 	r = emac_iff2rmr(ndev);
-	if (r & EMAC_RMR_MAE)
-		emac_hash_mc(dev);
+	if (r & EMAC_RMR_MAE) emac_hash_mc(dev);
 	out_be32(&p->rmr, r);
 
-	/* FIFOs thresholds */
+	/* FIFOs thresholds, 2K TX FIFO, 16K RX FIFO */
 	if (emac_has_feature(dev, EMAC_FTR_EMAC4))
 		r = EMAC4_TMR1((dev->mal_burst_size / dev->fifo_entry_size) + 1,
-			       tx_size / 2 / dev->fifo_entry_size);
-	else
-		r = EMAC_TMR1((dev->mal_burst_size / dev->fifo_entry_size) + 1,
+					tx_size / 4 * 3 / dev->fifo_entry_size);
+			       //tx_size / 2 / dev->fifo_entry_size);
+	else r = EMAC_TMR1((dev->mal_burst_size / dev->fifo_entry_size) + 1,
 			      tx_size / 2 / dev->fifo_entry_size);
 	out_be32(&p->tmr1, r);
 	out_be32(&p->trtr, emac_calc_trtr(dev, tx_size / 2));
@@ -708,11 +611,13 @@
 	   ----------
 	   3187 bytes
 
-	   I chose to set high-water mark to RX_FIFO_SIZE / 4 (1024 bytes)
+	   The original choice to set high-water mark to RX_FIFO_SIZE / 4 (1024 bytes)
 	   low-water mark  to RX_FIFO_SIZE / 8 (512 bytes)
-	 */
-	r = emac_calc_rwmr(dev, rx_size / 8 / dev->fifo_entry_size,
-			   rx_size / 4 / dev->fifo_entry_size);
+	   I believe a larger size is beneficial e.g. 3/4th given amount of overruns  */
+	//r = emac_calc_rwmr(dev, rx_size / 8 / dev->fifo_entry_size,
+	//		   rx_size / 4 / dev->fifo_entry_size);
+	r = emac_calc_rwmr(dev, rx_size / 8 / dev->fifo_entry_size, 
+			   rx_size / 4 / dev->fifo_entry_size * 3);
 	out_be32(&p->rwmr, r);
 
 	/* Set PAUSE timer to the maximum */
@@ -723,23 +628,19 @@
 		EMAC_ISR_ALE | EMAC_ISR_BFCS | EMAC_ISR_PTLE | EMAC_ISR_ORE |
 		EMAC_ISR_IRE | EMAC_ISR_TE;
 	if (emac_has_feature(dev, EMAC_FTR_EMAC4))
-	    r |= EMAC4_ISR_TXPE | EMAC4_ISR_RXPE /* | EMAC4_ISR_TXUE |
-						  EMAC4_ISR_RXOE | */;
+	    // Enable tx underrun interrupts but disable rx overruns given the massive amount of overruns
+	    r |= EMAC4_ISR_TXPE | EMAC4_ISR_RXPE | EMAC4_ISR_TXUE; // | EMAC4_ISR_RXOE;
 	out_be32(&p->iser,  r);
 
 	/* We need to take GPCS PHY out of isolate mode after EMAC reset */
 	if (emac_phy_gpcs(dev->phy.mode)) {
-		if (dev->phy.gpcs_address != 0xffffffff)
-			emac_mii_reset_gpcs(&dev->phy);
-		else
-			emac_mii_reset_phy(&dev->phy);
+		if (dev->phy.gpcs_address != 0xffffffff) emac_mii_reset_gpcs(&dev->phy);
+		else emac_mii_reset_phy(&dev->phy);
 	}
-
 	return 0;
 }
 
-static void emac_reinitialize(struct emac_instance *dev)
-{
+static void emac_reinitialize(struct emac_instance *dev) {
 	DBG(dev, "reinitialize" NL);
 
 	emac_netif_stop(dev);
@@ -750,8 +651,7 @@
 	emac_netif_start(dev);
 }
 
-static void emac_full_tx_reset(struct emac_instance *dev)
-{
+static void emac_full_tx_reset(struct emac_instance *dev) {
 	DBG(dev, "full_tx_reset" NL);
 
 	emac_tx_disable(dev);
@@ -766,12 +666,10 @@
 	emac_rx_enable(dev);
 }
 
-static void emac_reset_work(struct work_struct *work)
-{
+static void emac_reset_work(struct work_struct *work) {
 	struct emac_instance *dev = container_of(work, struct emac_instance, reset_work);
 
 	DBG(dev, "reset_work" NL);
-
 	mutex_lock(&dev->link_lock);
 	if (dev->opened) {
 		emac_netif_stop(dev);
@@ -781,41 +679,30 @@
 	mutex_unlock(&dev->link_lock);
 }
 
-static void emac_tx_timeout(struct net_device *ndev)
-{
+/* Respond to a TX hang */
+static void emac_tx_timeout(struct net_device *ndev) {
 	struct emac_instance *dev = netdev_priv(ndev);
-
+	printk(KERN_ERR "%pOF: tx timeout\n", dev->ofdev->dev.of_node);
 	DBG(dev, "tx_timeout" NL);
-
 	schedule_work(&dev->reset_work);
 }
 
 
-static inline int emac_phy_done(struct emac_instance *dev, u32 stacr)
-{
+static inline int emac_phy_done(struct emac_instance *dev, u32 stacr) {
 	int done = !!(stacr & EMAC_STACR_OC);
-
-	if (emac_has_feature(dev, EMAC_FTR_STACR_OC_INVERT))
-		done = !done;
-
+	if (likely(emac_has_feature(dev, EMAC_FTR_STACR_OC_INVERT))) done = !done;
 	return done;
 };
 
-static int __emac_mdio_read(struct emac_instance *dev, u8 id, u8 reg)
-{
+static inline int __emac_mdio_read(struct emac_instance *dev, u8 id, u8 reg) {
 	struct emac_regs __iomem *p = dev->emacp;
 	u32 r = 0;
 	int n, err = -ETIMEDOUT;
 
 	mutex_lock(&dev->mdio_lock);
-
 	DBG2(dev, "mdio_read(%02x,%02x)" NL, id, reg);
 
-	/* Enable proper MDIO port */
-	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII))
-		zmii_get_mdio(dev->zmii_dev, dev->zmii_port);
-	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII))
-		rgmii_get_mdio(dev->rgmii_dev, dev->rgmii_port);
+	MDIO(get, dev);		/* Enable proper MDIO port */
 
 	/* Wait for management interface to become idle */
 	n = 20;
@@ -828,18 +715,13 @@
 	}
 
 	/* Issue read command */
-	if (emac_has_feature(dev, EMAC_FTR_EMAC4))
-		r = EMAC4_STACR_BASE(dev->opb_bus_freq);
-	else
-		r = EMAC_STACR_BASE(dev->opb_bus_freq);
-	if (emac_has_feature(dev, EMAC_FTR_STACR_OC_INVERT))
-		r |= EMAC_STACR_OC;
-	if (emac_has_feature(dev, EMAC_FTR_HAS_NEW_STACR))
-		r |= EMACX_STACR_STAC_READ;
-	else
-		r |= EMAC_STACR_STAC_READ;
-	r |= (reg & EMAC_STACR_PRA_MASK)
-		| ((id & EMAC_STACR_PCDA_MASK) << EMAC_STACR_PCDA_SHIFT);
+	if (likely(emac_has_feature(dev, EMAC_FTR_EMAC4))) r = EMAC4_STACR_BASE(dev->opb_bus_freq);
+	else r = EMAC_STACR_BASE(dev->opb_bus_freq);
+
+	if (likely(emac_has_feature(dev, EMAC_FTR_STACR_OC_INVERT))) r |= EMAC_STACR_OC;
+	if (likely(emac_has_feature(dev, EMAC_FTR_HAS_NEW_STACR))) r |= EMACX_STACR_STAC_READ;
+	else r |= EMAC_STACR_STAC_READ;
+	r |= (reg & EMAC_STACR_PRA_MASK) | ((id & EMAC_STACR_PCDA_MASK) << EMAC_STACR_PCDA_SHIFT);
 	out_be32(&p->stacr, r);
 
 	/* Wait for read to complete */
@@ -863,31 +745,19 @@
 	DBG2(dev, "mdio_read -> %04x" NL, r);
 	err = 0;
  bail:
-	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII))
-		rgmii_put_mdio(dev->rgmii_dev, dev->rgmii_port);
-	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII))
-		zmii_put_mdio(dev->zmii_dev, dev->zmii_port);
+	MDIO(put, dev);
 	mutex_unlock(&dev->mdio_lock);
-
 	return err == 0 ? r : err;
 }
 
-static void __emac_mdio_write(struct emac_instance *dev, u8 id, u8 reg,
-			      u16 val)
-{
+static inline void __emac_mdio_write(struct emac_instance *dev, u8 id, u8 reg, u16 val) {
 	struct emac_regs __iomem *p = dev->emacp;
-	u32 r = 0;
+	register u32 r = 0;
 	int n, err = -ETIMEDOUT;
 
 	mutex_lock(&dev->mdio_lock);
-
 	DBG2(dev, "mdio_write(%02x,%02x,%04x)" NL, id, reg, val);
-
-	/* Enable proper MDIO port */
-	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII))
-		zmii_get_mdio(dev->zmii_dev, dev->zmii_port);
-	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII))
-		rgmii_get_mdio(dev->rgmii_dev, dev->rgmii_port);
+	MDIO(get, dev);		/* Enable proper MDIO port */
 
 	/* Wait for management interface to be idle */
 	n = 20;
@@ -900,19 +770,13 @@
 	}
 
 	/* Issue write command */
-	if (emac_has_feature(dev, EMAC_FTR_EMAC4))
-		r = EMAC4_STACR_BASE(dev->opb_bus_freq);
-	else
-		r = EMAC_STACR_BASE(dev->opb_bus_freq);
-	if (emac_has_feature(dev, EMAC_FTR_STACR_OC_INVERT))
-		r |= EMAC_STACR_OC;
-	if (emac_has_feature(dev, EMAC_FTR_HAS_NEW_STACR))
-		r |= EMACX_STACR_STAC_WRITE;
-	else
-		r |= EMAC_STACR_STAC_WRITE;
-	r |= (reg & EMAC_STACR_PRA_MASK) |
-		((id & EMAC_STACR_PCDA_MASK) << EMAC_STACR_PCDA_SHIFT) |
-		(val << EMAC_STACR_PHYD_SHIFT);
+	if (likely(emac_has_feature(dev, EMAC_FTR_EMAC4))) r = EMAC4_STACR_BASE(dev->opb_bus_freq);
+	else r = EMAC_STACR_BASE(dev->opb_bus_freq);
+	if (likely(emac_has_feature(dev, EMAC_FTR_STACR_OC_INVERT))) r |= EMAC_STACR_OC;
+	if (likely(emac_has_feature(dev, EMAC_FTR_HAS_NEW_STACR))) r |= EMACX_STACR_STAC_WRITE;
+	else r |= EMAC_STACR_STAC_WRITE;
+	r |= (reg & EMAC_STACR_PRA_MASK) | ((id & EMAC_STACR_PCDA_MASK) 
+		<< EMAC_STACR_PCDA_SHIFT) |	(val << EMAC_STACR_PHYD_SHIFT);
 	out_be32(&p->stacr, r);
 
 	/* Wait for write to complete */
@@ -926,38 +790,24 @@
 	}
 	err = 0;
  bail:
-	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII))
-		rgmii_put_mdio(dev->rgmii_dev, dev->rgmii_port);
-	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII))
-		zmii_put_mdio(dev->zmii_dev, dev->zmii_port);
+	MDIO (put, dev);
 	mutex_unlock(&dev->mdio_lock);
 }
 
-static int emac_mdio_read(struct net_device *ndev, int id, int reg)
-{
+static int emac_mdio_read(struct net_device *ndev, int id, int reg) {
 	struct emac_instance *dev = netdev_priv(ndev);
-	int res;
-
-	res = __emac_mdio_read((dev->mdio_instance &&
-				dev->phy.gpcs_address != id) ?
-				dev->mdio_instance : dev,
-			       (u8) id, (u8) reg);
-	return res;
+	return __emac_mdio_read((dev->mdio_instance && dev->phy.gpcs_address != id) ?
+				dev->mdio_instance : dev, (u8) id, (u8) reg);
 }
 
-static void emac_mdio_write(struct net_device *ndev, int id, int reg, int val)
-{
+static void emac_mdio_write(struct net_device *ndev, int id, int reg, int val) {
 	struct emac_instance *dev = netdev_priv(ndev);
-
-	__emac_mdio_write((dev->mdio_instance &&
-			   dev->phy.gpcs_address != id) ?
-			   dev->mdio_instance : dev,
-			  (u8) id, (u8) reg, (u16) val);
+	__emac_mdio_write((dev->mdio_instance && dev->phy.gpcs_address != id) ?
+			   dev->mdio_instance : dev, (u8) id, (u8) reg, (u16) val);
 }
 
 /* Tx lock BH */
-static void __emac_set_multicast_list(struct emac_instance *dev)
-{
+static void __emac_set_multicast_list(struct emac_instance *dev) {
 	struct emac_regs __iomem *p = dev->emacp;
 	u32 rmr = emac_iff2rmr(dev->ndev);
 
@@ -982,64 +832,50 @@
 	 */
 	dev->mcast_pending = 0;
 	emac_rx_disable(dev);
-	if (rmr & EMAC_RMR_MAE)
-		emac_hash_mc(dev);
+	if (rmr & EMAC_RMR_MAE)	emac_hash_mc(dev);
 	out_be32(&p->rmr, rmr);
 	emac_rx_enable(dev);
 }
 
 /* Tx lock BH */
-static void emac_set_multicast_list(struct net_device *ndev)
-{
+static void emac_set_multicast_list(struct net_device *ndev) {
 	struct emac_instance *dev = netdev_priv(ndev);
 
 	DBG(dev, "multicast" NL);
-
 	BUG_ON(!netif_running(dev->ndev));
 
 	if (dev->no_mcast) {
 		dev->mcast_pending = 1;
 		return;
 	}
-
 	mutex_lock(&dev->link_lock);
 	__emac_set_multicast_list(dev);
 	mutex_unlock(&dev->link_lock);
 }
 
-static int emac_set_mac_address(struct net_device *ndev, void *sa)
-{
+static int emac_set_mac_address(struct net_device *ndev, void *sa) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	struct sockaddr *addr = sa;
 	struct emac_regs __iomem *p = dev->emacp;
 
-	if (!is_valid_ether_addr(addr->sa_data))
-	       return -EADDRNOTAVAIL;
-
+	if (!is_valid_ether_addr(addr->sa_data)) return -EADDRNOTAVAIL;
 	mutex_lock(&dev->link_lock);
-
 	memcpy(ndev->dev_addr, addr->sa_data, ndev->addr_len);
-
 	emac_rx_disable(dev);
 	emac_tx_disable(dev);
 	out_be32(&p->iahr, (ndev->dev_addr[0] << 8) | ndev->dev_addr[1]);
 	out_be32(&p->ialr, (ndev->dev_addr[2] << 24) |
-		(ndev->dev_addr[3] << 16) | (ndev->dev_addr[4] << 8) |
-		ndev->dev_addr[5]);
+		(ndev->dev_addr[3] << 16) | (ndev->dev_addr[4] << 8) | ndev->dev_addr[5]);
 	emac_tx_enable(dev);
 	emac_rx_enable(dev);
-
 	mutex_unlock(&dev->link_lock);
-
 	return 0;
 }
 
-static int emac_resize_rx_ring(struct emac_instance *dev, int new_mtu)
-{
+static int emac_resize_rx_ring(struct emac_instance *dev, int new_mtu) {
 	int rx_sync_size = emac_rx_sync_size(new_mtu);
 	int rx_skb_size = emac_rx_skb_size(new_mtu);
-	int i, ret = 0;
-	int mr1_jumbo_bit_change = 0;
+	int i, ret = 0, mr1_jumbo_bit_change = 0;
 
 	mutex_lock(&dev->link_lock);
 	emac_netif_stop(dev);
@@ -1054,24 +890,23 @@
 
 	/* Make a first pass over RX ring and mark BDs ready, dropping
 	 * non-processed packets on the way. We need this as a separate pass
-	 * to simplify error recovery in the case of allocation failure later.
-	 */
+	 * to simplify error recovery in the case of allocation failure later */
 	for (i = 0; i < NUM_RX_BUFF; ++i) {
-		if (dev->rx_desc[i].ctrl & MAL_RX_CTRL_FIRST)
-			++dev->estats.rx_dropped_resize;
-
+		if (dev->rx_desc[i].ctrl & MAL_RX_CTRL_FIRST) ++dev->estats.rx_dropped_resize;
 		dev->rx_desc[i].data_len = 0;
-		dev->rx_desc[i].ctrl = MAL_RX_CTRL_EMPTY |
-		    (i == (NUM_RX_BUFF - 1) ? MAL_RX_CTRL_WRAP : 0);
+		dev->rx_desc[i].ctrl = MAL_RX_CTRL_EMPTY | MAL_RX_CTRL_INTR |
+			(i == (NUM_RX_BUFF - 1) ? MAL_RX_CTRL_WRAP : 0);  //ECO
 	}
 
 	/* Reallocate RX ring only if bigger skb buffers are required */
-	if (rx_skb_size <= dev->rx_skb_size)
-		goto skip;
+	DBG(dev, "New rx_skb_size = %d" NL, rx_skb_size);
+	DBG(dev, "Current rx_skb_size = %d" NL, dev->rx_skb_size);
+	if (rx_skb_size <= dev->rx_skb_size) goto skip;
 
 	/* Second pass, allocate new skbs */
 	for (i = 0; i < NUM_RX_BUFF; ++i) {
-		struct sk_buff *skb = alloc_skb(rx_skb_size, GFP_ATOMIC);
+		//struct sk_buff *skb = alloc_skb(rx_skb_size, GFP_ATOMIC);
+		struct sk_buff *skb = netdev_alloc_skb_ip_align(dev->ndev, rx_skb_size);
 		if (!skb) {
 			ret = -ENOMEM;
 			goto oom;
@@ -1080,20 +915,16 @@
 		BUG_ON(!dev->rx_skb[i]);
 		dev_kfree_skb(dev->rx_skb[i]);
 
-		skb_reserve(skb, EMAC_RX_SKB_HEADROOM + 2);
 		dev->rx_desc[i].data_ptr =
-		    dma_map_single(&dev->ofdev->dev, skb->data - 2, rx_sync_size,
-				   DMA_FROM_DEVICE) + 2;
+		    dma_map_single(dev->ofdev_dev, skb->data - NET_IP_ALIGN, rx_sync_size, DMA_FROM_DEVICE) + NET_IP_ALIGN;
 		dev->rx_skb[i] = skb;
 	}
  skip:
 	/* Check if we need to change "Jumbo" bit in MR1 */
 	if (emac_has_feature(dev, EMAC_APM821XX_REQ_JUMBO_FRAME_SIZE)) {
-		mr1_jumbo_bit_change = (new_mtu > ETH_DATA_LEN) ||
-				(dev->ndev->mtu > ETH_DATA_LEN);
+		mr1_jumbo_bit_change = (new_mtu > ETH_DATA_LEN) || (dev->ndev->mtu > ETH_DATA_LEN);
 	} else {
-		mr1_jumbo_bit_change = (new_mtu > ETH_DATA_LEN) ^
-				(dev->ndev->mtu > ETH_DATA_LEN);
+		mr1_jumbo_bit_change = (new_mtu > ETH_DATA_LEN) ^ (dev->ndev->mtu > ETH_DATA_LEN);
 	}
 
 	if (mr1_jumbo_bit_change) {
@@ -1118,8 +949,7 @@
 }
 
 /* Process ctx, rtnl_lock semaphore */
-static int emac_change_mtu(struct net_device *ndev, int new_mtu)
-{
+static int emac_change_mtu(struct net_device *ndev, int new_mtu) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	int ret = 0;
 
@@ -1132,6 +962,27 @@
 	}
 
 	if (!ret) {
+#if defined(CONFIG_IBM_EMAC_TAH) && defined(EMAC_HW_TSO)  //ECO
+		struct tah_instance *tdev;
+		int i, adj_val = 0;
+		u32 ssr_def[TAH_NO_SSR];
+		switch (new_mtu) {
+			case 9000:	memcpy(ssr_def, (u32[])TAH_SS_DEFAULT_9K, sizeof ssr_def); break;
+			case 4080:  
+            case 4000:  memcpy(ssr_def, (u32[])TAH_SS_DEFAULT_4K, sizeof ssr_def); break;
+			default:	memcpy(ssr_def, (u32[])TAH_SS_DEFAULT, sizeof ssr_def); break;
+		}
+		tdev = dev_get_drvdata(&dev->tah_dev->dev);
+		if (new_mtu > ssr_def[0]) {	/* add the current MTU if not 9000, 4080 or 1500 */
+			tah_set_ssr(dev->tah_dev, 0, new_mtu);
+			adj_val = 1;	/* update the adjustment var */
+		}
+		for (i = adj_val; i < TAH_NO_SSR; i++) {	/* don't allow values to exceed new MTU */
+			  if (ssr_def[i-adj_val] > new_mtu) tah_set_ssr(dev->tah_dev, i, new_mtu);					
+			  else tah_set_ssr(dev->tah_dev, i,	ssr_def[i-adj_val]);
+
+		}
+#endif
 		ndev->mtu = new_mtu;
 		dev->rx_skb_size = emac_rx_skb_size(new_mtu);
 		dev->rx_sync_size = emac_rx_sync_size(new_mtu);
@@ -1140,28 +991,25 @@
 	return ret;
 }
 
-static void emac_clean_tx_ring(struct emac_instance *dev)
-{
+static void emac_clean_tx_ring(struct emac_instance *dev) {
 	int i;
 
 	for (i = 0; i < NUM_TX_BUFF; ++i) {
-		if (dev->tx_skb[i]) {
+		if (likely(dev->tx_skb[i])) {
 			dev_kfree_skb(dev->tx_skb[i]);
 			dev->tx_skb[i] = NULL;
-			if (dev->tx_desc[i].ctrl & MAL_TX_CTRL_READY)
-				++dev->estats.tx_dropped;
+			if (dev->tx_desc[i].ctrl & MAL_TX_CTRL_READY) ++dev->estats.tx_dropped;
 		}
 		dev->tx_desc[i].ctrl = 0;
 		dev->tx_desc[i].data_ptr = 0;
 	}
 }
 
-static void emac_clean_rx_ring(struct emac_instance *dev)
-{
+static void emac_clean_rx_ring(struct emac_instance *dev) {
 	int i;
 
 	for (i = 0; i < NUM_RX_BUFF; ++i)
-		if (dev->rx_skb[i]) {
+		if (likely(dev->rx_skb[i])) {
 			dev->rx_desc[i].ctrl = 0;
 			dev_kfree_skb(dev->rx_skb[i]);
 			dev->rx_skb[i] = NULL;
@@ -1174,60 +1022,69 @@
 	}
 }
 
-static inline int emac_alloc_rx_skb(struct emac_instance *dev, int slot,
-				    gfp_t flags)
-{
-	struct sk_buff *skb = alloc_skb(dev->rx_skb_size, flags);
-	if (unlikely(!skb))
-		return -ENOMEM;
+#if !defined (CONFIG_APM821xx) // Moved to core.c for CONFIG_APM821xx
+static __always_inline int __prepare_rx_skb(struct sk_buff *skb, struct emac_instance *dev, int slot,
+        struct mal_descriptor *rx_desc) {
 
 	dev->rx_skb[slot] = skb;
-	dev->rx_desc[slot].data_len = 0;
-
-	skb_reserve(skb, EMAC_RX_SKB_HEADROOM + 2);
-	dev->rx_desc[slot].data_ptr =
-	    dma_map_single(&dev->ofdev->dev, skb->data - 2, dev->rx_sync_size,
-			   DMA_FROM_DEVICE) + 2;
+    rx_desc->data_len = 0;
+    rx_desc->data_ptr =
+	    dma_map_single(dev->ofdev_dev, skb->data - NET_IP_ALIGN, dev->rx_sync_size, DMA_FROM_DEVICE) + NET_IP_ALIGN;
 	wmb();
-	dev->rx_desc[slot].ctrl = MAL_RX_CTRL_EMPTY |
-	    (slot == (NUM_RX_BUFF - 1) ? MAL_RX_CTRL_WRAP : 0);
-
+	rx_desc->ctrl = MAL_RX_CTRL_EMPTY | MAL_RX_CTRL_INTR | (slot == (NUM_RX_BUFF - 1) ? MAL_RX_CTRL_WRAP : 0);
 	return 0;
 }
 
-static void emac_print_link_status(struct emac_instance *dev)
-{
+static inline int alloc_rx_skb(struct emac_instance *dev, int slot) {
+	struct sk_buff *skb;
+	skb = __netdev_alloc_skb_ip_align(dev->ndev, dev->rx_skb_size, GFP_KERNEL);
+    if (unlikely(!skb))	return -ENOMEM;
+	return __prepare_rx_skb(skb, dev, slot, &dev->rx_desc[slot]);
+}
+
+static inline int alloc_rx_skb_napi(struct emac_instance *dev, int slot) {
+	struct sk_buff *skb;
+	skb = napi_alloc_skb(&dev->mal->napi, dev->rx_skb_size);
+    if (unlikely(!skb))	return -ENOMEM;
+	return __prepare_rx_skb(skb, dev, slot, &dev->rx_desc[slot]);
+}
+#else
+extern int alloc_rx_skb(struct emac_instance *dev, int slot);
+#endif
+
+static void emac_print_link_status(struct emac_instance *dev) {
 	if (netif_carrier_ok(dev->ndev))
-		printk(KERN_INFO "%s: link is up, %d %s%s\n",
-		       dev->ndev->name, dev->phy.speed,
-		       dev->phy.duplex == DUPLEX_FULL ? "FDX" : "HDX",
-		       dev->phy.pause ? ", pause enabled" :
-		       dev->phy.asym_pause ? ", asymmetric pause enabled" : "");
-	else
-		printk(KERN_INFO "%s: link is down\n", dev->ndev->name);
+		printk(KERN_INFO "%s: link is up, %d %s%s\n", dev->ndev->name, dev->phy.speed,
+			dev->phy.duplex == DUPLEX_FULL ? "FDX" : "HDX",
+		    dev->phy.pause ? ", pause enabled" : dev->phy.asym_pause ? ", asymmetric pause enabled" : "");
+	else printk(KERN_INFO "%s: link is down\n", dev->ndev->name);
 }
 
-/* Process ctx, rtnl_lock semaphore */
-static int emac_open(struct net_device *ndev)
-{
+/* Called when the network interface is made active: Process ctx, rtnl_lock semaphore */
+static int emac_open(struct net_device *ndev) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	int err, i;
 
 	DBG(dev, "open" NL);
 
-	/* Setup error IRQ handler */
-	err = request_irq(dev->emac_irq, emac_irq, 0, "EMAC", dev);
+	err = request_irq(dev->emac_irq, emac_irq, 0, "EMAC", dev); // Setup error IRQ handler
 	if (err) {
-		printk(KERN_ERR "%s: failed to request IRQ %d\n",
-		       ndev->name, dev->emac_irq);
+		printk(KERN_ERR "%s: failed to request IRQ %d\n", ndev->name, dev->emac_irq);
 		return err;
 	}
 
+	if (dev->wol_irq != NO_IRQ) { 	// Setup WOL IRQ handler
+		err = request_irq(dev->wol_irq, wol_irq, 0, "EMAC WOL", dev);
+		if (err) {
+			printk(KERN_ERR "%s: failed to request IRQ %d\n", ndev->name, dev->wol_irq);
+			return err;
+		}
+	}
+
 	/* Allocate RX ring */
 	for (i = 0; i < NUM_RX_BUFF; ++i)
-		if (emac_alloc_rx_skb(dev, i, GFP_KERNEL)) {
-			printk(KERN_ERR "%s: failed to allocate RX ring\n",
-			       ndev->name);
+		if (unlikely(alloc_rx_skb(dev, i))) {
+			printk(KERN_ERR "%s: failed to allocate RX ring\n", ndev->name);
 			goto oom;
 		}
 
@@ -1238,8 +1095,7 @@
 	mutex_lock(&dev->link_lock);
 	dev->opened = 1;
 
-	/* Start PHY polling now.
-	 */
+	/* Start PHY polling now */
 	if (dev->phy.address >= 0) {
 		int link_poll_interval;
 		if (dev->phy.def->ops->poll_link(&dev->phy)) {
@@ -1262,6 +1118,7 @@
 	/* Required for Pause packet support in EMAC */
 	dev_mc_add_global(ndev, default_mcast_addr);
 
+	//local_irq_save(flags);	/* disable interrupts ECO */
 	emac_configure(dev);
 	mal_poll_add(dev->mal, &dev->commac);
 	mal_enable_tx_channel(dev->mal, dev->mal_tx_chan);
@@ -1269,6 +1126,8 @@
 	mal_enable_rx_channel(dev->mal, dev->mal_rx_chan);
 	emac_tx_enable(dev);
 	emac_rx_enable(dev);
+	//local_irq_restore(flags);
+
 	emac_netif_start(dev);
 
 	mutex_unlock(&dev->link_lock);
@@ -1277,61 +1136,49 @@
  oom:
 	emac_clean_rx_ring(dev);
 	free_irq(dev->emac_irq, dev);
+	if (dev->wol_irq != NO_IRQ) free_irq(dev->wol_irq, dev);
 
 	return -ENOMEM;
 }
 
 /* BHs disabled */
 #if 0
-static int emac_link_differs(struct emac_instance *dev)
-{
+static int emac_link_differs(struct emac_instance *dev) {
 	u32 r = in_be32(&dev->emacp->mr1);
 
 	int duplex = r & EMAC_MR1_FDE ? DUPLEX_FULL : DUPLEX_HALF;
 	int speed, pause, asym_pause;
 
-	if (r & EMAC_MR1_MF_1000)
-		speed = SPEED_1000;
-	else if (r & EMAC_MR1_MF_100)
-		speed = SPEED_100;
-	else
-		speed = SPEED_10;
+	if (r & EMAC_MR1_MF_1000) speed = SPEED_1000;
+	else if (r & EMAC_MR1_MF_100) speed = SPEED_100;
+	else speed = SPEED_10;
 
 	switch (r & (EMAC_MR1_EIFC | EMAC_MR1_APP)) {
-	case (EMAC_MR1_EIFC | EMAC_MR1_APP):
-		pause = 1;
-		asym_pause = 0;
-		break;
-	case EMAC_MR1_APP:
-		pause = 0;
-		asym_pause = 1;
-		break;
-	default:
-		pause = asym_pause = 0;
+	case (EMAC_MR1_EIFC | EMAC_MR1_APP):	pause = 1; asym_pause = 0; break;
+	case EMAC_MR1_APP:			pause = 0; asym_pause = 1; break;
+	default:				pause = asym_pause = 0;
 	}
 	return speed != dev->phy.speed || duplex != dev->phy.duplex ||
 	    pause != dev->phy.pause || asym_pause != dev->phy.asym_pause;
 }
 #endif
 
-static void emac_link_timer(struct work_struct *work)
-{
+static void emac_link_timer(struct work_struct *work) {
 	struct emac_instance *dev =
-		container_of(to_delayed_work(work),
-			     struct emac_instance, link_work);
+		container_of(to_delayed_work(work), struct emac_instance, link_work);
 	int link_poll_interval;
+    struct mii_phy *phy = &dev->phy;
 
 	mutex_lock(&dev->link_lock);
 	DBG2(dev, "link timer" NL);
 
-	if (!dev->opened)
-		goto bail;
+	if (unlikely(!dev->opened)) goto bail;
 
-	if (dev->phy.def->ops->poll_link(&dev->phy)) {
+	if (phy->def->ops->poll_link(phy)) {
 		if (!netif_carrier_ok(dev->ndev)) {
 			emac_rx_clk_default(dev);
 			/* Get new link parameters */
-			dev->phy.def->ops->read_link(&dev->phy);
+			phy->def->ops->read_link(phy);
 
 			netif_carrier_on(dev->ndev);
 			emac_netif_stop(dev);
@@ -1355,8 +1202,7 @@
 	mutex_unlock(&dev->link_lock);
 }
 
-static void emac_force_link_update(struct emac_instance *dev)
-{
+static void emac_force_link_update(struct emac_instance *dev) {
 	netif_carrier_off(dev->ndev);
 	smp_rmb();
 	if (dev->link_polling) {
@@ -1366,9 +1212,8 @@
 	}
 }
 
-/* Process ctx, rtnl_lock semaphore */
-static int emac_close(struct net_device *ndev)
-{
+/* Called when the network interface is disabled: Process ctx, rtnl_lock semaphore */
+static int emac_close(struct net_device *ndev) {
 	struct emac_instance *dev = netdev_priv(ndev);
 
 	DBG(dev, "close" NL);
@@ -1394,35 +1239,43 @@
 	free_irq(dev->emac_irq, dev);
 
 	netif_carrier_off(ndev);
+	if (dev->wol_irq != NO_IRQ) free_irq(dev->wol_irq, dev);
 
 	return 0;
 }
 
-static inline u16 emac_tx_csum(struct emac_instance *dev,
-			       struct sk_buff *skb)
-{
-	if (emac_has_feature(dev, EMAC_FTR_HAS_TAH) &&
-		(skb->ip_summed == CHECKSUM_PARTIAL)) {
+static __always_inline u16 emac_tx_csum(struct emac_instance *dev, struct sk_buff *skb) {
+	if (likely(skb->ip_summed == CHECKSUM_PARTIAL)) {
 		++dev->stats.tx_packets_csum;
+
+#if defined(CONFIG_IBM_EMAC_TAH) && defined(EMAC_HW_TSO)  //ECO
+#define SKB_GSO_TYPE(skb)	(skb_shinfo(skb)->gso_type)
+		// Only support TSO (TCP segmentation offload) for TCP
+		if (likely(skb_is_gso(skb) && likely(SKB_GSO_TYPE(skb) & (SKB_GSO_TCPV4 | SKB_GSO_TCPV6)))) {
+            return EMAC_TX_CTRL_TAH_SSR0;
+            /* Technically this is the correct code. However when skb_is_gso(skb) is true, 
+             * the packet is larger than MTU, hence returning ssr0 is OK */
+            /*
+ 			u32 i = 0, seg_size = skb->len + tcp_hdrlen(skb) + skb_network_header_len(skb);
+            do if (likely(dev->ssr[i] <= seg_size)) return EMAC_TX_CTRL_TAH_SSR(i);
+            while (++i < TAH_NO_SSR);
+            */
+		}
+#endif
 		return EMAC_TX_CTRL_TAH_CSUM;
 	}
 	return 0;
 }
 
-static inline int emac_xmit_finish(struct emac_instance *dev, int len)
-{
-	struct emac_regs __iomem *p = dev->emacp;
-	struct net_device *ndev = dev->ndev;
+static __always_inline int emac_xmit_finish(struct emac_instance *dev, struct net_device *ndev, int len) {
 
 	/* Send the packet out. If the if makes a significant perf
-	 * difference, then we can store the TMR0 value in "dev"
-	 * instead
-	 */
-	if (emac_has_feature(dev, EMAC_FTR_EMAC4))
-		out_be32(&p->tmr0, EMAC4_TMR0_XMIT);
-	else
-		out_be32(&p->tmr0, EMAC_TMR0_XMIT);
-
+	 * difference, then we can store the TMR0 value in "dev" instead */
+#ifdef CONFIG_IBM_EMAC_EMAC4	
+    out_be32(dev->tmr0, EMAC4_TMR0_XMIT);
+#else
+    out_be32(dev->tmr0, EMAC_TMR0_XMIT);
+#endif
 	if (unlikely(++dev->tx_cnt == NUM_TX_BUFF)) {
 		netif_stop_queue(ndev);
 		DBG2(dev, "stopped TX queue" NL);
@@ -1436,113 +1289,106 @@
 }
 
 /* Tx lock BH */
-static int emac_start_xmit(struct sk_buff *skb, struct net_device *ndev)
-{
-	struct emac_instance *dev = netdev_priv(ndev);
-	unsigned int len = skb->len;
-	int slot;
-
-	u16 ctrl = EMAC_TX_CTRL_GFCS | EMAC_TX_CTRL_GP | MAL_TX_CTRL_READY |
-	    MAL_TX_CTRL_LAST | emac_tx_csum(dev, skb);
-
-	slot = dev->tx_slot++;
-	if (dev->tx_slot == NUM_TX_BUFF) {
-		dev->tx_slot = 0;
-		ctrl |= MAL_TX_CTRL_WRAP;
-	}
-
-	DBG2(dev, "xmit(%u) %d" NL, len, slot);
-
+static __always_inline int _emac_start_xmit(struct sk_buff *skb, register struct emac_instance *dev, unsigned int len) {
+	register int slot = dev->tx_slot;
+    register struct mal_descriptor *desc = &dev->tx_desc[slot];
+	register u16 ctrl = EMAC_TX_CTRL_GFCS | EMAC_TX_CTRL_GP | MAL_TX_CTRL_READY |
+	    MAL_TX_CTRL_LAST | MAL_TX_CTRL_INTR | emac_tx_csum(dev, skb);
+#ifdef CONFIG_IBM_EMAC_MASK_CEXT
+	if (atomic_read(&dev->mask_cext_enable))
+		if (atomic_read(&dev->idle_mode)) {
+		    emac_exit_idlemode(dev);
+		    atomic_set(&dev->idle_mode, 0);
+		}
+#endif
+    dev->tx_slot = NXT_TX_SLOT(slot);
+	if (unlikely(slot == NUM_TX_BUFF - 1)) ctrl |= MAL_TX_CTRL_WRAP;  // set WRAP on last slot
+	DBG2(dev, "xmit(%u) %d\n" NL, len, slot);
 	dev->tx_skb[slot] = skb;
-	dev->tx_desc[slot].data_ptr = dma_map_single(&dev->ofdev->dev,
-						     skb->data, len,
-						     DMA_TO_DEVICE);
-	dev->tx_desc[slot].data_len = (u16) len;
+    desc->data_ptr = dma_map_single(dev->ofdev_dev, skb->data, len, DMA_TO_DEVICE);
+    desc->data_len = (u16) len;
 	wmb();
-	dev->tx_desc[slot].ctrl = ctrl;
+    desc->ctrl = ctrl;
 
-	return emac_xmit_finish(dev, len);
+	return emac_xmit_finish(dev, dev->ndev, len);
 }
 
-static inline int emac_xmit_split(struct emac_instance *dev, int slot,
-				  u32 pd, int len, int last, u16 base_ctrl)
-{
-	while (1) {
-		u16 ctrl = base_ctrl;
-		int chunk = min(len, MAL_MAX_TX_SIZE);
-		len -= chunk;
+static int emac_start_xmit(struct sk_buff *skb, struct net_device *ndev) {
+    return _emac_start_xmit(skb, netdev_priv(ndev), skb->len);
+}
 
-		slot = (slot + 1) % NUM_TX_BUFF;
+static inline int emac_xmit_split(struct emac_instance *dev, int slot, 
+				  u32 pd, int len, int last, u16 base_ctrl) {
+    struct mal_descriptor *desc = &dev->tx_desc[slot];
+    while (1) {
+		u16 ctrl = base_ctrl | MAL_TX_CTRL_INTR;
+		int chunk = min(len, MAL_MAX_TX_SIZE);
+  		len -= chunk;
 
-		if (last && !len)
-			ctrl |= MAL_TX_CTRL_LAST;
-		if (slot == NUM_TX_BUFF - 1)
-			ctrl |= MAL_TX_CTRL_WRAP;
+		slot = NXT_TX_SLOT(slot);
+		if (last && !len) ctrl |= MAL_TX_CTRL_LAST;
+		if (unlikely(slot == NUM_TX_BUFF - 1)) ctrl |= MAL_TX_CTRL_WRAP;
 
 		dev->tx_skb[slot] = NULL;
-		dev->tx_desc[slot].data_ptr = pd;
-		dev->tx_desc[slot].data_len = (u16) chunk;
-		dev->tx_desc[slot].ctrl = ctrl;
+        desc = &dev->tx_desc[slot];
+		desc->data_ptr = pd;
+		desc->data_len = (u16) chunk;
+		desc->ctrl = ctrl;
 		++dev->tx_cnt;
 
-		if (!len)
-			break;
-
+		if (!len) return slot;
 		pd += chunk;
 	}
-	return slot;
 }
 
 /* Tx lock BH disabled (SG version for TAH equipped EMACs) */
-static int emac_start_xmit_sg(struct sk_buff *skb, struct net_device *ndev)
-{
-	struct emac_instance *dev = netdev_priv(ndev);
-	int nr_frags = skb_shinfo(skb)->nr_frags;
-	int len = skb->len, chunk;
-	int slot, i;
-	u16 ctrl;
+/* Transmit the packet using specified transmit queue */
+static int emac_start_xmit_sg(struct sk_buff *skb, struct net_device *ndev) {
+    register struct emac_instance *dev = netdev_priv(ndev);
+ 	int nr_frags = skb_shinfo(skb)->nr_frags;
+	register unsigned int len = skb->len;
+ 	register int slot;
+    register int chunk, slot_prev, i;
+	register u16 ctrl;
 	u32 pd;
 
 	/* This is common "fast" path */
-	if (likely(!nr_frags && len <= MAL_MAX_TX_SIZE))
-		return emac_start_xmit(skb, ndev);
+	if (likely(!nr_frags) && likely(len <= MAL_MAX_TX_SIZE)) return _emac_start_xmit(skb, dev, len);
+    //if ((cnt1 % 200)) printk(KERN_INFO "eth0: cnt1:%d, cnt2:%d, cnt3:%d\n", cnt1, cnt2, cnt3);
+
+#ifdef CONFIG_IBM_EMAC_MASK_CEXT
+	if (atomic_read(&dev->mask_cext_enable))
+		if (atomic_read(&dev->idle_mode)) {
+		    emac_exit_idlemode(dev);
+		    atomic_set(&dev->idle_mode, 0);
+		}
+#endif
 
 	len -= skb->data_len;
 
-	/* Note, this is only an *estimation*, we can still run out of empty
-	 * slots because of the additional fragmentation into
-	 * MAL_MAX_TX_SIZE-sized chunks
-	 */
-	if (unlikely(dev->tx_cnt + nr_frags + mal_tx_chunks(len) > NUM_TX_BUFF))
+	/* Note, this is only an *estimation*, we can still run out of empty slots because
+	 * of the additional fragmentation into MAL_MAX_TX_SIZE-sized chunks */
+	if (unlikely(dev->tx_cnt + nr_frags + MAL_ESTIMATE_TX_CHUNKS(len) > NUM_TX_BUFF))
 		goto stop_queue;
 
-	ctrl = EMAC_TX_CTRL_GFCS | EMAC_TX_CTRL_GP | MAL_TX_CTRL_READY |
-	    emac_tx_csum(dev, skb);
-	slot = dev->tx_slot;
+	ctrl = EMAC_TX_CTRL_GFCS| EMAC_TX_CTRL_GP| MAL_TX_CTRL_READY| MAL_TX_CTRL_INTR| emac_tx_csum(dev, skb);
+	slot = slot_prev = dev->tx_slot;
 
 	/* skb data */
 	dev->tx_skb[slot] = NULL;
-	chunk = min(len, MAL_MAX_TX_SIZE);
+	chunk = (likely(len <= MAL_MAX_TX_SIZE)) ? len : MAL_MAX_TX_SIZE;
 	dev->tx_desc[slot].data_ptr = pd =
-	    dma_map_single(&dev->ofdev->dev, skb->data, len, DMA_TO_DEVICE);
+	    dma_map_single(dev->ofdev_dev, skb->data, SKB_DATA_ALIGN(len), DMA_TO_DEVICE);
 	dev->tx_desc[slot].data_len = (u16) chunk;
 	len -= chunk;
-	if (unlikely(len))
-		slot = emac_xmit_split(dev, slot, pd + chunk, len, !nr_frags,
-				       ctrl);
-	/* skb fragments */
+	if (unlikely(len)) slot = emac_xmit_split(dev, slot, pd + chunk, len, !nr_frags, ctrl);
+    /* skb fragments */
 	for (i = 0; i < nr_frags; ++i) {
 		struct skb_frag_struct *frag = &skb_shinfo(skb)->frags[i];
 		len = skb_frag_size(frag);
-
-		if (unlikely(dev->tx_cnt + mal_tx_chunks(len) >= NUM_TX_BUFF))
-			goto undo_frame;
-
-		pd = skb_frag_dma_map(&dev->ofdev->dev, frag, 0, len,
-				      DMA_TO_DEVICE);
-
-		slot = emac_xmit_split(dev, slot, pd, len, i == nr_frags - 1,
-				       ctrl);
+		if (unlikely(dev->tx_cnt + MAL_ESTIMATE_TX_CHUNKS(len) >= NUM_TX_BUFF)) goto undo_frame;
+		pd = skb_frag_dma_map(dev->ofdev_dev, frag, 0, len, DMA_TO_DEVICE);
+		slot = emac_xmit_split(dev, slot, pd, len, i == nr_frags - 1, ctrl);
 	}
 
 	DBG2(dev, "xmit_sg(%u) %d - %d" NL, skb->len, dev->tx_slot, slot);
@@ -1551,23 +1397,19 @@
 	dev->tx_skb[slot] = skb;
 
 	/* Send the packet out */
-	if (dev->tx_slot == NUM_TX_BUFF - 1)
-		ctrl |= MAL_TX_CTRL_WRAP;
+	if (unlikely(slot_prev == NUM_TX_BUFF - 1)) ctrl |= MAL_TX_CTRL_WRAP;
 	wmb();
-	dev->tx_desc[dev->tx_slot].ctrl = ctrl;
-	dev->tx_slot = (slot + 1) % NUM_TX_BUFF;
+	dev->tx_desc[slot_prev].ctrl = ctrl;
+	dev->tx_slot = NXT_TX_SLOT(slot);
 
-	return emac_xmit_finish(dev, skb->len);
+	return emac_xmit_finish(dev, ndev, skb->len);
 
  undo_frame:
-	/* Well, too bad. Our previous estimation was overly optimistic.
-	 * Undo everything.
-	 */
+	/* Well, too bad. Our previous estimation was overly optimistic. Undo everything */
 	while (slot != dev->tx_slot) {
-		dev->tx_desc[slot].ctrl = 0;
+		dev->tx_desc[slot].ctrl = 0 | (slot == (NUM_TX_BUFF - 1) ? MAL_TX_CTRL_WRAP : 0);
 		--dev->tx_cnt;
-		if (--slot < 0)
-			slot = NUM_TX_BUFF - 1;
+		if (--slot < 0)	slot = NUM_TX_BUFF - 1;
 	}
 	++dev->estats.tx_undo;
 
@@ -1577,240 +1419,239 @@
 	return NETDEV_TX_BUSY;
 }
 
+#if !defined (CONFIG_APM821xx) // Moved to mal.c
 /* Tx lock BHs */
-static void emac_parse_tx_error(struct emac_instance *dev, u16 ctrl)
-{
+static void emac_parse_tx_error(struct emac_instance *dev, u16 ctrl) {
 	struct emac_error_stats *st = &dev->estats;
 
 	DBG(dev, "BD TX error %04x" NL, ctrl);
 
 	++st->tx_bd_errors;
-	if (ctrl & EMAC_TX_ST_BFCS)
-		++st->tx_bd_bad_fcs;
-	if (ctrl & EMAC_TX_ST_LCS)
-		++st->tx_bd_carrier_loss;
-	if (ctrl & EMAC_TX_ST_ED)
-		++st->tx_bd_excessive_deferral;
-	if (ctrl & EMAC_TX_ST_EC)
-		++st->tx_bd_excessive_collisions;
-	if (ctrl & EMAC_TX_ST_LC)
-		++st->tx_bd_late_collision;
-	if (ctrl & EMAC_TX_ST_MC)
-		++st->tx_bd_multple_collisions;
-	if (ctrl & EMAC_TX_ST_SC)
-		++st->tx_bd_single_collision;
-	if (ctrl & EMAC_TX_ST_UR)
-		++st->tx_bd_underrun;
-	if (ctrl & EMAC_TX_ST_SQE)
-		++st->tx_bd_sqe;
+	if (ctrl & EMAC_TX_ST_BFCS)	++st->tx_bd_bad_fcs;
+	if (ctrl & EMAC_TX_ST_LCS)	++st->tx_bd_carrier_loss;
+	if (ctrl & EMAC_TX_ST_ED)	++st->tx_bd_excessive_deferral;
+	if (ctrl & EMAC_TX_ST_EC)	++st->tx_bd_excessive_collisions;
+	if (ctrl & EMAC_TX_ST_LC)	++st->tx_bd_late_collision;
+	if (ctrl & EMAC_TX_ST_MC)	++st->tx_bd_multple_collisions;
+	if (ctrl & EMAC_TX_ST_SC)	++st->tx_bd_single_collision;
+	if (ctrl & EMAC_TX_ST_UR)	++st->tx_bd_underrun;
+	if (ctrl & EMAC_TX_ST_SQE)	++st->tx_bd_sqe;
 }
 
-static void emac_poll_tx(void *param)
-{
-	struct emac_instance *dev = param;
-	u32 bad_mask;
+static void emac_poll_tx(void *param) {
+	register struct emac_instance *dev = param;
 
 	DBG2(dev, "poll_tx, %d %d" NL, dev->tx_cnt, dev->ack_slot);
 
-	if (emac_has_feature(dev, EMAC_FTR_HAS_TAH))
-		bad_mask = EMAC_IS_BAD_TX_TAH;
-	else
-		bad_mask = EMAC_IS_BAD_TX;
-
 	netif_tx_lock_bh(dev->ndev);
-	if (dev->tx_cnt) {
-		u16 ctrl;
-		int slot = dev->ack_slot, n = 0;
+	if (dev->tx_cnt) {	// We have something to transmit
+		register u16 ctrl;
+		register int slot = dev->ack_slot, n = 0;
 	again:
 		ctrl = dev->tx_desc[slot].ctrl;
-		if (!(ctrl & MAL_TX_CTRL_READY)) {
+		if (likely(!(ctrl & MAL_TX_CTRL_READY))) {
 			struct sk_buff *skb = dev->tx_skb[slot];
 			++n;
 
-			if (skb) {
+			if (likely(skb)) {
 				dev_kfree_skb(skb);
 				dev->tx_skb[slot] = NULL;
 			}
-			slot = (slot + 1) % NUM_TX_BUFF;
-
-			if (unlikely(ctrl & bad_mask))
-				emac_parse_tx_error(dev, ctrl);
+			slot = NXT_TX_SLOT(slot);
+			if (unlikely(ctrl & EMAC_IS_BAD_TX)) emac_parse_tx_error(dev, ctrl);
+			if (--dev->tx_cnt) goto again;
+
+#ifdef CONFIG_IBM_EMAC_MASK_CEXT
+			else {
+				DBG(dev, "Testing for idle... " NL);
+				if (atomic_read(&dev->mask_cext_enable)) {
+					if (!atomic_read(&dev->idle_mode)) {
+						DBG(dev, "Entering idle mode" NL);
+						emac_start_idlemode(dev);
+						atomic_set(&dev->idle_mode, 1);
+					} else DBG(dev, "Already In Idle Mode" NL);
+				}
+			}
+#endif
+		} 
 
-			if (--dev->tx_cnt)
-				goto again;
-		}
-		if (n) {
+		if (likely(n)) {
 			dev->ack_slot = slot;
-			if (netif_queue_stopped(dev->ndev) &&
-			    dev->tx_cnt < EMAC_TX_WAKEUP_THRESH)
+			if (netif_queue_stopped(dev->ndev) && dev->tx_cnt < EMAC_TX_WAKEUP_THRESH)
 				netif_wake_queue(dev->ndev);
 
 			DBG2(dev, "tx %d pkts" NL, n);
 		}
 	}
+#ifdef CONFIG_IBM_EMAC_MASK_CEXT
+	else {
+		DBG(dev, "Testing for idle... " NL);
+		if (atomic_read(&dev->mask_cext_enable)) {
+			if (!atomic_read(&dev->idle_mode)) {
+			      DBG(dev, "Entering idle mode" NL);
+			      emac_start_idlemode(dev);
+			      atomic_set(&dev->idle_mode, 1);
+			} else DBG(dev, "Already In Idle Mode" NL);
+		}
+	}
+#endif
 	netif_tx_unlock_bh(dev->ndev);
 }
 
-static inline void emac_recycle_rx_skb(struct emac_instance *dev, int slot,
-				       int len)
-{
-	struct sk_buff *skb = dev->rx_skb[slot];
-
-	DBG2(dev, "recycle %d %d" NL, slot, len);
+/*
+#define EMAC_RECYCLE_RX_SKB(dev, slot, len, skbdata) \
+	if (len) dma_map_single(&dev->ofdev->dev, skbdata-2, EMAC_DMA_ALIGN(len+2), DMA_FROM_DEVICE);\
+	dev->rx_desc[slot].data_len = 0;\
+	wmb();\
+	dev->rx_desc[slot].ctrl = MAL_RX_CTRL_EMPTY | MAL_RX_CTRL_INTR |(slot == (NUM_RX_BUFF-1) ? MAL_RX_CTRL_WRAP:0);
+*/
 
-	if (len)
-		dma_map_single(&dev->ofdev->dev, skb->data - 2,
-			       EMAC_DMA_ALIGN(len + 2), DMA_FROM_DEVICE);
+static __always_inline void emac_recycle_rx_skb(struct emac_instance *dev, int slot, int len, unsigned char* skbdata) {
+	//struct sk_buff *skb = dev->rx_skb[slot];
 
+	DBG2(dev, "recycle %d %d" NL, slot, len);
+	if (likely(len))
+		dma_map_single(dev->ofdev_dev, skbdata - NET_IP_ALIGN, SKB_DATA_ALIGN(len + NET_IP_ALIGN), DMA_FROM_DEVICE);
 	dev->rx_desc[slot].data_len = 0;
 	wmb();
-	dev->rx_desc[slot].ctrl = MAL_RX_CTRL_EMPTY |
-	    (slot == (NUM_RX_BUFF - 1) ? MAL_RX_CTRL_WRAP : 0);
+	dev->rx_desc[slot].ctrl = MAL_RX_CTRL_EMPTY| MAL_RX_CTRL_INTR| (slot == (NUM_RX_BUFF-1) ? MAL_RX_CTRL_WRAP : 0);
 }
 
-static void emac_parse_rx_error(struct emac_instance *dev, u16 ctrl)
-{
+static void emac_parse_rx_error(struct emac_instance *dev, u16 ctrl) {
 	struct emac_error_stats *st = &dev->estats;
 
 	DBG(dev, "BD RX error %04x" NL, ctrl);
 
 	++st->rx_bd_errors;
-	if (ctrl & EMAC_RX_ST_OE)
-		++st->rx_bd_overrun;
-	if (ctrl & EMAC_RX_ST_BP)
-		++st->rx_bd_bad_packet;
-	if (ctrl & EMAC_RX_ST_RP)
-		++st->rx_bd_runt_packet;
-	if (ctrl & EMAC_RX_ST_SE)
-		++st->rx_bd_short_event;
-	if (ctrl & EMAC_RX_ST_AE)
-		++st->rx_bd_alignment_error;
-	if (ctrl & EMAC_RX_ST_BFCS)
-		++st->rx_bd_bad_fcs;
-	if (ctrl & EMAC_RX_ST_PTL)
-		++st->rx_bd_packet_too_long;
-	if (ctrl & EMAC_RX_ST_ORE)
-		++st->rx_bd_out_of_range;
-	if (ctrl & EMAC_RX_ST_IRE)
-		++st->rx_bd_in_range;
+	if (ctrl & EMAC_RX_ST_OE)	++st->rx_bd_overrun;
+	if (ctrl & EMAC_RX_ST_BP)	++st->rx_bd_bad_packet;
+	if (ctrl & EMAC_RX_ST_RP)	++st->rx_bd_runt_packet;
+	if (ctrl & EMAC_RX_ST_SE)	++st->rx_bd_short_event;
+	if (ctrl & EMAC_RX_ST_AE)	++st->rx_bd_alignment_error;
+	if (ctrl & EMAC_RX_ST_BFCS)	++st->rx_bd_bad_fcs;
+	if (ctrl & EMAC_RX_ST_PTL)	++st->rx_bd_packet_too_long;
+	if (ctrl & EMAC_RX_ST_ORE)	++st->rx_bd_out_of_range;
+	if (ctrl & EMAC_RX_ST_IRE)	++st->rx_bd_in_range;
 }
 
-static inline void emac_rx_csum(struct emac_instance *dev,
-				struct sk_buff *skb, u16 ctrl)
-{
+static __always_inline void emac_rx_csum(struct emac_instance *dev, struct sk_buff *skb, u16 ctrl) {
 #ifdef CONFIG_IBM_EMAC_TAH
-	if (!ctrl && dev->tah_dev) {
+	//if (!ctrl && dev->tah_dev) {
+	if (likely(!ctrl)) {
 		skb->ip_summed = CHECKSUM_UNNECESSARY;
 		++dev->stats.rx_packets_csum;
 	}
 #endif
 }
 
-static inline int emac_rx_sg_append(struct emac_instance *dev, int slot)
-{
-	if (likely(dev->rx_sg_skb != NULL)) {
-		int len = dev->rx_desc[slot].data_len;
-		int tot_len = dev->rx_sg_skb->len + len;
+/* ECO: local version of skb_put for performance reasons */
+#define skb_tail_pointer(skb) (skb)->tail
+#define skb_putl(skb, len) SKB_LINEAR_ASSERT(skb); skb->tail += len; skb->len  += len;
+
+// emac_rx_sg_append was returning 0 and -1, used in ONLY in NOT comparisons --> invert return values 
+static __always_inline int emac_rx_sg_append(struct emac_instance *dev, int slot, int len, 
+    unsigned char *data, struct sk_buff **skb_sg) {
+    struct sk_buff *skb_sgp = *skb_sg;
+	if (likely(skb_sgp != NULL)) {
+		//int len = dev->rx_desc[slot].data_len;
+		int tot_len = skb_sgp->len + len;
 
-		if (unlikely(tot_len + 2 > dev->rx_skb_size)) {
+		if (unlikely(tot_len + NET_IP_ALIGN > dev->rx_skb_size)) {
 			++dev->estats.rx_dropped_mtu;
-			dev_kfree_skb(dev->rx_sg_skb);
-			dev->rx_sg_skb = NULL;
-		} else {
-			memcpy(skb_tail_pointer(dev->rx_sg_skb),
-					 dev->rx_skb[slot]->data, len);
-			skb_put(dev->rx_sg_skb, len);
-			emac_recycle_rx_skb(dev, slot, len);
-			return 0;
+			dev_kfree_skb(skb_sgp);
+            skb_sg = NULL;
+		} else {  //ECO
+			//if(unlikely((dev->rx_sg_skb->tail + len) > dev->rx_sg_skb->end)) goto out;
+			memcpy(skb_tail_pointer(skb_sgp), data, len);	// dev->rx_skb[slot]->data
+			skb_putl(skb_sgp, len);
+			emac_recycle_rx_skb(dev, slot, len, data);	//dev->rx_skb[slot]->data
+			return 1;
 		}
 	}
-	emac_recycle_rx_skb(dev, slot, 0);
-	return -1;
+	emac_recycle_rx_skb(dev, slot, 0, dev->rx_skb[slot]->data);
+	return 0;
 }
 
 /* NAPI poll context */
-static int emac_poll_rx(void *param, int budget)
-{
-	struct emac_instance *dev = param;
-	int slot = dev->rx_slot, received = 0;
+static int emac_poll_rx(void *param, int budget) {
+	register struct emac_instance *dev = param;
+	register int slot = dev->rx_slot, received = 0;
+	register int len;
+	register struct sk_buff *skb, **skb_sg;
 
 	DBG2(dev, "poll_rx(%d)" NL, budget);
 
  again:
 	while (budget > 0) {
-		int len;
-		struct sk_buff *skb;
-		u16 ctrl = dev->rx_desc[slot].ctrl;
+		register u16 ctrl = dev->rx_desc[slot].ctrl;
 
-		if (ctrl & MAL_RX_CTRL_EMPTY)
-			break;
+		if (unlikely(ctrl & MAL_RX_CTRL_EMPTY)) break;
 
 		skb = dev->rx_skb[slot];
 		mb();
 		len = dev->rx_desc[slot].data_len;
 
-		if (unlikely(!MAL_IS_SINGLE_RX(ctrl)))
-			goto sg;
+		if (unlikely(!MAL_IS_SINGLE_RX(ctrl))) goto sg;
 
 		ctrl &= EMAC_BAD_RX_MASK;
 		if (unlikely(ctrl && ctrl != EMAC_RX_TAH_BAD_CSUM)) {
 			emac_parse_rx_error(dev, ctrl);
 			++dev->estats.rx_dropped_error;
-			emac_recycle_rx_skb(dev, slot, 0);
+			emac_recycle_rx_skb(dev, slot, 0, skb->data);
 			len = 0;
 			goto next;
 		}
 
-		if (len < ETH_HLEN) {
+		if (unlikely(len < ETH_HLEN)) {
 			++dev->estats.rx_dropped_stack;
-			emac_recycle_rx_skb(dev, slot, len);
+			emac_recycle_rx_skb(dev, slot, len, skb->data);
 			goto next;
 		}
 
-		if (len && len < EMAC_RX_COPY_THRESH) {
-			struct sk_buff *copy_skb =
-			    alloc_skb(len + EMAC_RX_SKB_HEADROOM + 2, GFP_ATOMIC);
-			if (unlikely(!copy_skb))
-				goto oom;
-
-			skb_reserve(copy_skb, EMAC_RX_SKB_HEADROOM + 2);
-			memcpy(copy_skb->data - 2, skb->data - 2, len + 2);
-			emac_recycle_rx_skb(dev, slot, len);
+		if (len < EMAC_RX_COPY_THRESH) {
+			struct sk_buff *copy_skb = napi_alloc_skb(&dev->mal->napi, len);
+			if (unlikely(!copy_skb)) goto oom;
+
+			memcpy(copy_skb->data - NET_IP_ALIGN, skb->data - NET_IP_ALIGN, len + NET_IP_ALIGN);  //is cacheable_memcpy
+			emac_recycle_rx_skb(dev, slot, len, skb->data);
 			skb = copy_skb;
-		} else if (unlikely(emac_alloc_rx_skb(dev, slot, GFP_ATOMIC)))
-			goto oom;
+		} else if (unlikely(alloc_rx_skb_napi(dev, slot))) goto oom;
 
-		skb_put(skb, len);
+		skb_putl(skb, len); //ECO: local version of skb_put
 	push_packet:
 		skb->protocol = eth_type_trans(skb, dev->ndev);
 		emac_rx_csum(dev, skb, ctrl);
 
-		if (unlikely(netif_receive_skb(skb) == NET_RX_DROP))
-			++dev->estats.rx_dropped_stack;
+		if (unlikely(netif_receive_skb(skb) == NET_RX_DROP)) ++dev->estats.rx_dropped_stack;
 	next:
 		++dev->stats.rx_packets;
 	skip:
 		dev->stats.rx_bytes += len;
-		slot = (slot + 1) % NUM_RX_BUFF;
+		slot = NXT_RX_SLOT(slot);
 		--budget;
 		++received;
 		continue;
 	sg:
+        skb_sg = &dev->rx_sg_skb;
+        // ECO TODO test if unlikely
 		if (ctrl & MAL_RX_CTRL_FIRST) {
-			BUG_ON(dev->rx_sg_skb);
-			if (unlikely(emac_alloc_rx_skb(dev, slot, GFP_ATOMIC))) {
+			//BUG_ON(dev->rx_sg_skb); // panic is not really helpful here
+			if (unlikely(alloc_rx_skb_napi(dev, slot))) {
 				DBG(dev, "rx OOM %d" NL, slot);
 				++dev->estats.rx_dropped_oom;
-				emac_recycle_rx_skb(dev, slot, 0);
+				emac_recycle_rx_skb(dev, slot, 0, skb->data);
+                goto skip;
 			} else {
-				dev->rx_sg_skb = skb;
-				skb_put(skb, len);
+				//dev->rx_sg_skb = skb;
+                *skb_sg = skb;
+				skb_putl(skb, len);
+                goto skip;
 			}
-		} else if (!emac_rx_sg_append(dev, slot) &&
-			   (ctrl & MAL_RX_CTRL_LAST)) {
-
-			skb = dev->rx_sg_skb;
-			dev->rx_sg_skb = NULL;
+		} else if (emac_rx_sg_append(dev, slot, len, skb->data, skb_sg) && (ctrl & MAL_RX_CTRL_LAST)) {
+			//skb = dev->rx_sg_skb;
+            //dev->rx_sg_skb = NULL;
+            skb = *skb_sg;
+            *skb_sg = NULL; 			
 
 			ctrl &= EMAC_BAD_RX_MASK;
 			if (unlikely(ctrl && ctrl != EMAC_RX_TAH_BAD_CSUM)) {
@@ -1818,15 +1659,14 @@
 				++dev->estats.rx_dropped_error;
 				dev_kfree_skb(skb);
 				len = 0;
-			} else
-				goto push_packet;
+			} else goto push_packet;
 		}
 		goto skip;
 	oom:
 		DBG(dev, "rx OOM %d" NL, slot);
 		/* Drop the packet and recycle skb */
 		++dev->estats.rx_dropped_oom;
-		emac_recycle_rx_skb(dev, slot, 0);
+		emac_recycle_rx_skb(dev, slot, 0, skb->data);
 		goto next;
 	}
 
@@ -1859,47 +1699,36 @@
 }
 
 /* NAPI poll context */
-static int emac_peek_rx(void *param)
-{
+static int emac_peek_rx(void *param) {
 	struct emac_instance *dev = param;
-
 	return !(dev->rx_desc[dev->rx_slot].ctrl & MAL_RX_CTRL_EMPTY);
 }
 
 /* NAPI poll context */
-static int emac_peek_rx_sg(void *param)
-{
+static int emac_peek_rx_sg(void *param) {
 	struct emac_instance *dev = param;
-
-	int slot = dev->rx_slot;
+	register int slot = dev->rx_slot;  // start with last used slot
+    register u16 *rx_desc_ctrl = (u16 *)&dev->rx_desc[slot];
 	while (1) {
-		u16 ctrl = dev->rx_desc[slot].ctrl;
-		if (ctrl & MAL_RX_CTRL_EMPTY)
-			return 0;
-		else if (ctrl & MAL_RX_CTRL_LAST)
-			return 1;
-
-		slot = (slot + 1) % NUM_RX_BUFF;
-
-		/* I'm just being paranoid here :) */
-		if (unlikely(slot == dev->rx_slot))
-			return 0;
+        if (*rx_desc_ctrl & MAL_RX_CTRL_EMPTY) return 0;
+		if (unlikely(*rx_desc_ctrl & MAL_RX_CTRL_LAST)) return 1;
+		slot = NXT_RX_SLOT(slot);
+        rx_desc_ctrl += sizeof(struct mal_descriptor);
+        if (unlikely(slot == 0)) rx_desc_ctrl = (u16 *) &dev->rx_desc[0]; //wrap around
 	}
 }
+#endif // Moved to mal.c
 
 /* Hard IRQ */
-static void emac_rxde(void *param)
-{
+static void emac_rxde(void *param) {
 	struct emac_instance *dev = param;
-
 	++dev->estats.rx_stopped;
 	emac_rx_disable_async(dev);
 }
 
 /* Hard IRQ */
-static irqreturn_t emac_irq(int irq, void *dev_instance)
-{
-	struct emac_instance *dev = dev_instance;
+static irqreturn_t emac_irq(int irq, void *dev_instance) {
+ 	struct emac_instance *dev = dev_instance;
 	struct emac_regs __iomem *p = dev->emacp;
 	struct emac_error_stats *st = &dev->estats;
 	u32 isr;
@@ -1911,44 +1740,30 @@
 
 	DBG(dev, "isr = %08x" NL, isr);
 
-	if (isr & EMAC4_ISR_TXPE)
-		++st->tx_parity;
-	if (isr & EMAC4_ISR_RXPE)
-		++st->rx_parity;
-	if (isr & EMAC4_ISR_TXUE)
-		++st->tx_underrun;
-	if (isr & EMAC4_ISR_RXOE)
-		++st->rx_fifo_overrun;
-	if (isr & EMAC_ISR_OVR)
-		++st->rx_overrun;
-	if (isr & EMAC_ISR_BP)
-		++st->rx_bad_packet;
-	if (isr & EMAC_ISR_RP)
-		++st->rx_runt_packet;
-	if (isr & EMAC_ISR_SE)
-		++st->rx_short_event;
-	if (isr & EMAC_ISR_ALE)
-		++st->rx_alignment_error;
-	if (isr & EMAC_ISR_BFCS)
-		++st->rx_bad_fcs;
-	if (isr & EMAC_ISR_PTLE)
-		++st->rx_packet_too_long;
-	if (isr & EMAC_ISR_ORE)
-		++st->rx_out_of_range;
-	if (isr & EMAC_ISR_IRE)
-		++st->rx_in_range;
-	if (isr & EMAC_ISR_SQE)
-		++st->tx_sqe;
-	if (isr & EMAC_ISR_TE)
-		++st->tx_errors;
-
+ 	if (isr & EMAC4_ISR_TXPE)	++st->tx_parity;
+	if (isr & EMAC4_ISR_RXPE)	++st->rx_parity;
+	if (isr & EMAC4_ISR_TXUE)	++st->tx_underrun;
+	if (isr & EMAC4_ISR_RXOE)	++st->rx_fifo_overrun;
+	if (isr & EMAC_ISR_OVR)		++st->rx_overrun;
+	if (isr & EMAC_ISR_BP)		++st->rx_bad_packet;
+	if (isr & EMAC_ISR_RP)		++st->rx_runt_packet;
+	if (isr & EMAC_ISR_SE)		++st->rx_short_event;
+	if (isr & EMAC_ISR_ALE)		++st->rx_alignment_error;
+	if (isr & EMAC_ISR_BFCS)	++st->rx_bad_fcs;
+	if (isr & EMAC_ISR_PTLE)	++st->rx_packet_too_long;
+	if (isr & EMAC_ISR_ORE)		++st->rx_out_of_range;
+	if (isr & EMAC_ISR_IRE)		++st->rx_in_range;
+	if (isr & EMAC_ISR_SQE)		++st->tx_sqe;
+	if (isr & EMAC_ISR_TE)		++st->tx_errors;
 	spin_unlock(&dev->lock);
+	return IRQ_HANDLED;
+}
 
+static irqreturn_t wol_irq(int irq, void *dev_instance) {
 	return IRQ_HANDLED;
 }
 
-static struct net_device_stats *emac_stats(struct net_device *ndev)
-{
+static struct net_device_stats *emac_stats(struct net_device *ndev) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	struct emac_stats *st = &dev->stats;
 	struct emac_error_stats *est = &dev->estats;
@@ -1963,61 +1778,48 @@
 	nst->rx_bytes = (unsigned long)st->rx_bytes;
 	nst->tx_packets = (unsigned long)st->tx_packets;
 	nst->tx_bytes = (unsigned long)st->tx_bytes;
-	nst->rx_dropped = (unsigned long)(est->rx_dropped_oom +
-					  est->rx_dropped_error +
-					  est->rx_dropped_resize +
-					  est->rx_dropped_mtu);
+	nst->rx_dropped = (unsigned long)(est->rx_dropped_oom + est->rx_dropped_error +
+		est->rx_dropped_resize + est->rx_dropped_mtu);
 	nst->tx_dropped = (unsigned long)est->tx_dropped;
 
 	nst->rx_errors = (unsigned long)est->rx_bd_errors;
-	nst->rx_fifo_errors = (unsigned long)(est->rx_bd_overrun +
-					      est->rx_fifo_overrun +
-					      est->rx_overrun);
-	nst->rx_frame_errors = (unsigned long)(est->rx_bd_alignment_error +
-					       est->rx_alignment_error);
-	nst->rx_crc_errors = (unsigned long)(est->rx_bd_bad_fcs +
-					     est->rx_bad_fcs);
-	nst->rx_length_errors = (unsigned long)(est->rx_bd_runt_packet +
-						est->rx_bd_short_event +
-						est->rx_bd_packet_too_long +
-						est->rx_bd_out_of_range +
-						est->rx_bd_in_range +
-						est->rx_runt_packet +
-						est->rx_short_event +
-						est->rx_packet_too_long +
-						est->rx_out_of_range +
-						est->rx_in_range);
+	nst->rx_fifo_errors = (unsigned long)(est->rx_bd_overrun + est->rx_fifo_overrun + est->rx_overrun);
+	nst->rx_frame_errors = (unsigned long)(est->rx_bd_alignment_error + est->rx_alignment_error);
+	nst->rx_crc_errors = (unsigned long)(est->rx_bd_bad_fcs + est->rx_bad_fcs);
+	nst->rx_length_errors = (unsigned long)(est->rx_bd_runt_packet + est->rx_bd_short_event +
+		est->rx_bd_packet_too_long + est->rx_bd_out_of_range + est->rx_bd_in_range +
+		est->rx_runt_packet + est->rx_short_event + est->rx_packet_too_long +
+		est->rx_out_of_range + est->rx_in_range);
 
 	nst->tx_errors = (unsigned long)(est->tx_bd_errors + est->tx_errors);
-	nst->tx_fifo_errors = (unsigned long)(est->tx_bd_underrun +
-					      est->tx_underrun);
+	nst->tx_fifo_errors = (unsigned long)(est->tx_bd_underrun + est->tx_underrun);
 	nst->tx_carrier_errors = (unsigned long)est->tx_bd_carrier_loss;
 	nst->collisions = (unsigned long)(est->tx_bd_excessive_deferral +
-					  est->tx_bd_excessive_collisions +
-					  est->tx_bd_late_collision +
-					  est->tx_bd_multple_collisions);
+		est->tx_bd_excessive_collisions + est->tx_bd_late_collision + est->tx_bd_multple_collisions);
 	spin_unlock_irqrestore(&dev->lock, flags);
 	return nst;
 }
 
 static struct mal_commac_ops emac_commac_ops = {
+#if !defined (CONFIG_APM821xx) // Moved to mal.c
 	.poll_tx = &emac_poll_tx,
 	.poll_rx = &emac_poll_rx,
 	.peek_rx = &emac_peek_rx,
+#endif
 	.rxde = &emac_rxde,
 };
 
 static struct mal_commac_ops emac_commac_sg_ops = {
+#if !defined (CONFIG_APM821xx) // Moved to mal.c
 	.poll_tx = &emac_poll_tx,
 	.poll_rx = &emac_poll_rx,
 	.peek_rx = &emac_peek_rx_sg,
+#endif
 	.rxde = &emac_rxde,
 };
 
 /* Ethtool support */
-static int emac_ethtool_get_link_ksettings(struct net_device *ndev,
-					   struct ethtool_link_ksettings *cmd)
-{
+static int emac_ethtool_get_link_ksettings(struct net_device *ndev, struct ethtool_link_ksettings *cmd) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	u32 supported, advertising;
 
@@ -2032,84 +1834,54 @@
 	cmd->base.duplex = dev->phy.duplex;
 	mutex_unlock(&dev->link_lock);
 
-	ethtool_convert_legacy_u32_to_link_mode(cmd->link_modes.supported,
-						supported);
-	ethtool_convert_legacy_u32_to_link_mode(cmd->link_modes.advertising,
-						advertising);
+	ethtool_convert_legacy_u32_to_link_mode(cmd->link_modes.supported, supported);
+	ethtool_convert_legacy_u32_to_link_mode(cmd->link_modes.advertising, advertising);
 
 	return 0;
 }
 
-static int
-emac_ethtool_set_link_ksettings(struct net_device *ndev,
-				const struct ethtool_link_ksettings *cmd)
-{
+static int emac_ethtool_set_link_ksettings(struct net_device *ndev, const struct ethtool_link_ksettings *cmd) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	u32 f = dev->phy.features;
 	u32 advertising;
 
-	ethtool_convert_link_mode_to_legacy_u32(&advertising,
-						cmd->link_modes.advertising);
+	ethtool_convert_link_mode_to_legacy_u32(&advertising, cmd->link_modes.advertising);
 
 	DBG(dev, "set_settings(%d, %d, %d, 0x%08x)" NL,
 	    cmd->base.autoneg, cmd->base.speed, cmd->base.duplex, advertising);
 
 	/* Basic sanity checks */
-	if (dev->phy.address < 0)
-		return -EOPNOTSUPP;
-	if (cmd->base.autoneg != AUTONEG_ENABLE &&
-	    cmd->base.autoneg != AUTONEG_DISABLE)
-		return -EINVAL;
-	if (cmd->base.autoneg == AUTONEG_ENABLE && advertising == 0)
-		return -EINVAL;
-	if (cmd->base.duplex != DUPLEX_HALF && cmd->base.duplex != DUPLEX_FULL)
-		return -EINVAL;
+	if (dev->phy.address < 0) return -EOPNOTSUPP;
+	if (cmd->base.autoneg != AUTONEG_ENABLE && cmd->base.autoneg != AUTONEG_DISABLE) return -EINVAL;
+	if (cmd->base.autoneg == AUTONEG_ENABLE && advertising == 0) return -EINVAL;
+	if (cmd->base.duplex != DUPLEX_HALF && cmd->base.duplex != DUPLEX_FULL)	return -EINVAL;
 
 	if (cmd->base.autoneg == AUTONEG_DISABLE) {
 		switch (cmd->base.speed) {
-		case SPEED_10:
-			if (cmd->base.duplex == DUPLEX_HALF &&
-			    !(f & SUPPORTED_10baseT_Half))
-				return -EINVAL;
-			if (cmd->base.duplex == DUPLEX_FULL &&
-			    !(f & SUPPORTED_10baseT_Full))
-				return -EINVAL;
+		case SPEED_1000:
+			if (cmd->base.duplex == DUPLEX_HALF && !(f & SUPPORTED_1000baseT_Half)) return -EINVAL;
+			if (cmd->base.duplex == DUPLEX_FULL && !(f & SUPPORTED_1000baseT_Full)) return -EINVAL;
 			break;
 		case SPEED_100:
-			if (cmd->base.duplex == DUPLEX_HALF &&
-			    !(f & SUPPORTED_100baseT_Half))
-				return -EINVAL;
-			if (cmd->base.duplex == DUPLEX_FULL &&
-			    !(f & SUPPORTED_100baseT_Full))
-				return -EINVAL;
+			if (cmd->base.duplex == DUPLEX_HALF && !(f & SUPPORTED_100baseT_Half)) return -EINVAL;
+			if (cmd->base.duplex == DUPLEX_FULL && !(f & SUPPORTED_100baseT_Full)) return -EINVAL;
 			break;
-		case SPEED_1000:
-			if (cmd->base.duplex == DUPLEX_HALF &&
-			    !(f & SUPPORTED_1000baseT_Half))
-				return -EINVAL;
-			if (cmd->base.duplex == DUPLEX_FULL &&
-			    !(f & SUPPORTED_1000baseT_Full))
-				return -EINVAL;
+		case SPEED_10:
+			if (cmd->base.duplex == DUPLEX_HALF && !(f & SUPPORTED_10baseT_Half)) return -EINVAL;
+			if (cmd->base.duplex == DUPLEX_FULL && !(f & SUPPORTED_10baseT_Full)) return -EINVAL;
 			break;
-		default:
-			return -EINVAL;
+		default: return -EINVAL;
 		}
 
 		mutex_lock(&dev->link_lock);
-		dev->phy.def->ops->setup_forced(&dev->phy, cmd->base.speed,
-						cmd->base.duplex);
+		dev->phy.def->ops->setup_forced(&dev->phy, cmd->base.speed, cmd->base.duplex);
 		mutex_unlock(&dev->link_lock);
 
 	} else {
-		if (!(f & SUPPORTED_Autoneg))
-			return -EINVAL;
-
+		if (!(f & SUPPORTED_Autoneg)) return -EINVAL;
 		mutex_lock(&dev->link_lock);
-		dev->phy.def->ops->setup_aneg(&dev->phy,
-					      (advertising & f) |
-					      (dev->phy.advertising &
-					       (ADVERTISED_Pause |
-						ADVERTISED_Asym_Pause)));
+		dev->phy.def->ops->setup_aneg(&dev->phy, (advertising & f) | 
+			(dev->phy.advertising & (ADVERTISED_Pause | ADVERTISED_Asym_Pause)));
 		mutex_unlock(&dev->link_lock);
 	}
 	emac_force_link_update(dev);
@@ -2117,16 +1889,12 @@
 	return 0;
 }
 
-static void emac_ethtool_get_ringparam(struct net_device *ndev,
-				       struct ethtool_ringparam *rp)
-{
+static void emac_ethtool_get_ringparam(struct net_device *ndev, struct ethtool_ringparam *rp) {
 	rp->rx_max_pending = rp->rx_pending = NUM_RX_BUFF;
 	rp->tx_max_pending = rp->tx_pending = NUM_TX_BUFF;
 }
 
-static void emac_ethtool_get_pauseparam(struct net_device *ndev,
-					struct ethtool_pauseparam *pp)
-{
+static void emac_ethtool_get_pauseparam(struct net_device *ndev, struct ethtool_pauseparam *pp) {
 	struct emac_instance *dev = netdev_priv(ndev);
 
 	mutex_lock(&dev->link_lock);
@@ -2135,56 +1903,41 @@
 		pp->autoneg = 1;
 
 	if (dev->phy.duplex == DUPLEX_FULL) {
-		if (dev->phy.pause)
-			pp->rx_pause = pp->tx_pause = 1;
-		else if (dev->phy.asym_pause)
-			pp->tx_pause = 1;
+		if (dev->phy.pause)	pp->rx_pause = pp->tx_pause = 1;
+		else if (dev->phy.asym_pause) pp->tx_pause = 1;
 	}
 	mutex_unlock(&dev->link_lock);
 }
 
-static int emac_get_regs_len(struct emac_instance *dev)
-{
-		return sizeof(struct emac_ethtool_regs_subhdr) +
-			sizeof(struct emac_regs);
+static int emac_get_regs_len(struct emac_instance *dev) {
+		return sizeof(struct emac_ethtool_regs_subhdr) + sizeof(struct emac_regs);
 }
 
-static int emac_ethtool_get_regs_len(struct net_device *ndev)
-{
+static int emac_ethtool_get_regs_len(struct net_device *ndev) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	int size;
 
 	size = sizeof(struct emac_ethtool_regs_hdr) +
 		emac_get_regs_len(dev) + mal_get_regs_len(dev->mal);
-	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII))
-		size += zmii_get_regs_len(dev->zmii_dev);
-	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII))
-		size += rgmii_get_regs_len(dev->rgmii_dev);
-	if (emac_has_feature(dev, EMAC_FTR_HAS_TAH))
-		size += tah_get_regs_len(dev->tah_dev);
-
+	if (likely(emac_has_feature(dev, EMAC_FTR_HAS_RGMII))) size += rgmii_get_regs_len(dev->rgmii_dev);
+	else if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII)) size += zmii_get_regs_len(dev->zmii_dev);
+	if (likely(emac_has_feature(dev, EMAC_FTR_HAS_TAH))) size += tah_get_regs_len(dev->tah_dev);
 	return size;
 }
 
-static void *emac_dump_regs(struct emac_instance *dev, void *buf)
-{
+static void *emac_dump_regs(struct emac_instance *dev, void *buf) {
 	struct emac_ethtool_regs_subhdr *hdr = buf;
 
 	hdr->index = dev->cell_index;
-	if (emac_has_feature(dev, EMAC_FTR_EMAC4SYNC)) {
-		hdr->version = EMAC4SYNC_ETHTOOL_REGS_VER;
-	} else if (emac_has_feature(dev, EMAC_FTR_EMAC4)) {
-		hdr->version = EMAC4_ETHTOOL_REGS_VER;
-	} else {
-		hdr->version = EMAC_ETHTOOL_REGS_VER;
-	}
+	if (emac_has_feature(dev, EMAC_FTR_EMAC4SYNC))	hdr->version = EMAC4SYNC_ETHTOOL_REGS_VER;
+	else if (emac_has_feature(dev, EMAC_FTR_EMAC4)) hdr->version = EMAC4_ETHTOOL_REGS_VER;
+	else hdr->version = EMAC_ETHTOOL_REGS_VER;
+
 	memcpy_fromio(hdr + 1, dev->emacp, sizeof(struct emac_regs));
 	return (void *)(hdr + 1) + sizeof(struct emac_regs);
 }
 
-static void emac_ethtool_get_regs(struct net_device *ndev,
-				  struct ethtool_regs *regs, void *buf)
-{
+static void emac_ethtool_get_regs(struct net_device *ndev, struct ethtool_regs *regs, void *buf) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	struct emac_ethtool_regs_hdr *hdr = buf;
 
@@ -2207,15 +1960,12 @@
 	}
 }
 
-static int emac_ethtool_nway_reset(struct net_device *ndev)
-{
+static int emac_ethtool_nway_reset(struct net_device *ndev) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	int res = 0;
 
 	DBG(dev, "nway_reset" NL);
-
-	if (dev->phy.address < 0)
-		return -EOPNOTSUPP;
+	if (dev->phy.address < 0) return -EOPNOTSUPP;
 
 	mutex_lock(&dev->link_lock);
 	if (!dev->phy.autoneg) {
@@ -2230,25 +1980,17 @@
 	return res;
 }
 
-static int emac_ethtool_get_sset_count(struct net_device *ndev, int stringset)
-{
-	if (stringset == ETH_SS_STATS)
-		return EMAC_ETHTOOL_STATS_COUNT;
-	else
-		return -EINVAL;
+static int emac_ethtool_get_sset_count(struct net_device *ndev, int stringset) {
+	if (stringset == ETH_SS_STATS)	return EMAC_ETHTOOL_STATS_COUNT;
+	else return -EINVAL;
 }
 
-static void emac_ethtool_get_strings(struct net_device *ndev, u32 stringset,
-				     u8 * buf)
-{
-	if (stringset == ETH_SS_STATS)
-		memcpy(buf, &emac_stats_keys, sizeof(emac_stats_keys));
+static void emac_ethtool_get_strings(struct net_device *ndev, u32 stringset, u8 * buf) {
+	if (stringset == ETH_SS_STATS) memcpy(buf, &emac_stats_keys, sizeof(emac_stats_keys));
 }
 
 static void emac_ethtool_get_ethtool_stats(struct net_device *ndev,
-					   struct ethtool_stats *estats,
-					   u64 * tmp_stats)
-{
+					   struct ethtool_stats *estats, u64 * tmp_stats){
 	struct emac_instance *dev = netdev_priv(ndev);
 
 	memcpy(tmp_stats, &dev->stats, sizeof(dev->stats));
@@ -2256,9 +1998,7 @@
 	memcpy(tmp_stats, &dev->estats, sizeof(dev->estats));
 }
 
-static void emac_ethtool_get_drvinfo(struct net_device *ndev,
-				     struct ethtool_drvinfo *info)
-{
+static void emac_ethtool_get_drvinfo(struct net_device *ndev, struct ethtool_drvinfo *info) {
 	struct emac_instance *dev = netdev_priv(ndev);
 
 	strlcpy(info->driver, "ibm_emac", sizeof(info->driver));
@@ -2267,6 +2007,34 @@
 		 dev->cell_index, dev->ofdev->dev.of_node);
 }
 
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+static int emac_ethtool_get_coalesce(struct net_device *dev, struct ethtool_coalesce *ec) {
+	struct emac_instance *ei = netdev_priv(dev);
+    	memset(ec, 0, sizeof(*ec));			/* clean up */
+
+	/* Update with current status */
+	ec->rx_coalesce_usecs       = ei->mal->coales_param[0].rx_time / ei->plb_bus_freq;
+	ec->rx_max_coalesced_frames = ei->mal->coales_param[0].rx_count;
+	ec->tx_coalesce_usecs       = ei->mal->coales_param[0].tx_time / ei->plb_bus_freq;
+	ec->tx_max_coalesced_frames = ei->mal->coales_param[0].tx_count;
+    return 0;
+}
+
+static int emac_ethtool_set_coalesce(struct net_device *dev, struct ethtool_coalesce *ec) {
+	struct emac_instance *ei = netdev_priv(dev);
+	int i;
+	
+	for (i = 0; i < 4; i++) {
+		ei->mal->coales_param[i].tx_count =	(ec->tx_max_coalesced_frames & COAL_FRAME_MASK);
+		ei->mal->coales_param[i].rx_count =	(ec->rx_max_coalesced_frames & COAL_FRAME_MASK);
+		ei->mal->coales_param[i].tx_time =	(ec->tx_coalesce_usecs * ei->plb_bus_freq);
+		ei->mal->coales_param[i].rx_time =	(ec->rx_coalesce_usecs * ei->plb_bus_freq);
+	}
+	mal_enable_coal(ei->mal);
+	return 0;
+}
+#endif
+
 static const struct ethtool_ops emac_ethtool_ops = {
 	.get_drvinfo = emac_ethtool_get_drvinfo,
 
@@ -2285,33 +2053,225 @@
 	.get_link = ethtool_op_get_link,
 	.get_link_ksettings = emac_ethtool_get_link_ksettings,
 	.set_link_ksettings = emac_ethtool_set_link_ksettings,
+
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	.get_coalesce           = emac_ethtool_get_coalesce,
+	.set_coalesce           = emac_ethtool_set_coalesce,
+#endif
 };
 
-static int emac_ioctl(struct net_device *ndev, struct ifreq *rq, int cmd)
-{
+/* sysfs support for IBM NEW EMAC */
+#if defined(CONFIG_IBM_EMAC_SYSFS)
+
+#if defined(CONFIG_IBM_EMAC_INTR_COALESCE)
+/* Display interrupt coalesce parametters values */
+static ssize_t show_tx_count(struct device *dev, struct device_attribute *attr, char *buf) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+	return sprintf(buf, "%d\n", dev_ins->mal->coales_param[0].tx_count);
+}
+
+static ssize_t show_rx_count(struct device *dev, struct device_attribute *attr, char *buf) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+	return sprintf(buf, "%d\n", dev_ins->mal->coales_param[0].rx_count);
+}
+
+static ssize_t show_tx_time(struct device *dev, struct device_attribute *attr, char *buf) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+	return sprintf(buf, "%d\n", dev_ins->mal->coales_param[0].tx_time);
+}
+
+static ssize_t show_rx_time(struct device *dev, struct device_attribute *attr, char *buf) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+	return sprintf(buf, "%d\n", dev_ins->mal->coales_param[0].rx_time);
+}
+
+static int core_reset(struct emac_instance * dev) {
+	mutex_lock(&dev->link_lock);
+	emac_netif_stop(dev);
+	emac_rx_disable(dev);
+	mal_disable_rx_channel(dev->mal, dev->mal_rx_chan);
+	if (dev->rx_sg_skb) {
+		++dev->estats.rx_dropped_resize;
+		dev_kfree_skb(dev->rx_sg_skb);
+		dev->rx_sg_skb = NULL;
+	}
+	/* This is to prevent starting RX channel in emac_rx_enable() */
+	set_bit(MAL_COMMAC_RX_STOPPED, &dev->commac.flags);
+	emac_full_tx_reset(dev);	/* Restart RX */
+	clear_bit(MAL_COMMAC_RX_STOPPED, &dev->commac.flags);
+	dev->rx_slot = 0;	
+	mal_enable_rx_channel(dev->mal, dev->mal_rx_chan);
+	emac_rx_enable(dev);
+	emac_netif_start(dev);
+	mutex_unlock(&dev->link_lock);
+	return 0;
+}
+
+/* Set interrupt coalesce parametters values */
+static ssize_t store_tx_count(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+	
+	long tmp = simple_strtol(buf, NULL, 10);
+ 	dev_ins->mal->coales_param[0].tx_count = tmp; /* Reconfigure MAL intr coalesce parameters */
+	mutex_lock(&dev_ins->link_lock);
+	mal_enable_coal(dev_ins->mal);
+	mutex_unlock(&dev_ins->link_lock);
+	/* FIXME: It seems that not reset the interface cause it hangs after short period of time */
+	if (netif_running(dev_ins->ndev)) core_reset(dev_ins);
+	return count;
+}
+static ssize_t store_rx_count(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+
+	long tmp = simple_strtol(buf, NULL, 10);
+ 	dev_ins->mal->coales_param[0].rx_count = tmp; /* Reconfigure MAL intr coalesce parameters */
+	mutex_lock(&dev_ins->link_lock);
+	mal_enable_coal(dev_ins->mal);
+	mutex_unlock(&dev_ins->link_lock);
+	/* FIXME: It seems that not reset the interface cause it hangs after short period of time */
+	if (netif_running(dev_ins->ndev)) core_reset(dev_ins);
+	return count;
+}
+
+static ssize_t store_tx_time(struct device *dev, struct device_attribute *attr, const char *buf, size_t count) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+
+	long tmp = simple_strtol(buf, NULL, 10);
+	dev_ins->mal->coales_param[0].tx_time = tmp; /* Reconfigure MAL intr coalesce parameters */
+	mutex_lock(&dev_ins->link_lock);
+	mal_enable_coal(dev_ins->mal);
+	mutex_unlock(&dev_ins->link_lock);
+	/* FIXME: It seems that not reset the interface cause it hangs after short period of time */
+	if (netif_running(dev_ins->ndev)) core_reset(dev_ins);
+	return count;
+}
+
+static ssize_t store_rx_time(struct device *dev, struct device_attribute *attr,	const char *buf, size_t count) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+
+	long tmp = simple_strtol(buf, NULL, 10);
+	dev_ins->mal->coales_param[0].rx_time = tmp; /* Reconfigure MAL intr coalesce parameters */
+	mutex_lock(&dev_ins->link_lock);        
+	mal_enable_coal(dev_ins->mal);
+	mutex_unlock(&dev_ins->link_lock);
+	/* FIXME: It seems that not reset the interface cause it hangs after short period of time */
+	if (netif_running(dev_ins->ndev)) core_reset(dev_ins);
+	return count;
+}
+#endif
+
+#if defined(CONFIG_IBM_EMAC_MASK_CEXT)
+static ssize_t show_emi_fix_enable(struct device *dev, struct device_attribute *attr, char *buf) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+	return sprintf(buf, "%d\n", atomic_read(&dev_ins->mask_cext_enable));
+}
+
+static ssize_t store_emi_fix_enable(struct device *dev, struct device_attribute *attr,
+	const char *buf, size_t count) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+
+	long tmp = !!simple_strtol(buf, NULL, 10);
+	printk(KERN_INFO "%s EMAC EMI Fix\n", (tmp) ? "Enable" : "Disable");
+	atomic_set(&dev_ins->mask_cext_enable, tmp);	/* Exit idle mode before return */
+
+	if (atomic_read(&dev_ins->idle_mode)) {
+	   	emac_exit_idlemode(dev_ins);
+		atomic_set(&dev_ins->idle_mode, 0);
+	}
+	return count;
+}
+
+#endif
+
+#if defined(CONFIG_IBM_EMAC_TAH)
+static ssize_t show_tah_ssr(struct device *dev, struct device_attribute *attr, char *buf) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+    char tah_char = attr->attr.name[strlen(attr->attr.name) - 1];  // last character
+    u32 tah_index = (isodigit(tah_char)) ? tah_char - '0': 0;  // converted to digit
+
+	if (emac_has_feature(dev_ins, EMAC_FTR_HAS_TAH))
+		return sprintf(buf, "%d\n", TAH_SSR_2_SS(tah_get_ssr(dev_ins->tah_dev, tah_index)) << 1);
+	return 0;
+} 
+
+static ssize_t store_tah_ssr(struct device *dev, struct device_attribute *attr,	const char *buf, size_t count) {
+	struct net_device *ndev = to_net_dev(dev);
+	struct emac_instance *dev_ins = netdev_priv(ndev);
+    char tah_char = attr->attr.name[strlen(attr->attr.name) - 1];  // last character
+    u32 tah_index = (isodigit(tah_char)) ? tah_char - '0': 0;  // converted to digit
+
+	long tmp = simple_strtol(buf, NULL, 10);
+	if (emac_has_feature(dev_ins, EMAC_FTR_HAS_TAH)) tah_set_ssr(dev_ins->tah_dev, tah_index, tmp);	
+	return count;
+}
+static DEVICE_ATTR(tah_ssr0, S_IRUGO | S_IWUSR, show_tah_ssr, store_tah_ssr);
+static DEVICE_ATTR(tah_ssr1, S_IRUGO | S_IWUSR, show_tah_ssr, store_tah_ssr);
+static DEVICE_ATTR(tah_ssr2, S_IRUGO | S_IWUSR, show_tah_ssr, store_tah_ssr);
+static DEVICE_ATTR(tah_ssr3, S_IRUGO | S_IWUSR, show_tah_ssr, store_tah_ssr);
+static DEVICE_ATTR(tah_ssr4, S_IRUGO | S_IWUSR, show_tah_ssr, store_tah_ssr);
+static DEVICE_ATTR(tah_ssr5, S_IRUGO | S_IWUSR, show_tah_ssr, store_tah_ssr);
+#endif
+
+#if defined(CONFIG_IBM_EMAC_INTR_COALESCE)
+static DEVICE_ATTR(coalesce_param_tx_count, S_IRUGO | S_IWUSR, show_tx_count, store_tx_count);
+static DEVICE_ATTR(coalesce_param_rx_count, S_IRUGO | S_IWUSR, show_rx_count, store_rx_count);
+static DEVICE_ATTR(coalesce_param_tx_time, S_IRUGO | S_IWUSR, show_tx_time, store_tx_time);
+static DEVICE_ATTR(coalesce_param_rx_time, S_IRUGO | S_IWUSR, show_rx_time, store_rx_time);
+#endif
+
+#if defined(CONFIG_APM821xx) && defined(CONFIG_IBM_EMAC_MASK_CEXT)
+static DEVICE_ATTR(emi_fix_enable, S_IRUGO | S_IWUSR, show_emi_fix_enable, store_emi_fix_enable);
+#endif
+
+static struct attribute *ibm_emac_attr[] = {
+#if defined(CONFIG_IBM_EMAC_INTR_COALESCE)
+	&dev_attr_coalesce_param_tx_count.attr,	&dev_attr_coalesce_param_rx_count.attr,
+	&dev_attr_coalesce_param_tx_time.attr,	&dev_attr_coalesce_param_rx_time.attr,
+#endif
+
+#if defined(CONFIG_IBM_EMAC_TAH)
+	&dev_attr_tah_ssr0.attr, &dev_attr_tah_ssr1.attr, &dev_attr_tah_ssr2.attr,
+	&dev_attr_tah_ssr3.attr, &dev_attr_tah_ssr4.attr, &dev_attr_tah_ssr5.attr,
+#endif
+
+#if defined(CONFIG_APM821xx) && defined(CONFIG_IBM_EMAC_MASK_CEXT)
+	&dev_attr_emi_fix_enable.attr,
+#endif
+	NULL
+};
+
+static const struct attribute_group ibm_emac_attr_group = {
+	.attrs = ibm_emac_attr,
+};
+
+#endif
+
+/* IOCTL support for the interface */
+static int emac_ioctl(struct net_device *ndev, struct ifreq *rq, int cmd) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	struct mii_ioctl_data *data = if_mii(rq);
 
 	DBG(dev, "ioctl %08x" NL, cmd);
-
-	if (dev->phy.address < 0)
-		return -EOPNOTSUPP;
+	if (dev->phy.address < 0) return -EOPNOTSUPP;
 
 	switch (cmd) {
-	case SIOCGMIIPHY:
-		data->phy_id = dev->phy.address;
-		/* Fall through */
-	case SIOCGMIIREG:
-		data->val_out = emac_mdio_read(ndev, dev->phy.address,
-					       data->reg_num);
+	case SIOCGMIIPHY:	data->phy_id = dev->phy.address;	/* Fall through */
+	case SIOCGMIIREG:	data->val_out = emac_mdio_read(ndev, dev->phy.address, data->reg_num);
 		return 0;
-
-	case SIOCSMIIREG:
-		emac_mdio_write(ndev, dev->phy.address, data->reg_num,
-				data->val_in);
+	case SIOCSMIIREG:	emac_mdio_write(ndev, dev->phy.address, data->reg_num, data->val_in);
 		return 0;
-	default:
-		return -EOPNOTSUPP;
+	default:			return -EOPNOTSUPP;
 	}
 }
 
@@ -2330,9 +2290,7 @@
 #define	EMAC_DEP_PREV_IDX	5
 #define	EMAC_DEP_COUNT		6
 
-static int emac_check_deps(struct emac_instance *dev,
-			   struct emac_depentry *deps)
-{
+static int emac_check_deps(struct emac_instance *dev, struct emac_depentry *deps) {
 	int i, there = 0;
 	struct device_node *np;
 
@@ -2350,27 +2308,19 @@
 				there++;
 				continue;
 			}
-			if (deps[i].node == NULL)
-				deps[i].node = of_node_get(np);
+			if (deps[i].node == NULL) deps[i].node = of_node_get(np);
 		}
-		if (deps[i].node == NULL)
-			deps[i].node = of_find_node_by_phandle(deps[i].phandle);
-		if (deps[i].node == NULL)
-			continue;
-		if (deps[i].ofdev == NULL)
-			deps[i].ofdev = of_find_device_by_node(deps[i].node);
-		if (deps[i].ofdev == NULL)
-			continue;
-		if (deps[i].drvdata == NULL)
-			deps[i].drvdata = platform_get_drvdata(deps[i].ofdev);
-		if (deps[i].drvdata != NULL)
-			there++;
+		if (deps[i].node == NULL) 	deps[i].node = of_find_node_by_phandle(deps[i].phandle);
+		if (deps[i].node == NULL) 	continue;
+		if (deps[i].ofdev == NULL)	deps[i].ofdev = of_find_device_by_node(deps[i].node);
+		if (deps[i].ofdev == NULL)	continue;
+		if (deps[i].drvdata == NULL) deps[i].drvdata = platform_get_drvdata(deps[i].ofdev);
+		if (deps[i].drvdata != NULL) there++;
 	}
-	return there == EMAC_DEP_COUNT;
+	return (there == EMAC_DEP_COUNT);
 }
 
-static void emac_put_deps(struct emac_instance *dev)
-{
+static void emac_put_deps(struct emac_instance *dev) {
 	of_dev_put(dev->mal_dev);
 	of_dev_put(dev->zmii_dev);
 	of_dev_put(dev->rgmii_dev);
@@ -2378,12 +2328,9 @@
 	of_dev_put(dev->tah_dev);
 }
 
-static int emac_of_bus_notify(struct notifier_block *nb, unsigned long action,
-			      void *data)
-{
+static int emac_of_bus_notify(struct notifier_block *nb, unsigned long action, void *data) {
 	/* We are only intereted in device addition */
-	if (action == BUS_NOTIFY_BOUND_DRIVER)
-		wake_up_all(&emac_probe_wait);
+	if (action == BUS_NOTIFY_BOUND_DRIVER) wake_up_all(&emac_probe_wait);
 	return 0;
 }
 
@@ -2391,8 +2338,7 @@
 	.notifier_call = emac_of_bus_notify
 };
 
-static int emac_wait_deps(struct emac_instance *dev)
-{
+static int emac_wait_deps(struct emac_instance *dev) {
 	struct emac_depentry deps[EMAC_DEP_COUNT];
 	int i, err;
 
@@ -2401,22 +2347,16 @@
 	deps[EMAC_DEP_MAL_IDX].phandle = dev->mal_ph;
 	deps[EMAC_DEP_ZMII_IDX].phandle = dev->zmii_ph;
 	deps[EMAC_DEP_RGMII_IDX].phandle = dev->rgmii_ph;
-	if (dev->tah_ph)
-		deps[EMAC_DEP_TAH_IDX].phandle = dev->tah_ph;
-	if (dev->mdio_ph)
-		deps[EMAC_DEP_MDIO_IDX].phandle = dev->mdio_ph;
-	if (dev->blist && dev->blist > emac_boot_list)
-		deps[EMAC_DEP_PREV_IDX].phandle = 0xffffffffu;
+	if (dev->tah_ph)	deps[EMAC_DEP_TAH_IDX].phandle = dev->tah_ph;
+	if (dev->mdio_ph)	deps[EMAC_DEP_MDIO_IDX].phandle = dev->mdio_ph;
+	if (dev->blist && dev->blist > emac_boot_list) deps[EMAC_DEP_PREV_IDX].phandle = 0xffffffffu;
 	bus_register_notifier(&platform_bus_type, &emac_of_bus_notifier);
-	wait_event_timeout(emac_probe_wait,
-			   emac_check_deps(dev, deps),
-			   EMAC_PROBE_DEP_TIMEOUT);
+	wait_event_timeout(emac_probe_wait, emac_check_deps(dev, deps), EMAC_PROBE_DEP_TIMEOUT);
 	bus_unregister_notifier(&platform_bus_type, &emac_of_bus_notifier);
 	err = emac_check_deps(dev, deps) ? 0 : -ENODEV;
 	for (i = 0; i < EMAC_DEP_COUNT; i++) {
 		of_node_put(deps[i].node);
-		if (err)
-			of_dev_put(deps[i].ofdev);
+		if (err) of_dev_put(deps[i].ofdev);
 	}
 	if (err == 0) {
 		dev->mal_dev = deps[EMAC_DEP_MAL_IDX].ofdev;
@@ -2429,23 +2369,18 @@
 	return err;
 }
 
-static int emac_read_uint_prop(struct device_node *np, const char *name,
-			       u32 *val, int fatal)
-{
+static int emac_read_prop(struct device_node *np, const char *name, u32 *val, int fatal) {
 	int len;
 	const u32 *prop = of_get_property(np, name, &len);
 	if (prop == NULL || len < sizeof(u32)) {
-		if (fatal)
-			printk(KERN_ERR "%pOF: missing %s property\n",
-			       np, name);
+		if (fatal) printk(KERN_ERR "%pOF: missing %s property\n", np, name);
 		return -ENODEV;
 	}
 	*val = *prop;
 	return 0;
 }
 
-static void emac_adjust_link(struct net_device *ndev)
-{
+static void emac_adjust_link(struct net_device *ndev) {
 	struct emac_instance *dev = netdev_priv(ndev);
 	struct phy_device *phy = dev->phy_dev;
 
@@ -2457,8 +2392,7 @@
 	dev->phy.advertising = phy->advertising;
 }
 
-static int emac_mii_bus_read(struct mii_bus *bus, int addr, int regnum)
-{
+static int emac_mii_bus_read(struct mii_bus *bus, int addr, int regnum) {
 	int ret = emac_mdio_read(bus->priv, addr, regnum);
 	/* This is a workaround for powered down ports/phys.
 	 * In the wild, this was seen on the Cisco Meraki MX60(W).
@@ -2469,23 +2403,18 @@
 	return ret < 0 ? 0xffff : ret;
 }
 
-static int emac_mii_bus_write(struct mii_bus *bus, int addr,
-			      int regnum, u16 val)
-{
+static int emac_mii_bus_write(struct mii_bus *bus, int addr, int regnum, u16 val) {
 	emac_mdio_write(bus->priv, addr, regnum, val);
 	return 0;
 }
 
-static int emac_mii_bus_reset(struct mii_bus *bus)
-{
+static int emac_mii_bus_reset(struct mii_bus *bus) {
 	struct emac_instance *dev = netdev_priv(bus->priv);
 
 	return emac_reset(dev);
 }
 
-static int emac_mdio_phy_start_aneg(struct mii_phy *phy,
-				    struct phy_device *phy_dev)
-{
+static int emac_mdio_phy_start_aneg(struct mii_phy *phy, struct phy_device *phy_dev) {
 	phy_dev->autoneg = phy->autoneg;
 	phy_dev->speed = phy->speed;
 	phy_dev->duplex = phy->duplex;
@@ -2503,8 +2432,7 @@
 	return emac_mdio_phy_start_aneg(phy, dev->phy_dev);
 }
 
-static int emac_mdio_setup_forced(struct mii_phy *phy, int speed, int fd)
-{
+static int emac_mdio_setup_forced(struct mii_phy *phy, int speed, int fd) {
 	struct net_device *ndev = phy->dev;
 	struct emac_instance *dev = netdev_priv(ndev);
 
@@ -2514,13 +2442,10 @@
 	return emac_mdio_phy_start_aneg(phy, dev->phy_dev);
 }
 
-static int emac_mdio_poll_link(struct mii_phy *phy)
-{
+static int emac_mdio_poll_link(struct mii_phy *phy) {
 	struct net_device *ndev = phy->dev;
 	struct emac_instance *dev = netdev_priv(ndev);
-	int res;
-
-	res = phy_read_status(dev->phy_dev);
+	int res= phy_read_status(dev->phy_dev);
 	if (res) {
 		dev_err(&dev->ofdev->dev, "link update failed (%d).", res);
 		return ethtool_op_get_link(ndev);
@@ -2529,16 +2454,12 @@
 	return dev->phy_dev->link;
 }
 
-static int emac_mdio_read_link(struct mii_phy *phy)
-{
+static int emac_mdio_read_link(struct mii_phy *phy) {
 	struct net_device *ndev = phy->dev;
 	struct emac_instance *dev = netdev_priv(ndev);
 	struct phy_device *phy_dev = dev->phy_dev;
-	int res;
-
-	res = phy_read_status(phy_dev);
-	if (res)
-		return res;
+	int res = phy_read_status(phy_dev);
+	if (res) return res;
 
 	phy->speed = phy_dev->speed;
 	phy->duplex = phy_dev->duplex;
@@ -2547,8 +2468,7 @@
 	return 0;
 }
 
-static int emac_mdio_init_phy(struct mii_phy *phy)
-{
+static int emac_mdio_init_phy(struct mii_phy *phy) {
 	struct net_device *ndev = phy->dev;
 	struct emac_instance *dev = netdev_priv(ndev);
 
@@ -2564,8 +2484,7 @@
 	.read_link	= emac_mdio_read_link,
 };
 
-static int emac_dt_mdio_probe(struct emac_instance *dev)
-{
+static int emac_dt_mdio_probe(struct emac_instance *dev) {
 	struct device_node *mii_np;
 	int res;
 
@@ -2594,26 +2513,18 @@
 	dev->mii_bus->reset = &emac_mii_bus_reset;
 	snprintf(dev->mii_bus->id, MII_BUS_ID_SIZE, "%s", dev->ofdev->name);
 	res = of_mdiobus_register(dev->mii_bus, mii_np);
-	if (res) {
-		dev_err(&dev->ofdev->dev, "cannot register MDIO bus %s (%d)",
-			dev->mii_bus->name, res);
-	}
+	if (res) dev_err(&dev->ofdev->dev, "cannot register MDIO bus %s (%d)", dev->mii_bus->name, res);
 
  put_node:
 	of_node_put(mii_np);
 	return res;
 }
 
-static int emac_dt_phy_connect(struct emac_instance *dev,
-			       struct device_node *phy_handle)
-{
-	dev->phy.def = devm_kzalloc(&dev->ofdev->dev, sizeof(*dev->phy.def),
-				    GFP_KERNEL);
-	if (!dev->phy.def)
-		return -ENOMEM;
+static int emac_dt_phy_connect(struct emac_instance *dev, struct device_node *phy_handle) {
+	dev->phy.def = devm_kzalloc(&dev->ofdev->dev, sizeof(*dev->phy.def), GFP_KERNEL);
+	if (!dev->phy.def) return -ENOMEM;
 
-	dev->phy_dev = of_phy_connect(dev->ndev, phy_handle, &emac_adjust_link,
-				      0, dev->phy_mode);
+	dev->phy_dev = of_phy_connect(dev->ndev, phy_handle, &emac_adjust_link, 0, dev->phy_mode);
 	if (!dev->phy_dev) {
 		dev_err(&dev->ofdev->dev, "failed to connect to PHY.\n");
 		return -ENODEV;
@@ -2629,20 +2540,16 @@
 	return 0;
 }
 
-static int emac_dt_phy_probe(struct emac_instance *dev)
-{
-	struct device_node *np = dev->ofdev->dev.of_node;
-	struct device_node *phy_handle;
+static int emac_dt_phy_probe(struct emac_instance *dev) {
 	int res = 1;
-
-	phy_handle = of_parse_phandle(np, "phy-handle", 0);
+	struct device_node *np = dev->ofdev->dev.of_node;
+	struct device_node *phy_handle = of_parse_phandle(np, "phy-handle", 0);
 
 	if (phy_handle) {
 		res = emac_dt_mdio_probe(dev);
 		if (!res) {
 			res = emac_dt_phy_connect(dev, phy_handle);
-			if (res)
-				mdiobus_unregister(dev->mii_bus);
+			if (res) mdiobus_unregister(dev->mii_bus);
 		}
 	}
 
@@ -2650,8 +2557,7 @@
 	return res;
 }
 
-static int emac_init_phy(struct emac_instance *dev)
-{
+static int emac_init_phy(struct emac_instance *dev) {
 	struct device_node *np = dev->ofdev->dev.of_node;
 	struct net_device *ndev = dev->ndev;
 	u32 phy_map, adv;
@@ -2661,24 +2567,20 @@
 	dev->phy.mode = dev->phy_mode;
 
 	/* PHY-less configuration. */
-	if ((dev->phy_address == 0xffffffff && dev->phy_map == 0xffffffff) ||
-	    of_phy_is_fixed_link(np)) {
+	if ((dev->phy_address == 0xffffffff && dev->phy_map == 0xffffffff) || of_phy_is_fixed_link(np)) {
 		emac_reset(dev);
+        printk(KERN_INFO "eth0: Phy-less config\n");
 
-		/* PHY-less configuration. */
+		/* PHY-less configuration. Probably should move these settings to the dev tree */
 		dev->phy.address = -1;
 		dev->phy.features = SUPPORTED_MII;
-		if (emac_phy_supports_gige(dev->phy_mode))
-			dev->phy.features |= SUPPORTED_1000baseT_Full;
-		else
-			dev->phy.features |= SUPPORTED_100baseT_Full;
+		if (emac_phy_supports_gige(dev->phy_mode)) dev->phy.features |= SUPPORTED_1000baseT_Full;
+		else dev->phy.features |= SUPPORTED_100baseT_Full;
 		dev->phy.pause = 1;
 
 		if (of_phy_is_fixed_link(np)) {
 			int res = emac_dt_mdio_probe(dev);
-
-			if (res)
-				return res;
+			if (res) return res;
 
 			res = of_phy_register_fixed_link(np);
 			dev->phy_dev = of_phy_find_device(np);
@@ -2717,46 +2619,26 @@
 	 * This is needed mostly for 440GX
 	 */
 	if (emac_phy_gpcs(dev->phy.mode)) {
-		/* XXX
-		 * Make GPCS PHY address equal to EMAC index.
-		 * We probably should take into account busy_phy_map
-		 * and/or phy_map here.
-		 *
-		 * Note that the busy_phy_map is currently global
-		 * while it should probably be per-ASIC...
-		 */
+		/* Make GPCS PHY address equal to EMAC index.
+		 * We probably should take into account busy_phy_map and/or phy_map here.
+		 * Note that the busy_phy_map is currently global while it should probably be per-ASIC */
 		dev->phy.gpcs_address = dev->gpcs_address;
-		if (dev->phy.gpcs_address == 0xffffffff)
-			dev->phy.address = dev->cell_index;
+		if (dev->phy.gpcs_address == 0xffffffff) dev->phy.address = dev->cell_index;
 	}
 
 	emac_configure(dev);
 
 	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII)) {
 		int res = emac_dt_phy_probe(dev);
-
-		switch (res) {
-		case 1:
-			/* No phy-handle property configured.
-			 * Continue with the existing phy probe
-			 * and setup code.
-			 */
-			break;
-
-		case 0:
+		if (res != 1) {
 			mutex_unlock(&emac_phy_map_lock);
-			goto init_phy;
-
-		default:
-			mutex_unlock(&emac_phy_map_lock);
-			dev_err(&dev->ofdev->dev, "failed to attach dt phy (%d).\n",
-				res);
+			if (res == 0) goto init_phy;	
+			dev_err(&dev->ofdev->dev, "failed to attach dt phy (%d).\n", res);
 			return res;
 		}
 	}
 
-	if (dev->phy_address != 0xffffffff)
-		phy_map = ~(1 << dev->phy_address);
+	if (dev->phy_address != 0xffffffff)	phy_map = ~(1 << dev->phy_address);
 
 	for (i = 0; i < 0x20; phy_map >>= 1, ++i)
 		if (!(phy_map & 1)) {
@@ -2765,10 +2647,8 @@
 
 			/* Quick check if there is a PHY at the address */
 			r = emac_mdio_read(dev->ndev, i, MII_BMCR);
-			if (r == 0xffff || r < 0)
-				continue;
-			if (!emac_mii_phy_probe(&dev->phy, i))
-				break;
+			if (r == 0xffff || r < 0) continue;
+			if (!emac_mii_phy_probe(&dev->phy, i)) break;
 		}
 
 	/* Enable external clock source */
@@ -2806,15 +2686,12 @@
 		if (f & SUPPORTED_1000baseT_Full) {
 			speed = SPEED_1000;
 			fd = DUPLEX_FULL;
-		} else if (f & SUPPORTED_1000baseT_Half)
-			speed = SPEED_1000;
+		} else if (f & SUPPORTED_1000baseT_Half) speed = SPEED_1000;
 		else if (f & SUPPORTED_100baseT_Full) {
 			speed = SPEED_100;
 			fd = DUPLEX_FULL;
-		} else if (f & SUPPORTED_100baseT_Half)
-			speed = SPEED_100;
-		else if (f & SUPPORTED_10baseT_Full)
-			fd = DUPLEX_FULL;
+		} else if (f & SUPPORTED_100baseT_Half)	speed = SPEED_100;
+		else if (f & SUPPORTED_10baseT_Full) fd = DUPLEX_FULL;
 
 		/* Force link parameters */
 		dev->phy.def->ops->setup_forced(&dev->phy, speed, fd);
@@ -2822,83 +2699,60 @@
 	return 0;
 }
 
-static int emac_init_config(struct emac_instance *dev)
-{
+static int emac_init_config(struct emac_instance *dev) {
 	struct device_node *np = dev->ofdev->dev.of_node;
 	const void *p;
 
 	/* Read config from device-tree */
-	if (emac_read_uint_prop(np, "mal-device", &dev->mal_ph, 1))
-		return -ENXIO;
-	if (emac_read_uint_prop(np, "mal-tx-channel", &dev->mal_tx_chan, 1))
-		return -ENXIO;
-	if (emac_read_uint_prop(np, "mal-rx-channel", &dev->mal_rx_chan, 1))
-		return -ENXIO;
-	if (emac_read_uint_prop(np, "cell-index", &dev->cell_index, 1))
-		return -ENXIO;
-	if (emac_read_uint_prop(np, "max-frame-size", &dev->max_mtu, 0))
-		dev->max_mtu = ETH_DATA_LEN;
-	if (emac_read_uint_prop(np, "rx-fifo-size", &dev->rx_fifo_size, 0))
-		dev->rx_fifo_size = 2048;
-	if (emac_read_uint_prop(np, "tx-fifo-size", &dev->tx_fifo_size, 0))
-		dev->tx_fifo_size = 2048;
-	if (emac_read_uint_prop(np, "rx-fifo-size-gige", &dev->rx_fifo_size_gige, 0))
-		dev->rx_fifo_size_gige = dev->rx_fifo_size;
-	if (emac_read_uint_prop(np, "tx-fifo-size-gige", &dev->tx_fifo_size_gige, 0))
-		dev->tx_fifo_size_gige = dev->tx_fifo_size;
-	if (emac_read_uint_prop(np, "phy-address", &dev->phy_address, 0))
-		dev->phy_address = 0xffffffff;
-	if (emac_read_uint_prop(np, "phy-map", &dev->phy_map, 0))
-		dev->phy_map = 0xffffffff;
-	if (emac_read_uint_prop(np, "gpcs-address", &dev->gpcs_address, 0))
-		dev->gpcs_address = 0xffffffff;
-	if (emac_read_uint_prop(np->parent, "clock-frequency", &dev->opb_bus_freq, 1))
-		return -ENXIO;
-	if (emac_read_uint_prop(np, "tah-device", &dev->tah_ph, 0))
-		dev->tah_ph = 0;
-	if (emac_read_uint_prop(np, "tah-channel", &dev->tah_port, 0))
-		dev->tah_port = 0;
-	if (emac_read_uint_prop(np, "mdio-device", &dev->mdio_ph, 0))
-		dev->mdio_ph = 0;
-	if (emac_read_uint_prop(np, "zmii-device", &dev->zmii_ph, 0))
-		dev->zmii_ph = 0;
-	if (emac_read_uint_prop(np, "zmii-channel", &dev->zmii_port, 0))
-		dev->zmii_port = 0xffffffff;
-	if (emac_read_uint_prop(np, "rgmii-device", &dev->rgmii_ph, 0))
-		dev->rgmii_ph = 0;
-	if (emac_read_uint_prop(np, "rgmii-channel", &dev->rgmii_port, 0))
-		dev->rgmii_port = 0xffffffff;
-	if (emac_read_uint_prop(np, "fifo-entry-size", &dev->fifo_entry_size, 0))
-		dev->fifo_entry_size = 16;
-	if (emac_read_uint_prop(np, "mal-burst-size", &dev->mal_burst_size, 0))
-		dev->mal_burst_size = 256;
+	if (emac_read_prop(np, "mal-device", &dev->mal_ph, 1))	return -ENXIO;
+	if (emac_read_prop(np, "mal-tx-channel", &dev->mal_tx_chan, 1)) return -ENXIO;
+	if (emac_read_prop(np, "mal-rx-channel", &dev->mal_rx_chan, 1)) return -ENXIO;
+	if (emac_read_prop(np, "cell-index", &dev->cell_index, 1))	return -ENXIO;
+	if (emac_read_prop(np, "max-frame-size", &dev->max_mtu, 0)) dev->max_mtu = ETH_DATA_LEN;
+	if (emac_read_prop(np, "rx-fifo-size", &dev->rx_fifo_size, 0))	dev->rx_fifo_size = 2048;
+	if (emac_read_prop(np, "tx-fifo-size", &dev->tx_fifo_size, 0))	dev->tx_fifo_size = 2048;
+	if (emac_read_prop(np, "rx-fifo-size-gige", &dev->rx_fifo_size_gige, 0)) dev->rx_fifo_size_gige = dev->rx_fifo_size;
+	if (emac_read_prop(np, "tx-fifo-size-gige", &dev->tx_fifo_size_gige, 0)) dev->tx_fifo_size_gige = dev->tx_fifo_size;
+	if (emac_read_prop(np, "phy-address", &dev->phy_address, 0)) dev->phy_address = 0xffffffff;
+	if (emac_read_prop(np, "phy-map", &dev->phy_map, 0)) dev->phy_map = 0xffffffff;
+	if (emac_read_prop(np, "gpcs-address", &dev->gpcs_address, 0)) dev->gpcs_address = 0xffffffff;
+	if (emac_read_prop(np->parent, "clock-frequency", &dev->opb_bus_freq, 1)) return -ENXIO;
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	if (emac_read_prop(np->parent->parent, "clock-frequency", &dev->plb_bus_freq, 1)) return -ENXIO;
+	dev->plb_bus_freq /= 1000000;		/* save as MHz */
+#endif
+	if (emac_read_prop(np, "tah-device", &dev->tah_ph, 0)) dev->tah_ph = 0;
+	if (emac_read_prop(np, "tah-channel", &dev->tah_port, 0)) dev->tah_port = 0;
+	if (emac_read_prop(np, "mdio-device", &dev->mdio_ph, 0)) dev->mdio_ph = 0;
+	if (emac_read_prop(np, "zmii-device", &dev->zmii_ph, 0)) dev->zmii_ph = 0;
+	if (emac_read_prop(np, "zmii-channel", &dev->zmii_port, 0))	dev->zmii_port = 0xffffffff;
+	if (emac_read_prop(np, "rgmii-device", &dev->rgmii_ph, 0)) dev->rgmii_ph = 0;
+	if (emac_read_prop(np, "rgmii-channel", &dev->rgmii_port, 0)) dev->rgmii_port = 0xffffffff;
+	if (emac_read_prop(np, "fifo-entry-size", &dev->fifo_entry_size, 0)) dev->fifo_entry_size = 16;
+	if (emac_read_prop(np, "mal-burst-size", &dev->mal_burst_size, 0))	dev->mal_burst_size = 256;
 
 	/* PHY mode needs some decoding */
-	dev->phy_mode = of_get_phy_mode(np);
-	if (dev->phy_mode < 0)
-		dev->phy_mode = PHY_INTERFACE_MODE_NA;
+	dev->phy_mode = of_get_phy_mode(np);	/* Decode PHY-mode */
+	if (dev->phy_mode < 0) dev->phy_mode = PHY_INTERFACE_MODE_NA;
 
 	/* Check EMAC version */
 	if (of_device_is_compatible(np, "ibm,emac4sync")) {
 		dev->features |= (EMAC_FTR_EMAC4 | EMAC_FTR_EMAC4SYNC);
-		if (of_device_is_compatible(np, "ibm,emac-460ex") ||
-		    of_device_is_compatible(np, "ibm,emac-460gt"))
+		if (of_device_is_compatible(np, "ibm,emac-460ex") || of_device_is_compatible(np, "ibm,emac-460gt"))
 			dev->features |= EMAC_FTR_460EX_PHY_CLK_FIX;
-		if (of_device_is_compatible(np, "ibm,emac-405ex") ||
-		    of_device_is_compatible(np, "ibm,emac-405exr"))
+		if (of_device_is_compatible(np, "ibm,emac-405ex") || of_device_is_compatible(np, "ibm,emac-405exr"))
 			dev->features |= EMAC_FTR_440EP_PHY_CLK_FIX;
-		if (of_device_is_compatible(np, "ibm,emac-apm821xx")) {
+		if (of_device_is_compatible(np, "ibm,emac-apm821xx"))
 			dev->features |= (EMAC_APM821XX_REQ_JUMBO_FRAME_SIZE |
 					  EMAC_FTR_APM821XX_NO_HALF_DUPLEX |
 					  EMAC_FTR_460EX_PHY_CLK_FIX);
-		}
+
 	} else if (of_device_is_compatible(np, "ibm,emac4")) {
 		dev->features |= EMAC_FTR_EMAC4;
 		if (of_device_is_compatible(np, "ibm,emac-440gx"))
 			dev->features |= EMAC_FTR_440GX_PHY_CLK_FIX;
 	} else {
-		if (of_device_is_compatible(np, "ibm,emac-440ep") ||
-		    of_device_is_compatible(np, "ibm,emac-440gr"))
+		if (of_device_is_compatible(np, "ibm,emac-440ep") || of_device_is_compatible(np, "ibm,emac-440gr"))
 			dev->features |= EMAC_FTR_440EP_PHY_CLK_FIX;
 		if (of_device_is_compatible(np, "ibm,emac-405ez")) {
 #ifdef CONFIG_IBM_EMAC_NO_FLOW_CTRL
@@ -2909,7 +2763,6 @@
 			return -ENXIO;
 #endif
 		}
-
 	}
 
 	/* Fixup some feature bits based on the device tree */
@@ -2920,8 +2773,7 @@
 
 	/* CAB lacks the appropriate properties */
 	if (of_device_is_compatible(np, "ibm,emac-axon"))
-		dev->features |= EMAC_FTR_HAS_NEW_STACR |
-			EMAC_FTR_STACR_OC_INVERT;
+		dev->features |= EMAC_FTR_HAS_NEW_STACR | EMAC_FTR_STACR_OC_INVERT;
 
 	/* Enable TAH/ZMII/RGMII features as found */
 	if (dev->tah_ph != 0) {
@@ -2954,8 +2806,7 @@
 	/* Read MAC-address */
 	p = of_get_property(np, "local-mac-address", NULL);
 	if (p == NULL) {
-		printk(KERN_ERR "%pOF: Can't find local-mac-address property\n",
-		       np);
+		printk(KERN_ERR "%pOF: Can't find local-mac-address property\n", np);
 		return -ENXIO;
 	}
 	memcpy(dev->ndev->dev_addr, p, ETH_ALEN);
@@ -2968,6 +2819,9 @@
 		dev->xaht_slots_shift = EMAC4_XAHT_SLOTS_SHIFT;
 		dev->xaht_width_shift = EMAC4_XAHT_WIDTH_SHIFT;
 	}
+	/* This should never happen */
+	if (WARN_ON(EMAC_XAHT_REGS(dev) > EMAC_XAHT_MAX_REGS))
+		return -ENXIO;
 
 	DBG(dev, "features     : 0x%08x / 0x%08x\n", dev->features, EMAC_FTRS_POSSIBLE);
 	DBG(dev, "tx_fifo_size : %d (%d gige)\n", dev->tx_fifo_size, dev->tx_fifo_size_gige);
@@ -2978,6 +2832,9 @@
 	return 0;
 }
 
+#ifdef CONFIG_NET_POLL_CONTROLLER
+static void emac_netpoll(struct net_device *dev){}
+#endif
 static const struct net_device_ops emac_netdev_ops = {
 	.ndo_open		= emac_open,
 	.ndo_stop		= emac_close,
@@ -2988,6 +2845,9 @@
 	.ndo_validate_addr	= eth_validate_addr,
 	.ndo_set_mac_address	= emac_set_mac_address,
 	.ndo_start_xmit		= emac_start_xmit,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= emac_netpoll,
+#endif
 };
 
 static const struct net_device_ops emac_gige_netdev_ops = {
@@ -3001,40 +2861,57 @@
 	.ndo_set_mac_address	= emac_set_mac_address,
 	.ndo_start_xmit		= emac_start_xmit_sg,
 	.ndo_change_mtu		= emac_change_mtu,
+#ifdef CONFIG_NET_POLL_CONTROLLER
+	.ndo_poll_controller	= emac_netpoll,
+#endif
 };
 
-static int emac_probe(struct platform_device *ofdev)
-{
+//#define EMAC_ALLOC_ON_OCM
+#ifdef EMAC_ALLOC_ON_OCM
+struct net_device *alloc_netdev_mqs_ocm(void *addr, int sizeof_priv, const char *name,
+				    unsigned char name_assign_type, void (*setup)(struct net_device *),
+				    unsigned int txqs, unsigned int rxqs);
+#endif
+
+static int emac_probe(struct platform_device *ofdev) {
 	struct net_device *ndev;
 	struct emac_instance *dev;
 	struct device_node *np = ofdev->dev.of_node;
 	struct device_node **blist = NULL;
-	int err, i;
+	int err = -ENOMEM, i;
 
-	/* Skip unused/unwired EMACS.  We leave the check for an unused
-	 * property here for now, but new flat device trees should set a
-	 * status property to "disabled" instead.
-	 */
+	/* Skip unused/unwired EMACS.  We leave the check for an unused property here for now, 
+	 * but new flat device trees should set a status property to "disabled" instead. */
 	if (of_get_property(np, "unused", NULL) || !of_device_is_available(np))
 		return -ENODEV;
 
 	/* Find ourselves in the bootlist if we are there */
 	for (i = 0; i < EMAC_BOOT_LIST_SIZE; i++)
-		if (emac_boot_list[i] == np)
-			blist = &emac_boot_list[i];
+		if (emac_boot_list[i] == np) blist = &emac_boot_list[i];
 
-	/* Allocate our net_device structure */
-	err = -ENOMEM;
+#ifdef EMAC_ALLOC_ON_OCM
+	{
+		phys_addr_t	phys;
+		int alloc_size = ALIGN(sizeof(struct net_device), NETDEV_ALIGN) + sizeof(struct emac_instance) + NETDEV_ALIGN;
+		printk(KERN_INFO "eth0: Allocating EMAC and NETDEV instance on OCM\n");
+		ndev = ppc4xx_ocm_alloc(&phys, alloc_size, 4, PPC4XX_OCM_NON_CACHED, "emac_instance");
+		memset(ndev, 0, alloc_size);
+		ndev = alloc_netdev_mqs_ocm((void *)ndev, sizeof(struct emac_instance), "eth%d", NET_NAME_UNKNOWN, ether_setup, 1, 1);
+	}
+#else
 	ndev = alloc_etherdev(sizeof(struct emac_instance));
-	if (!ndev)
-		goto err_gone;
+	//ndev = alloc_netdev_mqs(sizeof(struct emac_instance), "eth%d", NET_NAME_UNKNOWN, ether_setup, 1, 1);
+#endif
+	 
+	if (!ndev) goto err_gone;
 
 	dev = netdev_priv(ndev);
 	dev->ndev = ndev;
 	dev->ofdev = ofdev;
 	dev->blist = blist;
 	SET_NETDEV_DEV(ndev, &ofdev->dev);
-
+    dev->ofdev_dev =  &ofdev->dev; // Cache dev
+	
 	/* Initialize some embedded data structures */
 	mutex_init(&dev->mdio_lock);
 	mutex_init(&dev->link_lock);
@@ -3043,8 +2920,7 @@
 
 	/* Init various config data based on device-tree */
 	err = emac_init_config(dev);
-	if (err)
-		goto err_free;
+	if (err != 0) goto err_free;
 
 	/* Get interrupts. EMAC irq is mandatory, WOL irq is optional */
 	dev->emac_irq = irq_of_parse_and_map(np, 0);
@@ -3057,8 +2933,12 @@
 	ndev->irq = dev->emac_irq;
 
 	/* Map EMAC regs */
-	// TODO : platform_get_resource() and devm_ioremap_resource()
-	dev->emacp = of_iomap(np, 0);
+	if (of_address_to_resource(np, 0, &dev->rsrc_regs)) {
+		printk(KERN_ERR "%s: Can't get registers address\n", np->full_name);
+		goto err_irq_unmap;
+	}
+	// TODO : request_mem_region
+	dev->emacp = ioremap(dev->rsrc_regs.start, resource_size(&dev->rsrc_regs));
 	if (dev->emacp == NULL) {
 		printk(KERN_ERR "%pOF: Can't map device registers!\n", np);
 		err = -ENOMEM;
@@ -3068,14 +2948,12 @@
 	/* Wait for dependent devices */
 	err = emac_wait_deps(dev);
 	if (err) {
-		printk(KERN_ERR
-		       "%pOF: Timeout waiting for dependent devices\n", np);
+		printk(KERN_ERR "%pOF: Timeout waiting for dependent devices\n", np);
 		/*  display more info about what's missing ? */
 		goto err_reg_unmap;
 	}
 	dev->mal = platform_get_drvdata(dev->mal_dev);
-	if (dev->mdio_dev != NULL)
-		dev->mdio_instance = platform_get_drvdata(dev->mdio_dev);
+	if (dev->mdio_dev != NULL) dev->mdio_instance = platform_get_drvdata(dev->mdio_dev);
 
 	/* Register with MAL */
 	dev->commac.ops = &emac_commac_ops;
@@ -3084,18 +2962,15 @@
 	dev->commac.rx_chan_mask = MAL_CHAN_MASK(dev->mal_rx_chan);
 	err = mal_register_commac(dev->mal, &dev->commac);
 	if (err) {
-		printk(KERN_ERR "%pOF: failed to register with mal %pOF!\n",
-		       np, dev->mal_dev->dev.of_node);
+		printk(KERN_ERR "%pOF: failed to register with mal %pOF!\n", np, dev->mal_dev->dev.of_node);
 		goto err_rel_deps;
 	}
 	dev->rx_skb_size = emac_rx_skb_size(ndev->mtu);
 	dev->rx_sync_size = emac_rx_sync_size(ndev->mtu);
 
 	/* Get pointers to BD rings */
-	dev->tx_desc =
-	    dev->mal->bd_virt + mal_tx_bd_offset(dev->mal, dev->mal_tx_chan);
-	dev->rx_desc =
-	    dev->mal->bd_virt + mal_rx_bd_offset(dev->mal, dev->mal_rx_chan);
+	dev->tx_desc = dev->mal->bd_virt + mal_tx_bd_offset(dev->mal, dev->mal_tx_chan);
+	dev->rx_desc = dev->mal->bd_virt + mal_rx_bd_offset(dev->mal, dev->mal_rx_chan);
 
 	DBG(dev, "tx_desc %p" NL, dev->tx_desc);
 	DBG(dev, "rx_desc %p" NL, dev->rx_desc);
@@ -3105,7 +2980,9 @@
 	memset(dev->rx_desc, 0, NUM_RX_BUFF * sizeof(struct mal_descriptor));
 	memset(dev->tx_skb, 0, NUM_TX_BUFF * sizeof(struct sk_buff *));
 	memset(dev->rx_skb, 0, NUM_RX_BUFF * sizeof(struct sk_buff *));
-
+#ifdef CONFIG_IBM_EMAC_MASK_CEXT
+	atomic_set(&dev->mask_cext_enable, 0);		/* By default: DISABLE EMI fix */
+#endif
 	/* Attach to ZMII, if needed */
 	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII) &&
 	    (err = zmii_attach(dev->zmii_dev, dev->zmii_port, &dev->phy_mode)) != 0)
@@ -3130,76 +3007,82 @@
 	INIT_DELAYED_WORK(&dev->link_work, emac_link_timer);
 
 	/* Some SoCs like APM821xx does not support Half Duplex mode. */
-	if (emac_has_feature(dev, EMAC_FTR_APM821XX_NO_HALF_DUPLEX)) {
-		dev->phy_feat_exc = (SUPPORTED_1000baseT_Half |
-				     SUPPORTED_100baseT_Half |
-				     SUPPORTED_10baseT_Half);
-	}
+	if (emac_has_feature(dev, EMAC_FTR_APM821XX_NO_HALF_DUPLEX))
+		dev->phy_feat_exc = (SUPPORTED_1000baseT_Half| SUPPORTED_100baseT_Half| SUPPORTED_10baseT_Half);
 
 	/* Find PHY if any */
 	err = emac_init_phy(dev);
-	if (err != 0)
-		goto err_detach_tah;
+	if (err != 0) goto err_detach_tah;
 
 	if (dev->tah_dev) {
-		ndev->hw_features = NETIF_F_IP_CSUM | NETIF_F_SG;
+		struct tah_instance *tah_dev = dev_get_drvdata(&dev->tah_dev->dev);
+		ndev->hw_features = NETIF_F_IP_CSUM | NETIF_F_SG  
+#if defined(CONFIG_IBM_EMAC_TAH) && defined(EMAC_HW_TSO)  //ECO
+		| NETIF_F_TSO;
+		dev->ssr = &tah_dev->ssr[0];	// cache in emac for performance reasons
+#else
+		;
+#endif
 		ndev->features |= ndev->hw_features | NETIF_F_RXCSUM;
 	}
-	ndev->watchdog_timeo = 5 * HZ;
+	//ndev->watchdog_timeo = 5 * HZ;
+	ndev->watchdog_timeo = 100 * HZ;
 	if (emac_phy_supports_gige(dev->phy_mode)) {
 		ndev->netdev_ops = &emac_gige_netdev_ops;
 		dev->commac.ops = &emac_commac_sg_ops;
-	} else
-		ndev->netdev_ops = &emac_netdev_ops;
+	} else ndev->netdev_ops = &emac_netdev_ops;
 	ndev->ethtool_ops = &emac_ethtool_ops;
 
+#if LINUX_VERSION_CODE >= KERNEL_VERSION(4,19,0)
 	/* MTU range: 46 - 1500 or whatever is in OF */
 	ndev->min_mtu = EMAC_MIN_MTU;
 	ndev->max_mtu = dev->max_mtu;
+#endif
 
 	netif_carrier_off(ndev);
 
 	err = register_netdev(ndev);
 	if (err) {
-		printk(KERN_ERR "%pOF: failed to register net device (%d)!\n",
-		       np, err);
+		printk(KERN_ERR "%pOF: failed to register net device (%d)!\n", np, err);
 		goto err_detach_tah;
 	}
 
-	/* Set our drvdata last as we don't want them visible until we are
-	 * fully initialized
-	 */
+	/* Set our drvdata last as we don't want them visible until we are fully initialized */
 	wmb();
 	platform_set_drvdata(ofdev, dev);
 
 	/* There's a new kid in town ! Let's tell everybody */
 	wake_up_all(&emac_probe_wait);
 
-
-	printk(KERN_INFO "%s: EMAC-%d %pOF, MAC %pM\n",
-	       ndev->name, dev->cell_index, np, ndev->dev_addr);
+	printk(KERN_INFO "%s: EMAC-%d %pOF, MAC %pM\n", ndev->name, dev->cell_index, np, ndev->dev_addr);
 
 	if (dev->phy_mode == PHY_INTERFACE_MODE_SGMII)
 		printk(KERN_NOTICE "%s: in SGMII mode\n", ndev->name);
 
 	if (dev->phy.address >= 0)
-		printk("%s: found %s PHY (0x%02x)\n", ndev->name,
-		       dev->phy.def->name, dev->phy.address);
-
-	/* Life is good */
-	return 0;
+		printk("%s: found %s PHY (0x%02x)\n", ndev->name, dev->phy.def->name, dev->phy.address);
 
-	/* I have a bad feeling about this ... */
+	/* performance optimization: cache time crtical register addresses */
+	//dev->mr0 = &dev->emacp->mr0; //ECO
+	dev->tmr0 = &dev->emacp->tmr0;
+
+#if defined(CONFIG_IBM_EMAC_SYSFS)
+	/* Register sys fs hooks */
+	err = sysfs_create_group(&dev->ndev->dev.kobj, &ibm_emac_attr_group);
+	if (err) {
+		printk("WARN: %s: failed to create sys interfaces for EMAC-%d %s\n",
+		ndev->name, dev->cell_index, np->full_name);
+		return err;
+	}
+#endif
+	return 0;		/* Life is good */
 
  err_detach_tah:
-	if (emac_has_feature(dev, EMAC_FTR_HAS_TAH))
-		tah_detach(dev->tah_dev, dev->tah_port);
+	if (emac_has_feature(dev, EMAC_FTR_HAS_TAH)) tah_detach(dev->tah_dev, dev->tah_port);
  err_detach_rgmii:
-	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII))
-		rgmii_detach(dev->rgmii_dev, dev->rgmii_port);
+	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII)) rgmii_detach(dev->rgmii_dev, dev->rgmii_port);
  err_detach_zmii:
-	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII))
-		zmii_detach(dev->zmii_dev, dev->zmii_port);
+	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII)) zmii_detach(dev->zmii_dev, dev->zmii_port);
  err_unreg_commac:
 	mal_unregister_commac(dev->mal, &dev->commac);
  err_rel_deps:
@@ -3207,17 +3090,13 @@
  err_reg_unmap:
 	iounmap(dev->emacp);
  err_irq_unmap:
-	if (dev->wol_irq)
-		irq_dispose_mapping(dev->wol_irq);
-	if (dev->emac_irq)
-		irq_dispose_mapping(dev->emac_irq);
+	if (dev->wol_irq)	irq_dispose_mapping(dev->wol_irq);
+	if (dev->emac_irq)	irq_dispose_mapping(dev->emac_irq);
  err_free:
 	free_netdev(ndev);
  err_gone:
 	/* if we were on the bootlist, remove us as we won't show up and
-	 * wake up all waiters to notify them in case they were waiting
-	 * on us
-	 */
+	 * wake up all waiters to notify them in case they were waiting on us */
 	if (blist) {
 		*blist = NULL;
 		wake_up_all(&emac_probe_wait);
@@ -3225,29 +3104,22 @@
 	return err;
 }
 
-static int emac_remove(struct platform_device *ofdev)
-{
+static int emac_remove(struct platform_device *ofdev) {
 	struct emac_instance *dev = platform_get_drvdata(ofdev);
 
 	DBG(dev, "remove" NL);
 
-	unregister_netdev(dev->ndev);
+#if defined(CONFIG_IBM_EMAC_SYSFS)
+	sysfs_remove_group(&dev->ndev->dev.kobj, &ibm_emac_attr_group);
+#endif
 
 	cancel_work_sync(&dev->reset_work);
+	if (emac_has_feature(dev, EMAC_FTR_HAS_TAH))	tah_detach(dev->tah_dev, dev->tah_port);
+	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII))	rgmii_detach(dev->rgmii_dev, dev->rgmii_port);
+	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII))	zmii_detach(dev->zmii_dev, dev->zmii_port);
 
-	if (emac_has_feature(dev, EMAC_FTR_HAS_TAH))
-		tah_detach(dev->tah_dev, dev->tah_port);
-	if (emac_has_feature(dev, EMAC_FTR_HAS_RGMII))
-		rgmii_detach(dev->rgmii_dev, dev->rgmii_port);
-	if (emac_has_feature(dev, EMAC_FTR_HAS_ZMII))
-		zmii_detach(dev->zmii_dev, dev->zmii_port);
-
-	if (dev->phy_dev)
-		phy_disconnect(dev->phy_dev);
-
-	if (dev->mii_bus)
-		mdiobus_unregister(dev->mii_bus);
-
+	if (dev->phy_dev) phy_disconnect(dev->phy_dev);
+	if (dev->mii_bus) mdiobus_unregister(dev->mii_bus);
 	busy_phy_map &= ~(1 << dev->phy.address);
 	DBG(dev, "busy_phy_map now %#x" NL, busy_phy_map);
 
@@ -3256,31 +3128,21 @@
 
 	iounmap(dev->emacp);
 
-	if (dev->wol_irq)
-		irq_dispose_mapping(dev->wol_irq);
-	if (dev->emac_irq)
-		irq_dispose_mapping(dev->emac_irq);
+	if (dev->wol_irq) 	irq_dispose_mapping(dev->wol_irq);
+	if (dev->emac_irq)	irq_dispose_mapping(dev->emac_irq);
 
 	free_netdev(dev->ndev);
-
 	return 0;
 }
 
 /* XXX Features in here should be replaced by properties... */
-static const struct of_device_id emac_match[] =
-{
-	{
-		.type		= "network",
-		.compatible	= "ibm,emac",
-	},
-	{
-		.type		= "network",
-		.compatible	= "ibm,emac4",
-	},
-	{
-		.type		= "network",
-		.compatible	= "ibm,emac4sync",
-	},
+static const struct of_device_id emac_match[] = {
+	{	.type		= "network",
+		.compatible	= "ibm,emac", },
+	{	.type		= "network",
+		.compatible	= "ibm,emac4", },
+	{	.type		= "network",
+		.compatible	= "ibm,emac4sync", },
 	{},
 };
 MODULE_DEVICE_TABLE(of, emac_match);
@@ -3294,8 +3156,7 @@
 	.remove = emac_remove,
 };
 
-static void __init emac_make_bootlist(void)
-{
+static void __init emac_make_bootlist(void) {
 	struct device_node *np = NULL;
 	int j, max, i = 0;
 	int cell_indices[EMAC_BOOT_LIST_SIZE];
@@ -3304,13 +3165,10 @@
 	while((np = of_find_all_nodes(np)) != NULL) {
 		const u32 *idx;
 
-		if (of_match_node(emac_match, np) == NULL)
-			continue;
-		if (of_get_property(np, "unused", NULL))
-			continue;
+		if (of_match_node(emac_match, np) == NULL) continue;
+		if (of_get_property(np, "unused", NULL)) continue;
 		idx = of_get_property(np, "cell-index", NULL);
-		if (idx == NULL)
-			continue;
+		if (idx == NULL) continue;
 		cell_indices[i] = *idx;
 		emac_boot_list[i++] = of_node_get(np);
 		if (i >= EMAC_BOOT_LIST_SIZE) {
@@ -3330,32 +3188,23 @@
 		}
 }
 
-static int __init emac_init(void)
-{
+static int __init emac_init(void) {
 	int rc;
 
 	printk(KERN_INFO DRV_DESC ", version " DRV_VERSION "\n");
-
-	/* Build EMAC boot list */
-	emac_make_bootlist();
+	emac_make_bootlist();		/* Build EMAC boot list */
 
 	/* Init submodules */
 	rc = mal_init();
-	if (rc)
-		goto err;
+	if (rc)	goto err;
 	rc = zmii_init();
-	if (rc)
-		goto err_mal;
+	if (rc)	goto err_mal;
 	rc = rgmii_init();
-	if (rc)
-		goto err_zmii;
+	if (rc)	goto err_zmii;
 	rc = tah_init();
-	if (rc)
-		goto err_rgmii;
+	if (rc)	goto err_rgmii;
 	rc = platform_driver_register(&emac_driver);
-	if (rc)
-		goto err_tah;
-
+	if (rc)	goto err_tah;
 	return 0;
 
  err_tah:
@@ -3370,21 +3219,19 @@
 	return rc;
 }
 
-static void __exit emac_exit(void)
-{
+static void __exit emac_exit(void) {
 	int i;
 
 	platform_driver_unregister(&emac_driver);
-
 	tah_exit();
 	rgmii_exit();
 	zmii_exit();
 	mal_exit();
 
 	/* Destroy EMAC boot list */
-	for (i = 0; i < EMAC_BOOT_LIST_SIZE; i++)
-		of_node_put(emac_boot_list[i]);
+	for (i = 0; i < EMAC_BOOT_LIST_SIZE; i++) of_node_put(emac_boot_list[i]);
 }
 
 module_init(emac_init);
 module_exit(emac_exit);
+
--- a/drivers/net/ethernet/ibm/emac/core.h	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/core.h	2019-04-02 11:28:08.915058000 +0200
@@ -22,8 +22,8 @@
  * option) any later version.
  *
  */
-#ifndef __IBM_NEWEMAC_CORE_H
-#define __IBM_NEWEMAC_CORE_H
+#ifndef __IBM_EMAC_CORE_H
+#define __IBM_EMAC_CORE_H
 
 #include <linux/module.h>
 #include <linux/list.h>
@@ -33,6 +33,18 @@
 #include <linux/dma-mapping.h>
 #include <linux/spinlock.h>
 #include <linux/of_platform.h>
+#include <linux/ctype.h>
+#include <linux/etherdevice.h>
+#include <linux/version.h>
+#include <linux/skbuff.h>
+#include <linux/tcp.h>
+
+#include <linux/delay.h>
+#include <linux/of.h>
+#include <linux/of_address.h>
+#include <linux/of_irq.h>
+#include <linux/of_net.h>
+#include <linux/of_mdio.h>
 #include <linux/slab.h>
 
 #include <asm/io.h>
@@ -45,45 +57,72 @@
 #include "mal.h"
 #include "tah.h"
 #include "debug.h"
+#include <asm/ppc4xx_ocm.h>
 
-#define NUM_TX_BUFF			CONFIG_IBM_EMAC_TXB
-#define NUM_RX_BUFF			CONFIG_IBM_EMAC_RXB
+#define EMAC_HW_TSO
+
+/* MDIO macro's */
+#define CONCAT(x,y,z)		x ## y ## z
+#if defined(CONFIG_IBM_EMAC_RGMII)
+#define MDIO(op, dev)		CONCAT(rgmii_,op,_mdio)(dev->rgmii_dev, dev->rgmii_port)	
+#elif defined (CONFIG_IBM_EMAC_ZMII)
+#define MDIO(op, dev)		CONCAT(zmii_,op,_mdio)(dev->zmii_dev, dev->zmii_port)		
+#endif
+
+#define NUM_TX_BUFF		CONFIG_IBM_EMAC_TXB
+#define NUM_RX_BUFF		CONFIG_IBM_EMAC_RXB
+
+#ifdef CONFIG_IBM_EMAC_MASK_CEXT
+#define TX_FIFO_SYNC_USEC	20
+#endif
 
 /* Simple sanity check */
-#if NUM_TX_BUFF > 256 || NUM_RX_BUFF > 256
+#if NUM_TX_BUFF >  256|| NUM_RX_BUFF > 256
 #error Invalid number of buffer descriptors (greater than 256)
 #endif
+#define ISPOWEROF2(X)		((X & (X - 1)) == 0)
+
+#if ISPOWEROF2(NUM_TX_BUFF)
+#define NXT_TX_SLOT(slot)	(slot + 1) & ~NUM_TX_BUFF;
+#else
+#define NXT_TX_SLOT(slot)	(slot + 1) % NUM_TX_BUFF;
+#endif
+#if ISPOWEROF2(NUM_RX_BUFF)
+#define NXT_RX_SLOT(slot)	(slot + 1) & ~NUM_RX_BUFF;
+#else
+#define NXT_RX_SLOT(slot)	(slot + 1) % NUM_RX_BUFF;
+#endif
 
 #define EMAC_MIN_MTU			46
 
 /* Maximum L2 header length (VLAN tagged, no FCS) */
 #define EMAC_MTU_OVERHEAD		(6 * 2 + 2 + 4)
 
+/* minimum number of free TX descriptors required to wake up TX process */
+#define EMAC_TX_WAKEUP_THRESH		(NUM_TX_BUFF / 4)
+
+/* If packet size is less than this number, we allocate small skb and copy packet
+ * contents into it instead of just sending original big skb up
+ */
+#define EMAC_RX_COPY_THRESH		CONFIG_IBM_EMAC_RX_COPY_THRESHOLD
+
 /* RX BD size for the given MTU */
-static inline int emac_rx_size(int mtu)
-{
-	if (mtu > ETH_DATA_LEN)
-		return MAL_MAX_RX_SIZE;
-	else
-		return mal_rx_size(ETH_DATA_LEN + EMAC_MTU_OVERHEAD);
+static inline int emac_rx_size(int mtu) {
+	if (mtu > ETH_DATA_LEN)	return MAL_MAX_RX_SIZE;
+	else return mal_rx_size(ETH_DATA_LEN + EMAC_MTU_OVERHEAD);
 }
 
-#define EMAC_DMA_ALIGN(x)		ALIGN((x), dma_get_cache_alignment())
-
-#define EMAC_RX_SKB_HEADROOM		\
-	EMAC_DMA_ALIGN(CONFIG_IBM_EMAC_RX_SKB_HEADROOM)
+//#define EMAC_RX_SKB_HEADROOM	EMAC_DMA_ALIGN(CONFIG_IBM_EMAC_RX_SKB_HEADROOM)
 
 /* Size of RX skb for the given MTU */
-static inline int emac_rx_skb_size(int mtu)
-{
+static inline int emac_rx_skb_size(int mtu) {
 	int size = max(mtu + EMAC_MTU_OVERHEAD, emac_rx_size(mtu));
-	return EMAC_DMA_ALIGN(size + 2) + EMAC_RX_SKB_HEADROOM;
+	return SKB_DATA_ALIGN(size + NET_IP_ALIGN);
 }
 
 /* RX DMA sync size */
-static inline int emac_rx_sync_size(int mtu)
-{
-	return EMAC_DMA_ALIGN(emac_rx_size(mtu) + 2);
+static inline int emac_rx_sync_size(int mtu) {
+	return SKB_DATA_ALIGN(emac_rx_size(mtu) + NET_IP_ALIGN);
 }
 
 /* Driver statistcs is split into two parts to make it more cache friendly:
@@ -162,13 +201,14 @@
 };
 
 #define EMAC_ETHTOOL_STATS_COUNT	((sizeof(struct emac_stats) + \
-					  sizeof(struct emac_error_stats)) \
-					 / sizeof(u64))
+	sizeof(struct emac_error_stats)) / sizeof(u64))
 
 struct emac_instance {
 	struct net_device		*ndev;
-	struct emac_regs		__iomem *emacp;
+	struct resource			rsrc_regs;
+	struct emac_regs __iomem 	*emacp;
 	struct platform_device		*ofdev;
+    struct device	        *ofdev_dev;     // Cache ofdev->dev
 	struct device_node		**blist; /* bootlist entry */
 
 	/* MAL linkage */
@@ -224,6 +264,11 @@
 	/* OPB bus frequency in Mhz */
 	u32				opb_bus_freq;
 
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	/* PLB bus frequency in Mhz */
+	u32				plb_bus_freq;
+#endif
+
 	/* Cell index within an ASIC (for clk mgmnt) */
 	u32				cell_index;
 
@@ -245,8 +290,10 @@
 	u32				xaht_slots_shift;
 	u32				xaht_width_shift;
 
-	/* Descriptor management
-	 */
+	u32 __iomem		*mr0;			/* Special ECO */
+	u32 __iomem		*tmr0;			/* Special ECO */
+
+	/* Descriptor management */
 	struct mal_descriptor		*tx_desc;
 	int				tx_cnt;
 	int				tx_slot;
@@ -260,117 +307,87 @@
 
 	struct sk_buff			*tx_skb[NUM_TX_BUFF];
 	struct sk_buff			*rx_skb[NUM_RX_BUFF];
-
-	/* Stats
-	 */
+	
+	/* Stats */
 	struct emac_error_stats		estats;
 	struct emac_stats 		stats;
 
-	/* Misc
-	 */
+	/* Misc	 */
+#ifdef CONFIG_IBM_EMAC_MASK_CEXT
+	atomic_t			idle_mode;
+	atomic_t 			mask_cext_enable;
+#endif
 	int				reset_failed;
 	int				stop_timeout;	/* in us */
 	int				no_mcast;
 	int				mcast_pending;
 	int				opened;
+#if defined(CONFIG_IBM_EMAC_TAH) && defined(EMAC_HW_TSO)  //ECO
+	u32				*ssr;			/* Special ECO */
+#endif
 	struct work_struct		reset_work;
 	spinlock_t			lock;
 };
 
-/*
- * Features of various EMAC implementations
- */
+/* Features of various EMAC implementations */
 
-/*
- * No flow control on 40x according to the original driver
- */
+/* No flow control on 40x according to the original driver*/
 #define EMAC_FTR_NO_FLOW_CONTROL_40x	0x00000001
-/*
- * Cell is an EMAC4
- */
+/* Cell is an EMAC4 */
 #define EMAC_FTR_EMAC4			0x00000002
-/*
- * For the 440SPe, AMCC inexplicably changed the polarity of
- * the "operation complete" bit in the MII control register.
- */
+/* For the 440SPe, AMCC inexplicably changed the polarity of
+ * the "operation complete" bit in the MII control register */
 #define EMAC_FTR_STACR_OC_INVERT	0x00000004
-/*
- * Set if we have a TAH.
- */
+/* Set if we have a TAH.*/
 #define EMAC_FTR_HAS_TAH		0x00000008
-/*
- * Set if we have a ZMII.
- */
+/* Set if we have a ZMII.*/
 #define EMAC_FTR_HAS_ZMII		0x00000010
-/*
- * Set if we have a RGMII.
- */
+/* Set if we have a RGMII.*/
 #define EMAC_FTR_HAS_RGMII		0x00000020
-/*
- * Set if we have new type STACR with STAOPC
- */
+/* Set if we have new type STACR with STAOPC */
 #define EMAC_FTR_HAS_NEW_STACR		0x00000040
-/*
- * Set if we need phy clock workaround for 440gx
- */
+/* Set if we need phy clock workaround for 440gx */
 #define EMAC_FTR_440GX_PHY_CLK_FIX	0x00000080
-/*
- * Set if we need phy clock workaround for 440ep or 440gr
- */
+/* Set if we need phy clock workaround for 440ep or 440gr */
 #define EMAC_FTR_440EP_PHY_CLK_FIX	0x00000100
-/*
- * The 405EX and 460EX contain the EMAC4SYNC core
- */
+/* The 405EX and 460EX contain the EMAC4SYNC core */
 #define EMAC_FTR_EMAC4SYNC		0x00000200
-/*
- * Set if we need phy clock workaround for 460ex or 460gt
- */
+/* Set if we need phy clock workaround for 460ex or 460gt */
 #define EMAC_FTR_460EX_PHY_CLK_FIX	0x00000400
-/*
- * APM821xx requires Jumbo frame size set explicitly
- */
-#define EMAC_APM821XX_REQ_JUMBO_FRAME_SIZE	0x00000800
-/*
- * APM821xx does not support Half Duplex mode
- */
-#define EMAC_FTR_APM821XX_NO_HALF_DUPLEX	0x00001000
+/* APM821xx requires Jumbo frame size set explicitly */
+#define EMAC_APM821XX_REQ_JUMBO_FRAME_SIZE 0x00000800
+/* APM821xx does not support Half Duplex mode */
+#define EMAC_FTR_APM821XX_NO_HALF_DUPLEX 0x00001000
+#define EMAC_FTR_APM821XX_PHY_CLK_FIX	0x000002000
 
 /* Right now, we don't quite handle the always/possible masks on the
  * most optimal way as we don't have a way to say something like
- * always EMAC4. Patches welcome.
- */
+ * always EMAC4. Patches welcome. */
 enum {
 	EMAC_FTRS_ALWAYS	= 0,
-
 	EMAC_FTRS_POSSIBLE	=
 #ifdef CONFIG_IBM_EMAC_EMAC4
-	    EMAC_FTR_EMAC4	| EMAC_FTR_EMAC4SYNC	|
-	    EMAC_FTR_HAS_NEW_STACR	|
-	    EMAC_FTR_STACR_OC_INVERT | EMAC_FTR_440GX_PHY_CLK_FIX |
+	EMAC_FTR_EMAC4 | EMAC_FTR_EMAC4SYNC | EMAC_FTR_HAS_NEW_STACR |
+	EMAC_FTR_STACR_OC_INVERT | EMAC_FTR_440GX_PHY_CLK_FIX |
 #endif
 #ifdef CONFIG_IBM_EMAC_TAH
-	    EMAC_FTR_HAS_TAH	|
+	EMAC_FTR_HAS_TAH |
 #endif
 #ifdef CONFIG_IBM_EMAC_ZMII
-	    EMAC_FTR_HAS_ZMII	|
+	EMAC_FTR_HAS_ZMII |
 #endif
 #ifdef CONFIG_IBM_EMAC_RGMII
-	    EMAC_FTR_HAS_RGMII	|
+	EMAC_FTR_HAS_RGMII |
 #endif
 #ifdef CONFIG_IBM_EMAC_NO_FLOW_CTRL
-	    EMAC_FTR_NO_FLOW_CONTROL_40x |
+	EMAC_FTR_NO_FLOW_CONTROL_40x |
 #endif
-	EMAC_FTR_460EX_PHY_CLK_FIX |
-	EMAC_FTR_440EP_PHY_CLK_FIX |
-	EMAC_APM821XX_REQ_JUMBO_FRAME_SIZE |
-	EMAC_FTR_APM821XX_NO_HALF_DUPLEX,
+	EMAC_FTR_460EX_PHY_CLK_FIX | EMAC_FTR_440EP_PHY_CLK_FIX |
+	EMAC_APM821XX_REQ_JUMBO_FRAME_SIZE | EMAC_FTR_APM821XX_NO_HALF_DUPLEX,
 };
 
-static inline int emac_has_feature(struct emac_instance *dev,
-				   unsigned long feature)
-{
-	return (EMAC_FTRS_ALWAYS & feature) ||
-	       (EMAC_FTRS_POSSIBLE & dev->features & feature);
+static inline int emac_has_feature(struct emac_instance *dev, unsigned long feature) {
+	return (EMAC_FTRS_ALWAYS & feature) || (EMAC_FTRS_POSSIBLE & dev->features & feature);
 }
 
 /*
@@ -389,53 +406,38 @@
 
 #define	EMAC4SYNC_XAHT_SLOTS_SHIFT	8
 #define	EMAC4SYNC_XAHT_WIDTH_SHIFT	5
+/* The largest span between slots and widths above is 3 */
+#define	EMAC_XAHT_MAX_REGS		    (1 << 3)
+
+#define	EMAC_XAHT_SLOTS(dev)        (1 << (dev)->xaht_slots_shift)
+#define	EMAC_XAHT_WIDTH(dev)        (1 << (dev)->xaht_width_shift)
+#define	EMAC_XAHT_REGS(dev)         (1 << ((dev)->xaht_slots_shift - (dev)->xaht_width_shift))
 
-#define	EMAC_XAHT_SLOTS(dev)         	(1 << (dev)->xaht_slots_shift)
-#define	EMAC_XAHT_WIDTH(dev)         	(1 << (dev)->xaht_width_shift)
-#define	EMAC_XAHT_REGS(dev)          	(1 << ((dev)->xaht_slots_shift - \
-					       (dev)->xaht_width_shift))
-
-#define	EMAC_XAHT_CRC_TO_SLOT(dev, crc)			\
-	((EMAC_XAHT_SLOTS(dev) - 1) -			\
-	 ((crc) >> ((sizeof (u32) * BITS_PER_BYTE) -	\
-		    (dev)->xaht_slots_shift)))
+#define	EMAC_XAHT_CRC_TO_SLOT(dev, crc)		((EMAC_XAHT_SLOTS(dev) - 1) - \
+	((crc) >> ((sizeof (u32) * BITS_PER_BYTE) -	(dev)->xaht_slots_shift)))
 
-#define	EMAC_XAHT_SLOT_TO_REG(dev, slot)		\
-	((slot) >> (dev)->xaht_width_shift)
+#define	EMAC_XAHT_SLOT_TO_REG(dev, slot)	((slot) >> (dev)->xaht_width_shift)
 
-#define	EMAC_XAHT_SLOT_TO_MASK(dev, slot)		\
-	((u32)(1 << (EMAC_XAHT_WIDTH(dev) - 1)) >>	\
+#define	EMAC_XAHT_SLOT_TO_MASK(dev, slot)	((u32)(1 << (EMAC_XAHT_WIDTH(dev) - 1)) >>	\
 	 ((slot) & (u32)(EMAC_XAHT_WIDTH(dev) - 1)))
 
-static inline u32 *emac_xaht_base(struct emac_instance *dev)
-{
+static inline u32 *emac_xaht_base(struct emac_instance *dev) {
 	struct emac_regs __iomem *p = dev->emacp;
-	int offset;
 
-	/* The first IAHT entry always is the base of the block of
-	 * IAHT and GAHT registers.
-	 */
-	if (emac_has_feature(dev, EMAC_FTR_EMAC4SYNC))
-		offset = offsetof(struct emac_regs, u1.emac4sync.iaht1);
+	/* The first IAHT entry always is the base of the block of IAHT and GAHT registers */
+	if (likely(emac_has_feature(dev, EMAC_FTR_EMAC4SYNC)))
+		return (u32 *)((ptrdiff_t)p + offsetof(struct emac_regs, u1.emac4sync.iaht1));
 	else
-		offset = offsetof(struct emac_regs, u0.emac4.iaht1);
-
-	return (u32 *)((ptrdiff_t)p + offset);
+		return (u32 *)((ptrdiff_t)p + offsetof(struct emac_regs, u0.emac4.iaht1));
 }
 
-static inline u32 *emac_gaht_base(struct emac_instance *dev)
-{
-	/* GAHT registers always come after an identical number of
-	 * IAHT registers.
-	 */
+static inline u32 *emac_gaht_base(struct emac_instance *dev) {
+	/* GAHT registers always come after an identical number of IAHT registers */
 	return emac_xaht_base(dev) + EMAC_XAHT_REGS(dev);
 }
 
-static inline u32 *emac_iaht_base(struct emac_instance *dev)
-{
-	/* IAHT registers always come before an identical number of
-	 * GAHT registers.
-	 */
+static inline u32 *emac_iaht_base(struct emac_instance *dev) {
+	/* IAHT registers always come before an identical number of GAHT registers */
 	return emac_xaht_base(dev);
 }
 
@@ -466,4 +468,4 @@
 #define EMAC4_ETHTOOL_REGS_VER		4
 #define EMAC4SYNC_ETHTOOL_REGS_VER	5
 
-#endif /* __IBM_NEWEMAC_CORE_H */
+#endif /* __IBM_EMAC_CORE_H */
--- a/drivers/net/ethernet/ibm/emac/emac.h	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/emac.h	2019-03-23 15:07:30.000000000 +0100
@@ -140,7 +140,7 @@
 #define EMAC4_MR1_RFS_4K		0x00180000
 #define EMAC4_MR1_RFS_8K		0x00200000
 #define EMAC4_MR1_RFS_16K		0x00280000
-#define EMAC4_MR1_TFS_2K       		0x00020000
+#define EMAC4_MR1_TFS_2K		0x00020000
 #define EMAC4_MR1_TFS_4K		0x00030000
 #define EMAC4_MR1_TFS_8K		0x00040000
 #define EMAC4_MR1_TFS_16K		0x00050000
@@ -256,7 +256,7 @@
 
 /* EMACx_TRTR */
 #define EMAC_TRTR_SHIFT_EMAC4		24
-#define EMAC_TRTR_SHIFT		27
+#define EMAC_TRTR_SHIFT			27
 
 /* EMAC specific TX descriptor control fields (write access) */
 #define EMAC_TX_CTRL_GFCS		0x0200
@@ -265,7 +265,14 @@
 #define EMAC_TX_CTRL_RSA		0x0040
 #define EMAC_TX_CTRL_IVT		0x0020
 #define EMAC_TX_CTRL_RVT		0x0010
-#define EMAC_TX_CTRL_TAH_CSUM		0x000e
+#define EMAC_TX_CTRL_TAH_SSR0		0x0002
+#define EMAC_TX_CTRL_TAH_SSR1		0x0004
+#define EMAC_TX_CTRL_TAH_SSR2		0x0006
+#define EMAC_TX_CTRL_TAH_SSR3		0x0008
+#define EMAC_TX_CTRL_TAH_SSR4		0x000a
+#define EMAC_TX_CTRL_TAH_SSR5		0x000c
+#define EMAC_TX_CTRL_TAH_SSR(i) 	(((i) + 1) << 1)
+#define EMAC_TX_CTRL_TAH_CSUM		0x000e		//HW Chksum but no TCP segmentation offload
 
 /* EMAC specific TX descriptor status fields (read access) */
 #define EMAC_TX_ST_BFCS			0x0200
@@ -277,11 +284,13 @@
 #define EMAC_TX_ST_SC			0x0004
 #define EMAC_TX_ST_UR			0x0002
 #define EMAC_TX_ST_SQE			0x0001
-#define EMAC_IS_BAD_TX			(EMAC_TX_ST_LCS | EMAC_TX_ST_ED | \
-					 EMAC_TX_ST_EC | EMAC_TX_ST_LC | \
-					 EMAC_TX_ST_MC | EMAC_TX_ST_UR)
-#define EMAC_IS_BAD_TX_TAH		(EMAC_TX_ST_LCS | EMAC_TX_ST_ED | \
-					 EMAC_TX_ST_EC | EMAC_TX_ST_LC)
+#ifdef CONFIG_IBM_EMAC_TAH
+#define EMAC_IS_BAD_TX			(EMAC_TX_ST_LCS | EMAC_TX_ST_ED | EMAC_TX_ST_EC | EMAC_TX_ST_LC)
+#else
+#define EMAC_IS_BAD_TX			(EMAC_TX_ST_LCS | EMAC_TX_ST_ED | EMAC_TX_ST_EC | \
+					             EMAC_TX_ST_LC | EMAC_TX_ST_MC | EMAC_TX_ST_UR)
+#endif
+//#define EMAC_IS_BAD_TX_TAH		(EMAC_TX_ST_LCS | EMAC_TX_ST_ED | EMAC_TX_ST_EC | EMAC_TX_ST_LC)
 
 /* EMAC specific RX descriptor status fields (read access) */
 #define EMAC_RX_ST_OE			0x0200
--- a/drivers/net/ethernet/ibm/emac/mal.c	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/mal.c	2019-04-01 18:24:01.412575000 +0200
@@ -25,364 +25,444 @@
  *
  */
 
-#include <linux/delay.h>
-#include <linux/slab.h>
-#include <linux/of_irq.h>
-
 #include "core.h"
+#include "mal.h"
 #include <asm/dcr-regs.h>
+#include <asm/ppc4xx_ocm.h>
 
 static int mal_count;
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+static char *tx_coal_irqname[] = {"TX0 COAL", "TX1 COAL", "TX2 COAL", "TX3 COAL",};
+static char *rx_coal_irqname[] = {"RX0 COAL", "RX1 COAL", "RX2 COAL", "RX3 COAL",};
+#endif
 
-int mal_register_commac(struct mal_instance *mal, struct mal_commac *commac)
-{
+int mal_register_commac(struct mal_instance *mal, struct mal_commac *commac) {
 	unsigned long flags;
 
 	spin_lock_irqsave(&mal->lock, flags);
 
-	MAL_DBG(mal, "reg(%08x, %08x)" NL,
-		commac->tx_chan_mask, commac->rx_chan_mask);
+	MAL_DBG(mal, "reg(%08x, %08x)" NL, commac->tx_chan_mask, commac->rx_chan_mask);
 
 	/* Don't let multiple commacs claim the same channel(s) */
-	if ((mal->tx_chan_mask & commac->tx_chan_mask) ||
-	    (mal->rx_chan_mask & commac->rx_chan_mask)) {
+	if ((mal->tx_chan_mask & commac->tx_chan_mask) || (mal->rx_chan_mask & commac->rx_chan_mask)) {
 		spin_unlock_irqrestore(&mal->lock, flags);
-		printk(KERN_WARNING "mal%d: COMMAC channels conflict!\n",
-		       mal->index);
+		printk(KERN_WARNING "mal%d: COMMAC channels conflict!\n", mal->index);
 		return -EBUSY;
 	}
 
-	if (list_empty(&mal->list))
-		napi_enable(&mal->napi);
 	mal->tx_chan_mask |= commac->tx_chan_mask;
 	mal->rx_chan_mask |= commac->rx_chan_mask;
-	list_add(&commac->list, &mal->list);
 
+#if defined (CONFIG_APM821xx) // APM821xx has just a single EMAC/MAL RX/TX channel
+	napi_enable(&mal->napi);
+    mal->commac = commac;
+#else
+	if (list_empty(&mal->list)) napi_enable(&mal->napi);
+	list_add(&commac->list, &mal->list);
+#endif
 	spin_unlock_irqrestore(&mal->lock, flags);
-
 	return 0;
 }
 
-void mal_unregister_commac(struct mal_instance	*mal,
-		struct mal_commac *commac)
-{
+void mal_unregister_commac(struct mal_instance	*mal, struct mal_commac *commac) {
 	unsigned long flags;
 
 	spin_lock_irqsave(&mal->lock, flags);
-
-	MAL_DBG(mal, "unreg(%08x, %08x)" NL,
-		commac->tx_chan_mask, commac->rx_chan_mask);
+	MAL_DBG(mal, "unreg(%08x, %08x)" NL, commac->tx_chan_mask, commac->rx_chan_mask);
 
 	mal->tx_chan_mask &= ~commac->tx_chan_mask;
 	mal->rx_chan_mask &= ~commac->rx_chan_mask;
-	list_del_init(&commac->list);
-	if (list_empty(&mal->list))
-		napi_disable(&mal->napi);
 
+#if defined(CONFIG_APM821xx) // APM821xx has just a single EMAC/MAL RX/TX channel
+    mal->commac = NULL;
+    napi_disable(&mal->napi);
+#else
+	list_del_init(&commac->list);
+	if (list_empty(&mal->list))	napi_disable(&mal->napi);
+#endif
 	spin_unlock_irqrestore(&mal->lock, flags);
 }
 
-int mal_set_rcbs(struct mal_instance *mal, int channel, unsigned long size)
-{
-	BUG_ON(channel < 0 || channel >= mal->num_rx_chans ||
-	       size > MAL_MAX_RX_SIZE);
-
+int mal_set_rcbs(struct mal_instance *mal, int channel, unsigned long size) {
+	BUG_ON(channel < 0 || channel >= mal->num_rx_chans || size > MAL_MAX_RX_SIZE);
 	MAL_DBG(mal, "set_rbcs(%d, %lu)" NL, channel, size);
-
 	if (size & 0xf) {
-		printk(KERN_WARNING
-		       "mal%d: incorrect RX size %lu for the channel %d\n",
+		printk(KERN_WARNING "mal%d: incorrect RX size %lu for the channel %d\n",
 		       mal->index, size, channel);
 		return -EINVAL;
 	}
-
 	set_mal_dcrn(mal, MAL_RCBS(channel), size >> 4);
 	return 0;
 }
 
-int mal_tx_bd_offset(struct mal_instance *mal, int channel)
-{
+int mal_tx_bd_offset(struct mal_instance *mal, int channel) {
 	BUG_ON(channel < 0 || channel >= mal->num_tx_chans);
-
 	return channel * NUM_TX_BUFF;
 }
 
-int mal_rx_bd_offset(struct mal_instance *mal, int channel)
-{
+int mal_rx_bd_offset(struct mal_instance *mal, int channel) {
 	BUG_ON(channel < 0 || channel >= mal->num_rx_chans);
 	return mal->num_tx_chans * NUM_TX_BUFF + channel * NUM_RX_BUFF;
 }
 
-void mal_enable_tx_channel(struct mal_instance *mal, int channel)
-{
+void mal_enable_tx_channel(struct mal_instance *mal, int channel) {
 	unsigned long flags;
 
 	spin_lock_irqsave(&mal->lock, flags);
-
 	MAL_DBG(mal, "enable_tx(%d)" NL, channel);
-
-	set_mal_dcrn(mal, MAL_TXCASR,
-		     get_mal_dcrn(mal, MAL_TXCASR) | MAL_CHAN_MASK(channel));
-
+	set_mal_dcrn(mal, MAL_TXCASR, get_mal_dcrn(mal, MAL_TXCASR) | MAL_CHAN_MASK(channel));
 	spin_unlock_irqrestore(&mal->lock, flags);
 }
 
-void mal_disable_tx_channel(struct mal_instance *mal, int channel)
-{
+void mal_disable_tx_channel(struct mal_instance *mal, int channel) {
 	set_mal_dcrn(mal, MAL_TXCARR, MAL_CHAN_MASK(channel));
-
 	MAL_DBG(mal, "disable_tx(%d)" NL, channel);
 }
 
-void mal_enable_rx_channel(struct mal_instance *mal, int channel)
-{
+void mal_enable_rx_channel(struct mal_instance *mal, int channel) {
+	/* On some 4xx PPC's (e.g. 460EX/GT), the rx channel is a multiple of 8,
+	 * but enabling in MAL_RXCASR needs dividing by 8 value for the bitmask */
 	unsigned long flags;
-
-	/*
-	 * On some 4xx PPC's (e.g. 460EX/GT), the rx channel is a multiple
-	 * of 8, but enabling in MAL_RXCASR needs the divided by 8 value
-	 * for the bitmask
-	 */
-	if (!(channel % 8))
-		channel >>= 3;
+	if (!(channel % 8)) channel >>= 3;
 
 	spin_lock_irqsave(&mal->lock, flags);
-
 	MAL_DBG(mal, "enable_rx(%d)" NL, channel);
-
-	set_mal_dcrn(mal, MAL_RXCASR,
-		     get_mal_dcrn(mal, MAL_RXCASR) | MAL_CHAN_MASK(channel));
-
+	set_mal_dcrn(mal, MAL_RXCASR, get_mal_dcrn(mal, MAL_RXCASR) | MAL_CHAN_MASK(channel));
 	spin_unlock_irqrestore(&mal->lock, flags);
 }
 
-void mal_disable_rx_channel(struct mal_instance *mal, int channel)
-{
-	/*
-	 * On some 4xx PPC's (e.g. 460EX/GT), the rx channel is a multiple
-	 * of 8, but enabling in MAL_RXCASR needs the divided by 8 value
-	 * for the bitmask
-	 */
-	if (!(channel % 8))
-		channel >>= 3;
-
+void mal_disable_rx_channel(struct mal_instance *mal, int channel) {
+	/* On some 4xx PPC's (e.g. 460EX/GT), the rx channel is a multiple of 8,
+	 * but enabling in MAL_RXCASR needs dividing by 8 value for the bitmask */
+	if (!(channel % 8)) channel >>= 3;
 	set_mal_dcrn(mal, MAL_RXCARR, MAL_CHAN_MASK(channel));
-
 	MAL_DBG(mal, "disable_rx(%d)" NL, channel);
 }
 
-void mal_poll_add(struct mal_instance *mal, struct mal_commac *commac)
-{
+void mal_poll_add(struct mal_instance *mal, struct mal_commac *commac) {
 	unsigned long flags;
 
 	spin_lock_irqsave(&mal->lock, flags);
-
 	MAL_DBG(mal, "poll_add(%p)" NL, commac);
+	set_bit(MAL_COMMAC_POLL_DISABLED, &commac->flags);	/* starts disabled */
 
-	/* starts disabled */
-	set_bit(MAL_COMMAC_POLL_DISABLED, &commac->flags);
-
+#if defined (CONFIG_APM821xx) // APM821xx has just a single EMAC/MAL RX/TX channel
+    mal->poll_commac = commac;
+#else
 	list_add_tail(&commac->poll_list, &mal->poll_list);
-
+#endif
 	spin_unlock_irqrestore(&mal->lock, flags);
 }
 
-void mal_poll_del(struct mal_instance *mal, struct mal_commac *commac)
-{
+void mal_poll_del(struct mal_instance *mal, struct mal_commac *commac) {
 	unsigned long flags;
 
 	spin_lock_irqsave(&mal->lock, flags);
-
 	MAL_DBG(mal, "poll_del(%p)" NL, commac);
 
+#if defined (CONFIG_APM821xx) // APM821xx has just a single EMAC/MAL RX/TX channel
+    mal->poll_commac = NULL;
+#else
 	list_del(&commac->poll_list);
-
+#endif
 	spin_unlock_irqrestore(&mal->lock, flags);
 }
 
 /* synchronized by mal_poll() */
-static inline void mal_enable_eob_irq(struct mal_instance *mal)
-{
+static __always_inline void mal_enable_eob_irq(struct mal_instance *mal) {
 	MAL_DBG2(mal, "enable_irq" NL);
-
-	// XXX might want to cache MAL_CFG as the DCR read can be slooooow
-	set_mal_dcrn(mal, MAL_CFG, get_mal_dcrn(mal, MAL_CFG) | MAL_CFG_EOPIE);
+	// Cache MAL_CFG as the DCR read can be slooooow
+	//set_mal_dcrn(mal, MAL_CFG, get_mal_dcrn(mal, MAL_CFG) | MAL_CFG_EOPIE);
+	set_mal_dcrn(mal, MAL_CFG, mal->mal_cfg | MAL_CFG_EOPIE);  
 }
 
 /* synchronized by NAPI state */
-static inline void mal_disable_eob_irq(struct mal_instance *mal)
-{
-	// XXX might want to cache MAL_CFG as the DCR read can be slooooow
-	set_mal_dcrn(mal, MAL_CFG, get_mal_dcrn(mal, MAL_CFG) & ~MAL_CFG_EOPIE);
-
+static __always_inline void mal_disable_eob_irq(struct mal_instance *mal) {
+	// Cache MAL_CFG as the DCR read can be slooooow
+	//set_mal_dcrn(mal, MAL_CFG, get_mal_dcrn(mal, MAL_CFG) & ~MAL_CFG_EOPIE);
+	set_mal_dcrn(mal, MAL_CFG, mal->mal_cfg & ~MAL_CFG_EOPIE);  
 	MAL_DBG2(mal, "disable_irq" NL);
 }
 
-static irqreturn_t mal_serr(int irq, void *dev_instance)
-{
-	struct mal_instance *mal = dev_instance;
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+#if defined(CONFIG_460SX)
+/* Set Tx fram count */
+static void set_ic_txfthr(struct mal_instance *mal) {
+	int val = (mal->coales_param[0].tx_count) << SDR0_ICC_FTHR_SHIFT;
+	int reg = val | (1<<SDR0_ICC_FLUSH);  // Flush bit 1 enables counter
+
+	SDR_WRITE(DCRN_SDR0_ICCRTX0, reg);	/* set counter */
+	SDR_WRITE(DCRN_SDR0_ICCRTX0, val);	/* enable counter */
+	SDR_WRITE(DCRN_SDR0_ICCRTX1, reg);	/* set counter */
+	SDR_WRITE(DCRN_SDR0_ICCRTX1, val);	/* enable counter */
+	SDR_WRITE(DCRN_SDR0_ICCRTX2, reg);	/* set counter */
+	SDR_WRITE(DCRN_SDR0_ICCRTX2, val);	/* enable counter */
+	SDR_WRITE(DCRN_SDR0_ICCRTX3, reg);	/* set counter */
+	SDR_WRITE(DCRN_SDR0_ICCRTX3, val);	/* enable counter */
+	mal->enet_coales_iccrtx = reg;
+}
+/* Set Rx fram count */
+static void set_ic_rxfthr(struct mal_instance *mal) {
+	int val = (mal->coales_param[0].rx_count) << SDR0_ICC_FTHR_SHIFT;
+	int reg = val | (1<<SDR0_ICC_FLUSH);  // Flush bit 1 enables counter
+
+	SDR_WRITE(DCRN_SDR0_ICCRRX0, reg);	/* set counter */
+	SDR_WRITE(DCRN_SDR0_ICCRRX0, val);/* enable counter */
+	SDR_WRITE(DCRN_SDR0_ICCRRX1, reg);	/* set counter */
+	SDR_WRITE(DCRN_SDR0_ICCRRX1, val);/* enable counter */
+	SDR_WRITE(DCRN_SDR0_ICCRRX2, reg);	/* set counter */
+	SDR_WRITE(DCRN_SDR0_ICCRRX2, val);/* enable counter */
+	SDR_WRITE(DCRN_SDR0_ICCRRX3, reg);	/* set counter */
+	SDR_WRITE(DCRN_SDR0_ICCRRX3, val);/* enable counter */
+	mal->enet_coales_iccrrx = reg;
+}
+#endif
 
-	u32 esr = get_mal_dcrn(mal, MAL_ESR);
+#define TX_COAL_CNT (CONFIG_IBM_EMAC_TX_COAL_COUNT & COAL_FRAME_MASK)
+#define RX_COAL_CNT (CONFIG_IBM_EMAC_RX_COAL_COUNT & COAL_FRAME_MASK)
+
+inline void mal_enable_coal(struct mal_instance *mal) {
+#if defined(CONFIG_405EX)
+	/* Clear the counters */
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX, SDR0_ICC_FLUSH0 | SDR0_ICC_FLUSH1);
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX, SDR0_ICC_FLUSH0 | SDR0_ICC_FLUSH1);
+
+	/* Set Tx/Rx Timer values */
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX0, CONFIG_IBM_EMAC_TX_COAL_TIMER);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX1, CONFIG_IBM_EMAC_TX_COAL_TIMER);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX0, CONFIG_IBM_EMAC_RX_COAL_TIMER);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX1, CONFIG_IBM_EMAC_RX_COAL_TIMER);
+
+	/* Enable the Tx/Rx Coalescing interrupt */
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX, (TX_COAL_CNT<<SDR0_ICC_FTHR0_SHIFT)|(TX_COAL_CNT<<SDR0_ICC_FTHR1_SHIFT));
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX, (RX_COAL_CNT<<SDR0_ICC_FTHR0_SHIFT)|(RX_COAL_CNT<<SDR0_ICC_FTHR1_SHIFT));
+#elif defined(CONFIG_APM821xx)
+	/* Clear the counters */
+	//val = SDR0_ICC_FLUSH;
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX0, SDR0_ICC_FLUSH);	//val
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX0, SDR0_ICC_FLUSH);	//val
+
+	/* Set Tx/Rx Timer values, with SYSFS the user can change those */
+#if defined(CONFIG_IBM_EMAC_SYSFS)
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX0, mal->coales_param[0].tx_time);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX0, mal->coales_param[0].rx_time);
+	/* Enable the Tx/Rx Coalescing interrupt */	
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX0, (mal->coales_param[0].tx_count & COAL_FRAME_MASK)<<SDR0_ICC_FTHR_SHIFT);
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX0, (mal->coales_param[0].rx_count & COAL_FRAME_MASK)<<SDR0_ICC_FTHR_SHIFT);
+#else
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX0, CONFIG_IBM_EMAC_TX_COAL_TIMER);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX0, CONFIG_IBM_EMAC_RX_COAL_TIMER);
+
+	/* Enable the Tx/Rx Coalescing interrupt */	
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX0, TX_COAL_CNT << SDR0_ICC_FTHR_SHIFT);
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX0, RX_COAL_CNT << SDR0_ICC_FTHR_SHIFT);
+#endif
+#elif (defined(CONFIG_460EX) || defined(CONFIG_460GT)) && !defined(CONFIG_APM821xx)
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX0, SDR0_ICC_FLUSH);	/* Clear the counters */
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX1, SDR0_ICC_FLUSH);
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX0, SDR0_ICC_FLUSH);
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX1, SDR0_ICC_FLUSH);
+#if defined(CONFIG_460GT)
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX2, SDR0_ICC_FLUSH);
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX3, SDR0_ICC_FLUSH);
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX2, SDR0_ICC_FLUSH);
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX3, SDR0_ICC_FLUSH);
+#endif
 
-	/* Clear the error status register */
-	set_mal_dcrn(mal, MAL_ESR, esr);
+	/* Set Tx/Rx Timer values */
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX0, CONFIG_IBM_EMAC_TX_COAL_TIMER);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX1, CONFIG_IBM_EMAC_TX_COAL_TIMER);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX0, CONFIG_IBM_EMAC_RX_COAL_TIMER);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX1, CONFIG_IBM_EMAC_RX_COAL_TIMER);
+
+#if defined(CONFIG_460GT)
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX2, CONFIG_IBM_EMAC_TX_COAL_TIMER);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX3, CONFIG_IBM_EMAC_TX_COAL_TIMER);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX2, CONFIG_IBM_EMAC_RX_COAL_TIMER);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX3, CONFIG_IBM_EMAC_RX_COAL_TIMER);
+#endif
+
+	/* Enable the Tx/Rx Coalescing interrupt */
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX0, TX_COAL_CNT << SDR0_ICC_FTHR_SHIFT);
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX1, TX_COAL_CNT << SDR0_ICC_FTHR_SHIFT);
+#if defined(CONFIG_460GT)
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX2, TX_COAL_CNT << SDR0_ICC_FTHR_SHIFT);
+	mtdcri(SDR0, DCRN_SDR0_ICCRTX3, TX_COAL_CNT << SDR0_ICC_FTHR_SHIFT);
+#endif
 
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX0, RX_COAL_CNT << SDR0_ICC_FTHR_SHIFT);
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX1, RX_COAL_CNT << SDR0_ICC_FTHR_SHIFT);
+#if defined(CONFIG_460GT)
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX2, RX_COAL_CNT << SDR0_ICC_FTHR_SHIFT);
+	mtdcri(SDR0, DCRN_SDR0_ICCRRX3, RX_COAL_CNT << SDR0_ICC_FTHR_SHIFT);
+#endif
+
+#elif defined(CONFIG_460SX)
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX0, mal->coales_param[0].tx_time);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX1, mal->coales_param[1].tx_time);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX2, mal->coales_param[2].tx_time);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRTX3, mal->coales_param[3].tx_time);
+
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX0, mal->coales_param[0].rx_time);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX1, mal->coales_param[1].rx_time);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX2, mal->coales_param[2].rx_time);
+	mtdcri(SDR0, DCRN_SDR0_ICCTRRX3, mal->coales_param[3].rx_time);
+
+	set_ic_rxfthr(mal);
+	set_ic_txfthr(mal);
+#endif
+	printk(KERN_INFO "MAL: Interrupt Coalescing TxCnt:%d RxCnt:%d TxTimer:%d RxTimer:%d\n",
+		mal->coales_param[0].tx_count, mal->coales_param[0].rx_count,
+		mal->coales_param[0].tx_time, mal->coales_param[0].rx_time);
+}
+#endif
+
+static irqreturn_t mal_serr(int irq, void *dev_instance) {
+	struct mal_instance *mal = dev_instance;
+	u32 esr = get_mal_dcrn(mal, MAL_ESR);
+
+	set_mal_dcrn(mal, MAL_ESR, esr);		/* Clear the error status register */
 	MAL_DBG(mal, "SERR %08x" NL, esr);
 
 	if (esr & MAL_ESR_EVB) {
-		if (esr & MAL_ESR_DE) {
-			/* We ignore Descriptor error,
-			 * TXDE or RXDE interrupt will be generated anyway.
-			 */
-			return IRQ_HANDLED;
-		}
-
+		/* Ignore Descriptor error, TXDE or RXDE interrupt will be generated anyway */
+		if (esr & MAL_ESR_DE) return IRQ_HANDLED;
+		/* PLB error, buggy hardware or incorrect physical address in BD (i.e. bug) */
 		if (esr & MAL_ESR_PEIN) {
-			/* PLB error, it's probably buggy hardware or
-			 * incorrect physical address in BD (i.e. bug)
-			 */
 			if (net_ratelimit())
-				printk(KERN_ERR
-				       "mal%d: system error, "
-				       "PLB (ESR = 0x%08x)\n",
-				       mal->index, esr);
+				printk(KERN_ERR "mal%d: system error, PLB (ESR = 0x%08x)\n", mal->index, esr);
 			return IRQ_HANDLED;
 		}
 
-		/* OPB error, it's probably buggy hardware or incorrect
-		 * EBC setup
-		 */
+		/* OPB error, it's probably buggy hardware or incorrect EBC setup */
 		if (net_ratelimit())
-			printk(KERN_ERR
-			       "mal%d: system error, OPB (ESR = 0x%08x)\n",
-			       mal->index, esr);
+			printk(KERN_ERR "mal%d: system error, OPB (ESR = 0x%08x)\n", mal->index, esr);
 	}
 	return IRQ_HANDLED;
 }
 
-static inline void mal_schedule_poll(struct mal_instance *mal)
-{
-	if (likely(napi_schedule_prep(&mal->napi))) {
+__always_inline bool local_napi_schedule_prep(struct napi_struct *n) {
+	unsigned long val, new;
+	do {
+		val = READ_ONCE(n->state);
+		if (unlikely(val & NAPIF_STATE_DISABLE)) return false;
+		new = val | NAPIF_STATE_SCHED;
+		new |= (val & NAPIF_STATE_SCHED) / NAPIF_STATE_SCHED * NAPIF_STATE_MISSED;
+	} while (cmpxchg(&n->state, val, new) != val);
+	return !(val & NAPIF_STATE_SCHED);
+}
+
+static __always_inline void mal_schedule_poll(struct mal_instance *mal) {
+	if (likely(local_napi_schedule_prep(&mal->napi))) {
 		MAL_DBG2(mal, "schedule_poll" NL);
 		spin_lock(&mal->lock);
 		mal_disable_eob_irq(mal);
 		spin_unlock(&mal->lock);
-		__napi_schedule(&mal->napi);
-	} else
-		MAL_DBG2(mal, "already in poll" NL);
+		__napi_schedule_irqoff(&mal->napi); //__napi_schedule(&mal->napi);
+	} else MAL_DBG2(mal, "already in poll" NL);
 }
 
-static irqreturn_t mal_txeob(int irq, void *dev_instance)
-{
+static irqreturn_t mal_txeob(int irq, void *dev_instance) {
 	struct mal_instance *mal = dev_instance;
-
 	u32 r = get_mal_dcrn(mal, MAL_TXEOBISR);
 
 	MAL_DBG2(mal, "txeob %08x" NL, r);
-
 	mal_schedule_poll(mal);
 	set_mal_dcrn(mal, MAL_TXEOBISR, r);
 
 #ifdef CONFIG_PPC_DCR_NATIVE
 	if (mal_has_feature(mal, MAL_FTR_CLEAR_ICINTSTAT))
-		mtdcri(SDR0, DCRN_SDR_ICINTSTAT,
-				(mfdcri(SDR0, DCRN_SDR_ICINTSTAT) | ICINTSTAT_ICTX));
+		mtdcri(SDR0, DCRN_SDR_ICINTSTAT, (mfdcri(SDR0, DCRN_SDR_ICINTSTAT) | ICINTSTAT_ICTX));
 #endif
 
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t mal_rxeob(int irq, void *dev_instance)
-{
+static irqreturn_t mal_rxeob(int irq, void *dev_instance) {
 	struct mal_instance *mal = dev_instance;
-
 	u32 r = get_mal_dcrn(mal, MAL_RXEOBISR);
 
 	MAL_DBG2(mal, "rxeob %08x" NL, r);
-
 	mal_schedule_poll(mal);
 	set_mal_dcrn(mal, MAL_RXEOBISR, r);
 
 #ifdef CONFIG_PPC_DCR_NATIVE
 	if (mal_has_feature(mal, MAL_FTR_CLEAR_ICINTSTAT))
-		mtdcri(SDR0, DCRN_SDR_ICINTSTAT,
-				(mfdcri(SDR0, DCRN_SDR_ICINTSTAT) | ICINTSTAT_ICRX));
+		mtdcri(SDR0, DCRN_SDR_ICINTSTAT, (mfdcri(SDR0, DCRN_SDR_ICINTSTAT) | ICINTSTAT_ICRX));
 #endif
 
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t mal_txde(int irq, void *dev_instance)
-{
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+static irqreturn_t mal_coal_tx(int irq, void *dev_instance) {
+	struct mal_instance *mal = dev_instance;
+    //mal->poll_tx = 1;
+	mal_schedule_poll(mal);
+	return IRQ_HANDLED;
+}
+static irqreturn_t mal_coal_rx(int irq, void *dev_instance) {
 	struct mal_instance *mal = dev_instance;
+    //mal->poll_rx = 1;
+	mal_schedule_poll(mal);
+	return IRQ_HANDLED;
+}
+#endif
 
+static irqreturn_t mal_txde(int irq, void *dev_instance) {
+	struct mal_instance *mal = dev_instance;
 	u32 deir = get_mal_dcrn(mal, MAL_TXDEIR);
+	
 	set_mal_dcrn(mal, MAL_TXDEIR, deir);
-
 	MAL_DBG(mal, "txde %08x" NL, deir);
-
 	if (net_ratelimit())
-		printk(KERN_ERR
-		       "mal%d: TX descriptor error (TXDEIR = 0x%08x)\n",
-		       mal->index, deir);
-
+		printk(KERN_ERR "mal%d: TX descriptor error (TXDEIR = 0x%08x)\n", mal->index, deir);
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t mal_rxde(int irq, void *dev_instance)
-{
+static irqreturn_t mal_rxde(int irq, void *dev_instance) {
 	struct mal_instance *mal = dev_instance;
-	struct list_head *l;
-
 	u32 deir = get_mal_dcrn(mal, MAL_RXDEIR);
 
-	MAL_DBG(mal, "rxde %08x" NL, deir);
-
+#if defined (CONFIG_APM821xx) // APM821xx has just one single EMAC and one MAL channel
+    {   struct mal_commac *mc = mal->commac;
+#else
 	list_for_each(l, &mal->list) {
 		struct mal_commac *mc = list_entry(l, struct mal_commac, list);
-		if (deir & mc->rx_chan_mask) {
+#endif
+		if (likely(deir & mc->rx_chan_mask)) {
 			set_bit(MAL_COMMAC_RX_STOPPED, &mc->flags);
 			mc->ops->rxde(mc->dev);
 		}
 	}
-
+	MAL_DBG(mal, "rxde %08x" NL, deir);
 	mal_schedule_poll(mal);
 	set_mal_dcrn(mal, MAL_RXDEIR, deir);
-
 	return IRQ_HANDLED;
 }
 
-static irqreturn_t mal_int(int irq, void *dev_instance)
-{
+static irqreturn_t mal_int(int irq, void *dev_instance) {
 	struct mal_instance *mal = dev_instance;
 	u32 esr = get_mal_dcrn(mal, MAL_ESR);
 
-	if (esr & MAL_ESR_EVB) {
-		/* descriptor error */
-		if (esr & MAL_ESR_DE) {
-			if (esr & MAL_ESR_CIDT)
-				return mal_rxde(irq, dev_instance);
-			else
-				return mal_txde(irq, dev_instance);
-		} else { /* SERR */
-			return mal_serr(irq, dev_instance);
-		}
+	if (esr & MAL_ESR_EVB) { // ESR contains at least one error
+		if (esr & MAL_ESR_DE) {			/* descriptor error */
+			if (esr & MAL_ESR_CIDT)	return mal_rxde(irq, dev_instance);
+			else return mal_txde(irq, dev_instance);
+		} else return mal_serr(irq, dev_instance);	/* SERR */
 	}
 	return IRQ_HANDLED;
 }
 
-void mal_poll_disable(struct mal_instance *mal, struct mal_commac *commac)
-{
+void mal_poll_disable(struct mal_instance *mal, struct mal_commac *commac) {
 	/* Spinlock-type semantics: only one caller disable poll at a time */
-	while (test_and_set_bit(MAL_COMMAC_POLL_DISABLED, &commac->flags))
-		msleep(1);
+	while (test_and_set_bit(MAL_COMMAC_POLL_DISABLED, &commac->flags)) msleep(1);
 
 	/* Synchronize with the MAL NAPI poller */
 	napi_synchronize(&mal->napi);
 }
 
-void mal_poll_enable(struct mal_instance *mal, struct mal_commac *commac)
-{
+void mal_poll_enable(struct mal_instance *mal, struct mal_commac *commac) {
 	smp_wmb();
 	clear_bit(MAL_COMMAC_POLL_DISABLED, &commac->flags);
 
@@ -394,79 +474,412 @@
 	napi_schedule(&mal->napi);
 }
 
-static int mal_poll(struct napi_struct *napi, int budget)
-{
+#if defined (CONFIG_APM821xx) // Moved from core.c
+
+extern void emac_rx_enable(struct emac_instance *dev);
+
+static void parse_rx_error(struct emac_instance *dev, u16 ctrl) {
+	struct emac_error_stats *st = &dev->estats;
+
+	DBG(dev, "BD RX error %04x" NL, ctrl);
+
+	++st->rx_bd_errors;
+	if (ctrl & EMAC_RX_ST_OE)	++st->rx_bd_overrun;
+	if (ctrl & EMAC_RX_ST_BP)	++st->rx_bd_bad_packet;
+	if (ctrl & EMAC_RX_ST_RP)	++st->rx_bd_runt_packet;
+	if (ctrl & EMAC_RX_ST_SE)	++st->rx_bd_short_event;
+	if (ctrl & EMAC_RX_ST_AE)	++st->rx_bd_alignment_error;
+	if (ctrl & EMAC_RX_ST_BFCS)	++st->rx_bd_bad_fcs;
+	if (ctrl & EMAC_RX_ST_PTL)	++st->rx_bd_packet_too_long;
+	if (ctrl & EMAC_RX_ST_ORE)	++st->rx_bd_out_of_range;
+	if (ctrl & EMAC_RX_ST_IRE)	++st->rx_bd_in_range;
+}
+
+static __always_inline void recycle_rx_skb(struct emac_instance *dev, int slot, int len, unsigned char* skbdata) {
+	//struct sk_buff *skb = dev->rx_skb[slot];
+
+	DBG2(dev, "recycle %d %d" NL, slot, len);
+	if (likely(len))
+		dma_map_single(&dev->ofdev->dev, skbdata - NET_IP_ALIGN, SKB_DATA_ALIGN(len + NET_IP_ALIGN), DMA_FROM_DEVICE);
+	dev->rx_desc[slot].data_len = 0;
+	wmb();
+	dev->rx_desc[slot].ctrl = MAL_RX_CTRL_EMPTY| MAL_RX_CTRL_INTR| (slot == (NUM_RX_BUFF-1) ? MAL_RX_CTRL_WRAP : 0);
+}
+
+static __always_inline void rx_csum(struct emac_instance *dev, struct sk_buff *skb, u16 ctrl) {
+#ifdef CONFIG_IBM_EMAC_TAH
+	//if (!ctrl && dev->tah_dev) {
+	if (likely(!ctrl)) {
+		skb->ip_summed = CHECKSUM_UNNECESSARY;
+		++dev->stats.rx_packets_csum;
+	}
+#endif
+}
+
+/* ECO: local version of skb_put for performance reasons */
+#define skb_tail_pointer(skb) (skb)->tail
+#define skb_putl(skb, len) SKB_LINEAR_ASSERT(skb); skb->tail += len; skb->len  += len;
+
+// rx_sg_append was returning 0 and -1, used in ONLY in NOT comparisons --> invert return values 
+static __always_inline int rx_sg_append(struct emac_instance *dev, int slot, int len, 
+    unsigned char *data, struct sk_buff **skb_sg) {
+    struct sk_buff *skb_sgp = *skb_sg;
+	if (likely(skb_sgp != NULL)) {
+		//int len = dev->rx_desc[slot].data_len;
+		int tot_len = skb_sgp->len + len;
+
+		if (unlikely(tot_len + NET_IP_ALIGN > dev->rx_skb_size)) {
+			++dev->estats.rx_dropped_mtu;
+			dev_kfree_skb(skb_sgp);
+            skb_sg = NULL;
+		} else {  //ECO
+			//if(unlikely((dev->rx_sg_skb->tail + len) > dev->rx_sg_skb->end)) goto out;
+			memcpy(skb_tail_pointer(skb_sgp), data, len);	// dev->rx_skb[slot]->data
+			skb_putl(skb_sgp, len);
+			recycle_rx_skb(dev, slot, len, data);	//dev->rx_skb[slot]->data
+			return 1;
+		}
+	}
+	recycle_rx_skb(dev, slot, 0, dev->rx_skb[slot]->data);
+	return 0;
+}
+
+static __always_inline int __prepare_rx_skb(struct sk_buff *skb, struct emac_instance *dev, int slot,
+        struct mal_descriptor *rx_desc) {
+
+	dev->rx_skb[slot] = skb;
+    rx_desc->data_len = 0;
+    rx_desc->data_ptr =
+	    dma_map_single(&dev->ofdev->dev, skb->data - NET_IP_ALIGN, dev->rx_sync_size, DMA_FROM_DEVICE) + NET_IP_ALIGN;
+	wmb();
+	rx_desc->ctrl = MAL_RX_CTRL_EMPTY | MAL_RX_CTRL_INTR | (slot == (NUM_RX_BUFF - 1) ? MAL_RX_CTRL_WRAP : 0);
+	return 0;
+}
+
+inline int alloc_rx_skb(struct emac_instance *dev, int slot) {
+	struct sk_buff *skb;
+	skb = __netdev_alloc_skb_ip_align(dev->ndev, dev->rx_skb_size, GFP_KERNEL);
+    if (unlikely(!skb))	return -ENOMEM;
+	return __prepare_rx_skb(skb, dev, slot, &dev->rx_desc[slot]);
+}
+EXPORT_SYMBOL_GPL(alloc_rx_skb);
+
+static inline int alloc_rx_skb_napi(struct emac_instance *dev, int slot) {
+	struct sk_buff *skb;
+	skb = napi_alloc_skb(&dev->mal->napi, dev->rx_skb_size);
+    if (unlikely(!skb))	return -ENOMEM;
+	return __prepare_rx_skb(skb, dev, slot, &dev->rx_desc[slot]);
+}
+
+/* NAPI poll context */
+static int __always_inline poll_rx(void *param, int budget) {
+	register struct emac_instance *dev = param;
+	register int slot = dev->rx_slot, received = 0;
+	register int len;
+	register struct sk_buff *skb, **skb_sg;
+
+	DBG2(dev, "poll_rx(%d)" NL, budget);
+
+ again:
+	while (budget > 0) {
+		register u16 ctrl = dev->rx_desc[slot].ctrl;
+
+		if (unlikely(ctrl & MAL_RX_CTRL_EMPTY)) break;
+
+		skb = dev->rx_skb[slot];
+		mb();
+		len = dev->rx_desc[slot].data_len;
+
+		if (unlikely(!MAL_IS_SINGLE_RX(ctrl))) goto sg;
+
+		ctrl &= EMAC_BAD_RX_MASK;
+		if (unlikely(ctrl && ctrl != EMAC_RX_TAH_BAD_CSUM)) {
+			parse_rx_error(dev, ctrl);
+			++dev->estats.rx_dropped_error;
+			recycle_rx_skb(dev, slot, 0, skb->data);
+			len = 0;
+			goto next;
+		}
+
+		if (unlikely(len < ETH_HLEN)) {
+			++dev->estats.rx_dropped_stack;
+			recycle_rx_skb(dev, slot, len, skb->data);
+			goto next;
+		}
+
+		if (len < EMAC_RX_COPY_THRESH) {
+			struct sk_buff *copy_skb = napi_alloc_skb(&dev->mal->napi, len);
+			if (unlikely(!copy_skb)) goto oom;
+
+			memcpy(copy_skb->data - NET_IP_ALIGN, skb->data - NET_IP_ALIGN, len + NET_IP_ALIGN);  //is cacheable_memcpy
+			recycle_rx_skb(dev, slot, len, skb->data);
+			skb = copy_skb;
+		} else if (unlikely(alloc_rx_skb_napi(dev, slot))) goto oom;
+
+		skb_putl(skb, len); //ECO: local version of skb_put
+	push_packet:
+		skb->protocol = eth_type_trans(skb, dev->ndev);
+		rx_csum(dev, skb, ctrl);
+
+		if (unlikely(netif_receive_skb(skb) == NET_RX_DROP)) ++dev->estats.rx_dropped_stack;
+	next:
+		++dev->stats.rx_packets;
+	skip:
+		dev->stats.rx_bytes += len;
+		slot = NXT_RX_SLOT(slot);
+		--budget;
+		++received;
+		continue;
+	sg:
+        skb_sg = &dev->rx_sg_skb;
+        // ECO TODO test if unlikely
+		if (ctrl & MAL_RX_CTRL_FIRST) {
+			//BUG_ON(dev->rx_sg_skb); // panic is not really helpful here
+			if (unlikely(alloc_rx_skb_napi(dev, slot))) {
+				DBG(dev, "rx OOM %d" NL, slot);
+				++dev->estats.rx_dropped_oom;
+				recycle_rx_skb(dev, slot, 0, skb->data);
+                goto skip;
+			} else {
+				//dev->rx_sg_skb = skb;
+                *skb_sg = skb;
+				skb_putl(skb, len);
+                goto skip;
+			}
+		} else if (rx_sg_append(dev, slot, len, skb->data, skb_sg) && (ctrl & MAL_RX_CTRL_LAST)) {
+			//skb = dev->rx_sg_skb;
+            //dev->rx_sg_skb = NULL;
+            skb = *skb_sg;
+            *skb_sg = NULL; 			
+
+			ctrl &= EMAC_BAD_RX_MASK;
+			if (unlikely(ctrl && ctrl != EMAC_RX_TAH_BAD_CSUM)) {
+				parse_rx_error(dev, ctrl);
+				++dev->estats.rx_dropped_error;
+				dev_kfree_skb(skb);
+				len = 0;
+			} else goto push_packet;
+		}
+		goto skip;
+	oom:
+		DBG(dev, "rx OOM %d" NL, slot);
+		/* Drop the packet and recycle skb */
+		++dev->estats.rx_dropped_oom;
+		recycle_rx_skb(dev, slot, 0, skb->data);
+		goto next;
+	}
+
+	if (received) {
+		DBG2(dev, "rx %d BDs" NL, received);
+		dev->rx_slot = slot;
+	}
+
+	if (unlikely(budget && test_bit(MAL_COMMAC_RX_STOPPED, &dev->commac.flags))) {
+		mb();
+		if (!(dev->rx_desc[slot].ctrl & MAL_RX_CTRL_EMPTY)) {
+			DBG2(dev, "rx restart" NL);
+			received = 0;
+			goto again;
+		}
+
+		if (dev->rx_sg_skb) {
+			DBG2(dev, "dropping partial rx packet" NL);
+			++dev->estats.rx_dropped_error;
+			dev_kfree_skb(dev->rx_sg_skb);
+			dev->rx_sg_skb = NULL;
+		}
+
+		clear_bit(MAL_COMMAC_RX_STOPPED, &dev->commac.flags);
+		mal_enable_rx_channel(dev->mal, dev->mal_rx_chan);
+		emac_rx_enable(dev);
+		dev->rx_slot = 0;
+	}
+	return received;
+}
+
+/* Tx lock BHs */
+static void parse_tx_error(struct emac_instance *dev, u16 ctrl) {
+	struct emac_error_stats *st = &dev->estats;
+
+	DBG(dev, "BD TX error %04x" NL, ctrl);
+
+	++st->tx_bd_errors;
+	if (ctrl & EMAC_TX_ST_BFCS)	++st->tx_bd_bad_fcs;
+	if (ctrl & EMAC_TX_ST_LCS)	++st->tx_bd_carrier_loss;
+	if (ctrl & EMAC_TX_ST_ED)	++st->tx_bd_excessive_deferral;
+	if (ctrl & EMAC_TX_ST_EC)	++st->tx_bd_excessive_collisions;
+	if (ctrl & EMAC_TX_ST_LC)	++st->tx_bd_late_collision;
+	if (ctrl & EMAC_TX_ST_MC)	++st->tx_bd_multple_collisions;
+	if (ctrl & EMAC_TX_ST_SC)	++st->tx_bd_single_collision;
+	if (ctrl & EMAC_TX_ST_UR)	++st->tx_bd_underrun;
+	if (ctrl & EMAC_TX_ST_SQE)	++st->tx_bd_sqe;
+}
+
+#define netif_queue_stopped(ndev) test_bit(__QUEUE_STATE_DRV_XOFF, &ndev->_tx[0].state)
+static __always_inline void poll_tx(struct emac_instance *dev) {
+	DBG2(dev, "poll_tx, %d %d" NL, dev->tx_cnt, dev->ack_slot);
+
+    netif_tx_lock_bh(dev->ndev);
+	if (likely(dev->tx_cnt)) {	// We have something to transmit
+    	register u16 ctrl;
+		register int slot = dev->ack_slot, n = 0;
+		//register int slot = ack_slot, n = 0;
+        register struct sk_buff *skb;
+	again:  /* Release slots and skb's for packets transmited */
+		ctrl = dev->tx_desc[slot].ctrl;
+ 		if (likely(!(ctrl & MAL_TX_CTRL_READY))) {
+			skb = dev->tx_skb[slot];
+			++n;
+
+			if (likely(skb)) {
+				dev_kfree_skb(skb);
+				dev->tx_skb[slot] = NULL;
+			}
+			slot = NXT_TX_SLOT(slot);
+			if (unlikely(ctrl & EMAC_IS_BAD_TX)) parse_tx_error(dev, ctrl);
+			if (--dev->tx_cnt) goto again;
+		} 
+
+		if (likely(n)) {
+			dev->ack_slot = slot;
+			//ack_slot = slot;
+ 			if (unlikely(netif_queue_stopped(dev->ndev) && dev->tx_cnt < EMAC_TX_WAKEUP_THRESH))
+				netif_wake_queue(dev->ndev);
+			DBG2(dev, "tx %d pkts" NL, n);
+		}
+ 	}
+#ifdef CONFIG_IBM_EMAC_MASK_CEXT
+	else {
+		DBG(dev, "Testing for idle... " NL);
+		if (atomic_read(&dev->mask_cext_enable)) {
+			if (!atomic_read(&dev->idle_mode)) {
+			      DBG(dev, "Entering idle mode" NL);
+			      emac_start_idlemode(dev);
+			      atomic_set(&dev->idle_mode, 1);
+			} else DBG(dev, "Already In Idle Mode" NL);
+		}
+	}
+#endif
+	netif_tx_unlock_bh(dev->ndev);
+}
+
+/* NAPI poll context */
+static __always_inline int peek_rx_sg(struct emac_instance *dev) {
+	register int slot = dev->rx_slot;  // start will last used slot
+    register u16 *rx_desc_ctrl = (u16 *)&dev->rx_desc[slot];
+	while (1) {
+        if (*rx_desc_ctrl & MAL_RX_CTRL_EMPTY) return 0;
+		else if (unlikely(*rx_desc_ctrl & MAL_RX_CTRL_LAST)) return 1;
+		slot = NXT_RX_SLOT(slot);
+        rx_desc_ctrl += sizeof(struct mal_descriptor);
+        if (unlikely(slot == 0)) rx_desc_ctrl = (u16 *) &dev->rx_desc[0]; //wrap around
+	}
+}
+
+#endif
+
+static int mal_poll(struct napi_struct *napi, int budget) {
 	struct mal_instance *mal = container_of(napi, struct mal_instance, napi);
-	struct list_head *l;
-	int received = 0;
+	register int received = 0;
 	unsigned long flags;
-
+#if defined (CONFIG_APM821xx) // APM821xx has just a single EMAC/MAL RX/TX channel
+    #define continue_if(x)
+    register struct mal_commac *mc = mal->poll_commac;
+    register struct emac_instance *dev = mc->dev;
+#else
+    #define continue_if(x) if (x) continue
+	struct list_head *l;
+#endif
 	MAL_DBG2(mal, "poll(%d)" NL, budget);
 
 	/* Process TX skbs */
+#if defined (CONFIG_APM821xx) // APM821xx has just a single EMAC/MAL RX/TX channel
+	//if (mal->poll_tx) {
+    {
+#else
 	list_for_each(l, &mal->poll_list) {
-		struct mal_commac *mc =
-			list_entry(l, struct mal_commac, poll_list);
-		mc->ops->poll_tx(mc->dev);
-	}
+		struct mal_commac *mc =	list_entry(l, struct mal_commac, poll_list);
+        struct emac_instance *dev = mc->dev;
+#endif
+		//mc->ops->poll_tx(dev); // Poll if we have something to transmit
+		poll_tx(dev);
+        //mal->poll_tx = 0;
+ 	}
 
-	/* Process RX skbs.
-	 *
-	 * We _might_ need something more smart here to enforce polling
-	 * fairness.
-	 */
-	list_for_each(l, &mal->poll_list) {
-		struct mal_commac *mc =
-			list_entry(l, struct mal_commac, poll_list);
-		int n;
-		if (unlikely(test_bit(MAL_COMMAC_POLL_DISABLED, &mc->flags)))
-			continue;
-		n = mc->ops->poll_rx(mc->dev, budget - received);
+
+	/* Process RX skbs */
+#if defined (CONFIG_APM821xx) // APM821xx has just a single EMAC/MAL RX/TX channel
+    //if (!mal->poll_rx) return 0;
+	if (likely(mc && !test_bit(MAL_COMMAC_POLL_DISABLED, &mc->flags))) {
+#else
+	 // We _might_ need something more smart here to enforce polling fairness. 
+    list_for_each(l, &mal->poll_list) {
+		struct mal_commac *mc =	list_entry(l, struct mal_commac, poll_list);
+        struct emac_instance *dev = mc->dev;
+#endif
+
+		register int n;
+        continue_if(unlikely(test_bit(MAL_COMMAC_POLL_DISABLED, &mc->flags)));
+		//n = mc->ops->poll_rx(dev, budget - received);
+		n = poll_rx(dev, budget - received);
 		if (n) {
 			received += n;
-			if (received >= budget)
-				return budget;
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,19,0)
+			budget -= n;
+			if (budget <= 0) return received;  //goto more_work; // XXX What if this is the last one ?
+#else
+			if (received >= budget) return budget;
+#endif
 		}
 	}
 
+	/* We need to disable IRQs to protect from RXDE IRQ here */
+#if LINUX_VERSION_CODE < KERNEL_VERSION(4,19,0)
+	spin_lock_irqsave(&mal->lock, flags);
+	__napi_complete(napi);
+	mal_enable_eob_irq(mal);
+	spin_unlock_irqrestore(&mal->lock, flags);
+#else
 	if (napi_complete_done(napi, received)) {
-		/* We need to disable IRQs to protect from RXDE IRQ here */
 		spin_lock_irqsave(&mal->lock, flags);
 		mal_enable_eob_irq(mal);
 		spin_unlock_irqrestore(&mal->lock, flags);
 	}
-
+#endif
 	/* Check for "rotting" packet(s) */
+#if defined (CONFIG_APM821xx) // APM821xx has just a single EMAC/MAL RX/TX channel
+	if (likely(mc && !test_bit(MAL_COMMAC_POLL_DISABLED, &mc->flags))) {
+#else
 	list_for_each(l, &mal->poll_list) {
-		struct mal_commac *mc =
-			list_entry(l, struct mal_commac, poll_list);
-		if (unlikely(test_bit(MAL_COMMAC_POLL_DISABLED, &mc->flags)))
-			continue;
-		if (unlikely(mc->ops->peek_rx(mc->dev) ||
-			     test_bit(MAL_COMMAC_RX_STOPPED, &mc->flags))) {
+		struct mal_commac *mc =	list_entry(l, struct mal_commac, poll_list);
+        struct emac_instance *dev = mc->dev;
+#endif
+        continue_if(unlikely(test_bit(MAL_COMMAC_POLL_DISABLED, &mc->flags)));
+
+		//if (unlikely(mc->ops->peek_rx(dev) || test_bit(MAL_COMMAC_RX_STOPPED, &mc->flags))) {
+		if (unlikely(peek_rx_sg(dev) || test_bit(MAL_COMMAC_RX_STOPPED, &mc->flags))) {
 			MAL_DBG2(mal, "rotting packet" NL);
-			if (!napi_reschedule(napi))
-				goto more_work;
+			if (!napi_reschedule(napi))	return received; //	goto more_work;
 
 			spin_lock_irqsave(&mal->lock, flags);
 			mal_disable_eob_irq(mal);
 			spin_unlock_irqrestore(&mal->lock, flags);
 		}
-		mc->ops->poll_tx(mc->dev);
+		//mc->ops->poll_tx(dev);
+		poll_tx(dev);
 	}
 
- more_work:
+// more_work:
 	MAL_DBG2(mal, "poll() %d <- %d" NL, budget, received);
 	return received;
 }
 
-static void mal_reset(struct mal_instance *mal)
-{
+static void mal_reset(struct mal_instance *mal) {
 	int n = 10;
 
 	MAL_DBG(mal, "reset" NL);
-
 	set_mal_dcrn(mal, MAL_CFG, MAL_CFG_SR);
+    mal->mal_cfg = MAL_CFG_SR;
 
 	/* Wait for reset to complete (1 system clock) */
 	while ((get_mal_dcrn(mal, MAL_CFG) & MAL_CFG_SR) && n)
@@ -476,10 +889,8 @@
 		printk(KERN_ERR "mal%d: reset timeout\n", mal->index);
 }
 
-int mal_get_regs_len(struct mal_instance *mal)
-{
-	return sizeof(struct emac_ethtool_regs_subhdr) +
-	    sizeof(struct mal_regs);
+int mal_get_regs_len(struct mal_instance *mal) {
+	return sizeof(struct emac_ethtool_regs_subhdr) + sizeof(struct mal_regs);
 }
 
 void *mal_dump_regs(struct mal_instance *mal, void *buf)
@@ -516,102 +927,141 @@
 	return regs + 1;
 }
 
-static int mal_probe(struct platform_device *ofdev)
-{
+static int mal_probe(struct platform_device *ofdev) {
 	struct mal_instance *mal;
-	int err = 0, i, bd_size;
-	int index = mal_count++;
+	int err = 0, i, bd_size, index = mal_count++;
 	unsigned int dcr_base;
 	const u32 *prop;
+	const char *str_prop;
 	u32 cfg;
 	unsigned long irqflags;
 	irq_handler_t hdlr_serr, hdlr_txde, hdlr_rxde;
+	struct device *dp = &ofdev->dev;
+	struct device_node *np = dp->of_node;
+
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	int num_phys_chans, coal_intr_index;
+#endif
 
-	mal = kzalloc(sizeof(struct mal_instance), GFP_KERNEL);
+	str_prop = of_get_property(np, "descriptor-memory", NULL);
+	if (str_prop && (!strcmp(str_prop,"ocm") || !strcmp(str_prop,"OCM"))) {
+		phys_addr_t	bd_phys;
+		printk(KERN_INFO "MAL: Allocating MAL instance on OCM\n");
+		mal = ppc4xx_ocm_alloc(&bd_phys, sizeof(struct mal_instance), 4, PPC4XX_OCM_NON_CACHED, "mal_instance");
+		memset(mal, 0, sizeof(struct mal_instance));
+		mal->desc_memory = MAL_DESC_MEM_OCM;
+	} else mal = kzalloc(sizeof(struct mal_instance), GFP_KERNEL);
 	if (!mal)
 		return -ENOMEM;
 
 	mal->index = index;
 	mal->ofdev = ofdev;
-	mal->version = of_device_is_compatible(ofdev->dev.of_node, "ibm,mcmal2") ? 2 : 1;
+	mal->version = of_device_is_compatible(np, "ibm,mcmal2") ? 2 : 1;
 
 	MAL_DBG(mal, "probe" NL);
 
-	prop = of_get_property(ofdev->dev.of_node, "num-tx-chans", NULL);
+	prop = of_get_property(np, "num-tx-chans", NULL);
 	if (prop == NULL) {
-		printk(KERN_ERR
-		       "mal%d: can't find MAL num-tx-chans property!\n",
-		       index);
+		printk(KERN_ERR "mal%d: can't find MAL num-tx-chans property!\n", index);
 		err = -ENODEV;
 		goto fail;
 	}
 	mal->num_tx_chans = prop[0];
 
-	prop = of_get_property(ofdev->dev.of_node, "num-rx-chans", NULL);
+	prop = of_get_property(np, "num-rx-chans", NULL);
 	if (prop == NULL) {
-		printk(KERN_ERR
-		       "mal%d: can't find MAL num-rx-chans property!\n",
-		       index);
+		printk(KERN_ERR "mal%d: can't find MAL num-rx-chans property!\n", index);
 		err = -ENODEV;
 		goto fail;
 	}
 	mal->num_rx_chans = prop[0];
 
-	dcr_base = dcr_resource_start(ofdev->dev.of_node, 0);
+	dcr_base = dcr_resource_start(np, 0);
 	if (dcr_base == 0) {
-		printk(KERN_ERR
-		       "mal%d: can't find DCR resource!\n", index);
+		printk(KERN_ERR "mal%d: can't find DCR resource!\n", index);
 		err = -ENODEV;
 		goto fail;
 	}
-	mal->dcr_host = dcr_map(ofdev->dev.of_node, dcr_base, 0x100);
+	mal->dcr_host = dcr_map(np, dcr_base, 0x100);
 	if (!DCR_MAP_OK(mal->dcr_host)) {
-		printk(KERN_ERR
-		       "mal%d: failed to map DCRs !\n", index);
+		printk(KERN_ERR "mal%d: failed to map DCRs !\n", index);
 		err = -ENODEV;
 		goto fail;
 	}
 
-	if (of_device_is_compatible(ofdev->dev.of_node, "ibm,mcmal-405ez")) {
-#if defined(CONFIG_IBM_EMAC_MAL_CLR_ICINTSTAT) && \
-		defined(CONFIG_IBM_EMAC_MAL_COMMON_ERR)
-		mal->features |= (MAL_FTR_CLEAR_ICINTSTAT |
-				MAL_FTR_COMMON_ERR_INT);
+	if (of_device_is_compatible(np, "ibm,mcmal-405ez")) {
+#if defined(CONFIG_IBM_EMAC_MAL_CLR_ICINTSTAT) && defined(CONFIG_IBM_EMAC_MAL_COMMON_ERR)
+		mal->features |= (MAL_FTR_CLEAR_ICINTSTAT | MAL_FTR_COMMON_ERR_INT);
 #else
-		printk(KERN_ERR "%pOF: Support for 405EZ not enabled!\n",
-				ofdev->dev.of_node);
+		printk(KERN_ERR "%pOF: Support for 405EZ not enabled!\n", np);
 		err = -ENODEV;
 		goto fail;
 #endif
 	}
 
-	mal->txeob_irq = irq_of_parse_and_map(ofdev->dev.of_node, 0);
-	mal->rxeob_irq = irq_of_parse_and_map(ofdev->dev.of_node, 1);
-	mal->serr_irq = irq_of_parse_and_map(ofdev->dev.of_node, 2);
+	mal->txeob_irq = irq_of_parse_and_map(np, 0);
+	mal->rxeob_irq = irq_of_parse_and_map(np, 1);
+	mal->serr_irq  = irq_of_parse_and_map(np, 2);
 
-	if (mal_has_feature(mal, MAL_FTR_COMMON_ERR_INT)) {
+	if (mal_has_feature(mal, MAL_FTR_COMMON_ERR_INT))
 		mal->txde_irq = mal->rxde_irq = mal->serr_irq;
-	} else {
-		mal->txde_irq = irq_of_parse_and_map(ofdev->dev.of_node, 3);
-		mal->rxde_irq = irq_of_parse_and_map(ofdev->dev.of_node, 4);
+	else {
+		mal->txde_irq = irq_of_parse_and_map(np, 3);
+		mal->rxde_irq = irq_of_parse_and_map(np, 4);
 	}
 
-	if (!mal->txeob_irq || !mal->rxeob_irq || !mal->serr_irq ||
-	    !mal->txde_irq  || !mal->rxde_irq) {
-		printk(KERN_ERR
-		       "mal%d: failed to map interrupts !\n", index);
+	if (!mal->txeob_irq|| !mal->rxeob_irq|| !mal->serr_irq|| !mal->txde_irq|| !mal->rxde_irq) {
+		printk(KERN_ERR "mal%d: failed to map interrupts !\n", index);
 		err = -ENODEV;
 		goto fail_unmap;
 	}
 
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	/* Number of Tx channels is equal to Physical channels */
+	/* Rx channels include Virtual channels so use Tx channels */
+	BUG_ON(mal->num_tx_chans > MAL_MAX_PHYS_CHANNELS);
+	num_phys_chans = mal->num_tx_chans;
+	/* Older revs in 460EX and 460GT have coalesce bug in h/w */
+#if (defined(CONFIG_460EX) || defined(CONFIG_460GT)) && !defined(CONFIG_APM821xx)
+	{	unsigned int pvr = mfspr(SPRN_PVR);
+		unsigned short min = PVR_MIN(pvr);
+		if (min < 4) {
+			printk(KERN_INFO "PVR %x Intr Coal disabled: H/W bug\n", pvr);
+			mal->coalesce_disabled = 1;
+		}
+	}
+#else
+	mal->coalesce_disabled = 0;
+#endif
+	coal_intr_index = 5;
+	/* If device tree doesn't Interrupt coal IRQ, fall back to EOB IRQ */
+	for (i = 0; (i < num_phys_chans) && (mal->coalesce_disabled == 0) ; i++) {
+		mal->txcoal_irq[i] = irq_of_parse_and_map(np, coal_intr_index++);
+		if (mal->txcoal_irq[i] == NO_IRQ) {
+			printk(KERN_INFO "MAL: No device tree IRQ for TxCoal%d  - disabling coalescing\n", i);
+			mal->coalesce_disabled = 1;
+		}
+	}
+	for (i = 0; (i < num_phys_chans) && (mal->coalesce_disabled == 0); i++) {
+		mal->rxcoal_irq[i] = irq_of_parse_and_map(np, coal_intr_index++);
+		if (mal->rxcoal_irq[i] == NO_IRQ) {
+			printk(KERN_INFO "MAL: No device tree IRQ for RxCoal%d  - disabling coalescing\n", i);
+			mal->coalesce_disabled = 1;
+		}
+	}
+#endif
+
+#if defined (CONFIG_APM821xx) // APM821xx has just a single EMAC/MAL RX/TX channel
+    mal->commac = NULL;
+#else
 	INIT_LIST_HEAD(&mal->poll_list);
 	INIT_LIST_HEAD(&mal->list);
+#endif
 	spin_lock_init(&mal->lock);
 
 	init_dummy_netdev(&mal->dummy_dev);
 
-	netif_napi_add(&mal->dummy_dev, &mal->napi, mal_poll,
-		       CONFIG_IBM_EMAC_POLL_WEIGHT);
+	netif_napi_add(&mal->dummy_dev, &mal->napi, mal_poll, CONFIG_IBM_EMAC_POLL_WEIGHT);
 
 	/* Load power-on reset defaults */
 	mal_reset(mal);
@@ -620,38 +1070,55 @@
 	cfg = (mal->version == 2) ? MAL2_CFG_DEFAULT : MAL1_CFG_DEFAULT;
 	cfg |= MAL_CFG_PLBB | MAL_CFG_OPBBL | MAL_CFG_LEA;
 
-	/* Current Axon is not happy with priority being non-0, it can
-	 * deadlock, fix it up here
-	 */
-	if (of_device_is_compatible(ofdev->dev.of_node, "ibm,mcmal-axon"))
+	/* Current Axon is not happy with priority being non-0, it can deadlock, fix it up here */
+	if (of_device_is_compatible(np, "ibm,mcmal-axon"))
 		cfg &= ~(MAL2_CFG_RPP_10 | MAL2_CFG_WPP_10);
 
 	/* Apply configuration */
 	set_mal_dcrn(mal, MAL_CFG, cfg);
+    mal->mal_cfg = cfg; // Cache mal configuration
 
 	/* Allocate space for BD rings */
 	BUG_ON(mal->num_tx_chans <= 0 || mal->num_tx_chans > 32);
 	BUG_ON(mal->num_rx_chans <= 0 || mal->num_rx_chans > 32);
 
-	bd_size = sizeof(struct mal_descriptor) *
-		(NUM_TX_BUFF * mal->num_tx_chans +
-		 NUM_RX_BUFF * mal->num_rx_chans);
-	mal->bd_virt = dma_zalloc_coherent(&ofdev->dev, bd_size, &mal->bd_dma,
-					   GFP_KERNEL);
+	bd_size = sizeof(struct mal_descriptor) * 
+		(NUM_TX_BUFF * mal->num_tx_chans + NUM_RX_BUFF * mal->num_rx_chans);
+	
+	if (mal->desc_memory == MAL_DESC_MEM_OCM) {
+		mal->bd_virt = ppc4xx_ocm_alloc(&mal->bd_phys, bd_size, 4, PPC4XX_OCM_NON_CACHED, "mal_descriptors");
+		mal->bd_dma  = (u32)mal->bd_phys;
+	}
+
 	if (mal->bd_virt == NULL) {
+		// Allocate BD on SDRAM in case !MAL_DESC_MEM_OCM or failed OCM alloc
+		if (mal->desc_memory == MAL_DESC_MEM_OCM){
+			printk(KERN_INFO "mal%d: failed OCM alloc, descriptor-memory = SDRAM\n", index);
+			mal->desc_memory = MAL_DESC_MEM_SDRAM;
+		}
+		mal->bd_virt = dma_alloc_coherent(dp, bd_size, &mal->bd_dma, GFP_KERNEL |__GFP_ZERO);
+	}
+
+	if (mal->bd_virt == NULL) {
+		printk(KERN_ERR "mal%d: out of memory allocating RX/TX descriptors!\n", index);
 		err = -ENOMEM;
 		goto fail_unmap;
 	}
 
-	for (i = 0; i < mal->num_tx_chans; ++i)
-		set_mal_dcrn(mal, MAL_TXCTPR(i), mal->bd_dma +
-			     sizeof(struct mal_descriptor) *
+	//memset(mal->bd_virt, 0, bd_size);
+	for (i = 0; i < mal->num_tx_chans; ++i) {
+		if (mal->desc_memory == MAL_DESC_MEM_OCM)
+			set_mal_dcrn(mal, MAL_TXBADDR, (mal->bd_phys >> 32));
+		set_mal_dcrn(mal, MAL_TXCTPR(i), mal->bd_dma + sizeof(struct mal_descriptor) *
 			     mal_tx_bd_offset(mal, i));
+	}
 
-	for (i = 0; i < mal->num_rx_chans; ++i)
-		set_mal_dcrn(mal, MAL_RXCTPR(i), mal->bd_dma +
-			     sizeof(struct mal_descriptor) *
+	for (i = 0; i < mal->num_rx_chans; ++i) {
+		if (mal->desc_memory == MAL_DESC_MEM_OCM)
+			set_mal_dcrn(mal, MAL_RXBADDR, (u32)(mal->bd_phys >> 32));
+		set_mal_dcrn(mal, MAL_RXCTPR(i), mal->bd_dma + sizeof(struct mal_descriptor) *
 			     mal_rx_bd_offset(mal, i));
+	}
 
 	if (mal_has_feature(mal, MAL_FTR_COMMON_ERR_INT)) {
 		irqflags = IRQF_SHARED;
@@ -664,30 +1131,70 @@
 	}
 
 	err = request_irq(mal->serr_irq, hdlr_serr, irqflags, "MAL SERR", mal);
-	if (err)
-		goto fail2;
+	if (err) goto fail2;
 	err = request_irq(mal->txde_irq, hdlr_txde, irqflags, "MAL TX DE", mal);
-	if (err)
-		goto fail3;
-	err = request_irq(mal->txeob_irq, mal_txeob, 0, "MAL TX EOB", mal);
-	if (err)
-		goto fail4;
+	if (err) goto fail3;
+
 	err = request_irq(mal->rxde_irq, hdlr_rxde, irqflags, "MAL RX DE", mal);
-	if (err)
-		goto fail5;
+	if (err) goto fail4;
+
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	for (i = 0; (i < num_phys_chans) && (!mal->coalesce_disabled); i++) {
+		err = request_irq(mal->txcoal_irq[i], mal_coal_tx, 0, tx_coal_irqname[i], mal);
+		if (err) {
+			printk(KERN_INFO "MAL: TxCoal%d ReqIRQ failed - disabling coalescing\n", i);
+			mal->txcoal_irq[i] = NO_IRQ;
+			mal->coalesce_disabled = 1;
+			break;
+		}
+	}
+	for (i = 0; (i < num_phys_chans) && (!mal->coalesce_disabled); i++) {
+		err = request_irq(mal->rxcoal_irq[i], mal_coal_rx, 0, rx_coal_irqname[i], mal);
+		if (err) {
+			printk(KERN_INFO "MAL: RxCoal%d ReqIRQ failed - disabling coalescing\n", i);
+			mal->rxcoal_irq[i] = NO_IRQ;
+			mal->coalesce_disabled = 1;
+			break;
+		}
+	}
+
+	if (mal->coalesce_disabled) {	/* Fall back to EOB IRQ if coalesce not supported */
+		for (i = 0; i < num_phys_chans; i++) { 	// Clean up any IRQs allocated for Coalescing
+			if (mal->txcoal_irq[i] != NO_IRQ) free_irq(mal->txcoal_irq[i], mal);
+			if (mal->rxcoal_irq[i] != NO_IRQ) free_irq(mal->rxcoal_irq[i], mal);
+		}
+#endif
+
+	err = request_irq(mal->txeob_irq, mal_txeob, 0, "MAL TX EOB", mal);
+	if (err) goto fail5;
 	err = request_irq(mal->rxeob_irq, mal_rxeob, 0, "MAL RX EOB", mal);
-	if (err)
+	if (err) {
+		mal->rxeob_irq = NO_IRQ;
 		goto fail6;
+	}
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	}
+#endif
 
 	/* Enable all MAL SERR interrupt sources */
 	set_mal_dcrn(mal, MAL_IER, MAL_IER_EVENTS);
 
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	if (!mal->coalesce_disabled) {
+		for (i = 0; i < 4; i++) {
+			mal->coales_param[i].tx_count = CONFIG_IBM_EMAC_TX_COAL_COUNT & COAL_FRAME_MASK;
+			mal->coales_param[i].rx_count = CONFIG_IBM_EMAC_RX_COAL_COUNT & COAL_FRAME_MASK;
+			mal->coales_param[i].tx_time  = CONFIG_IBM_EMAC_TX_COAL_TIMER;
+			mal->coales_param[i].rx_time  = CONFIG_IBM_EMAC_RX_COAL_TIMER;
+		}
+		mal_enable_coal(mal);
+	}
+#endif
+
 	/* Enable EOB interrupt */
 	mal_enable_eob_irq(mal);
 
-	printk(KERN_INFO
-	       "MAL v%d %pOF, %d TX channels, %d RX channels\n",
-	       mal->version, ofdev->dev.of_node,
+	printk(KERN_INFO "MAL v%d %pOF, %d TX channels, %d RX channels\n", mal->version, np,
 	       mal->num_tx_chans, mal->num_rx_chans);
 
 	/* Advertise this instance to the rest of the world */
@@ -697,73 +1204,79 @@
 	return 0;
 
  fail6:
-	free_irq(mal->rxde_irq, mal);
- fail5:
 	free_irq(mal->txeob_irq, mal);
+ fail5:
+	free_irq(mal->rxde_irq, mal);
  fail4:
 	free_irq(mal->txde_irq, mal);
  fail3:
 	free_irq(mal->serr_irq, mal);
  fail2:
-	dma_free_coherent(&ofdev->dev, bd_size, mal->bd_virt, mal->bd_dma);
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	if (!mal->coalesce_disabled) {
+		for (i = 0; i < num_phys_chans; i++) {
+			if (mal->txcoal_irq[i] != NO_IRQ) free_irq(mal->txcoal_irq[i], mal);
+			if (mal->rxcoal_irq[i] != NO_IRQ) free_irq(mal->rxcoal_irq[i], mal);
+		}
+	}
+#endif
+	if (mal->desc_memory == MAL_DESC_MEM_OCM) ppc4xx_ocm_free(mal->bd_virt);
+	else dma_free_coherent(dp, bd_size, mal->bd_virt, mal->bd_dma);
  fail_unmap:
 	dcr_unmap(mal->dcr_host, 0x100);
  fail:
 	kfree(mal);
-
 	return err;
 }
 
-static int mal_remove(struct platform_device *ofdev)
-{
+static int mal_remove(struct platform_device *ofdev) {
 	struct mal_instance *mal = platform_get_drvdata(ofdev);
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	int i, num_phys_chans;
+#endif
 
 	MAL_DBG(mal, "remove" NL);
 
 	/* Synchronize with scheduled polling */
 	napi_disable(&mal->napi);
 
-	if (!list_empty(&mal->list))
-		/* This is *very* bad */
-		WARN(1, KERN_EMERG
-		       "mal%d: commac list is not empty on remove!\n",
-		       mal->index);
-
+#if defined (CONFIG_APM821xx) // APM821xx has just a single EMAC/MAL RX/TX channel
+    if (mal->commac) 
+#else
+	if (!list_empty(&mal->list)) /* Removing mal with active commac : *very* bad */
+#endif
+		WARN(1, KERN_EMERG "mal%d: commac list is not empty on remove!\n", mal->index);
 	free_irq(mal->serr_irq, mal);
 	free_irq(mal->txde_irq, mal);
 	free_irq(mal->txeob_irq, mal);
 	free_irq(mal->rxde_irq, mal);
 	free_irq(mal->rxeob_irq, mal);
 
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	num_phys_chans = mal->num_tx_chans;
+	if (!mal->coalesce_disabled) {
+		for (i = 0; i < num_phys_chans; i++) {
+			if (mal->txcoal_irq[i]) free_irq(mal->txcoal_irq[i], mal);
+			if (mal->rxcoal_irq[i]) free_irq(mal->rxcoal_irq[i], mal);
+		}
+	}
+#endif
 	mal_reset(mal);
 
-	dma_free_coherent(&ofdev->dev,
-			  sizeof(struct mal_descriptor) *
-			  (NUM_TX_BUFF * mal->num_tx_chans +
-			   NUM_RX_BUFF * mal->num_rx_chans), mal->bd_virt,
-			  mal->bd_dma);
+	if (mal->desc_memory == MAL_DESC_MEM_OCM) ppc4xx_ocm_free(mal->bd_virt);
+	else dma_free_coherent(&ofdev->dev, sizeof(struct mal_descriptor) *
+		(NUM_TX_BUFF * mal->num_tx_chans + NUM_RX_BUFF * mal->num_rx_chans), mal->bd_virt, mal->bd_dma);
 	kfree(mal);
-
 	return 0;
 }
 
-static const struct of_device_id mal_platform_match[] =
-{
-	{
-		.compatible	= "ibm,mcmal",
-	},
-	{
-		.compatible	= "ibm,mcmal2",
-	},
-	/* Backward compat */
-	{
-		.type		= "mcmal-dma",
-		.compatible	= "ibm,mcmal",
-	},
-	{
-		.type		= "mcmal-dma",
-		.compatible	= "ibm,mcmal2",
-	},
+static const struct of_device_id mal_platform_match[] = {
+	{	.compatible	= "ibm,mcmal",	},
+	{	.compatible	= "ibm,mcmal2",	},
+	{	.type		= "mcmal-dma",			/* Backward compat */
+		.compatible	= "ibm,mcmal",	},
+	{	.type		= "mcmal-dma",
+		.compatible	= "ibm,mcmal2",	},
 	{},
 };
 
@@ -776,12 +1289,10 @@
 	.remove = mal_remove,
 };
 
-int __init mal_init(void)
-{
+int __init mal_init(void) {
 	return platform_driver_register(&mal_of_driver);
 }
 
-void mal_exit(void)
-{
+void mal_exit(void) {
 	platform_driver_unregister(&mal_of_driver);
 }
--- a/drivers/net/ethernet/ibm/emac/mal.h	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/mal.h	2019-04-02 12:01:24.253775000 +0200
@@ -39,104 +39,103 @@
  */
 
 /* MALx DCR registers */
-#define	MAL_CFG			0x00
-#define	  MAL_CFG_SR		0x80000000
-#define   MAL_CFG_PLBB		0x00004000
-#define   MAL_CFG_OPBBL		0x00000080
-#define   MAL_CFG_EOPIE		0x00000004
-#define   MAL_CFG_LEA		0x00000002
-#define   MAL_CFG_SD		0x00000001
+#define	MAL_CFG 		0x00
+#define	MAL_CFG_SR  		0x80000000
+#define MAL_CFG_PLBB		0x00004000
+#define MAL_CFG_OPBBL		0x00000080
+#define MAL_CFG_EOPIE		0x00000004
+#define MAL_CFG_LEA	    	0x00000002
+#define MAL_CFG_SD	    	0x00000001
 
 /* MAL V1 CFG bits */
-#define   MAL1_CFG_PLBP_MASK	0x00c00000
-#define   MAL1_CFG_PLBP_10	0x00800000
-#define   MAL1_CFG_GA		0x00200000
-#define   MAL1_CFG_OA		0x00100000
-#define   MAL1_CFG_PLBLE	0x00080000
-#define   MAL1_CFG_PLBT_MASK	0x00078000
-#define   MAL1_CFG_DEFAULT	(MAL1_CFG_PLBP_10 | MAL1_CFG_PLBT_MASK)
+#define MAL1_CFG_PLBP_MASK	0x00c00000
+#define MAL1_CFG_PLBP_10	0x00800000
+#define MAL1_CFG_GA	    	0x00200000
+#define MAL1_CFG_OA	    	0x00100000
+#define MAL1_CFG_PLBLE  	0x00080000
+#define MAL1_CFG_PLBT_MASK	0x00078000
+#define MAL1_CFG_DEFAULT	(MAL1_CFG_PLBP_10 | MAL1_CFG_PLBT_MASK)
 
 /* MAL V2 CFG bits */
-#define   MAL2_CFG_RPP_MASK	0x00c00000
-#define   MAL2_CFG_RPP_10	0x00800000
-#define   MAL2_CFG_RMBS_MASK	0x00300000
-#define   MAL2_CFG_WPP_MASK	0x000c0000
-#define   MAL2_CFG_WPP_10	0x00080000
-#define   MAL2_CFG_WMBS_MASK	0x00030000
-#define   MAL2_CFG_PLBLE	0x00008000
-#define   MAL2_CFG_DEFAULT	(MAL2_CFG_RMBS_MASK | MAL2_CFG_WMBS_MASK | \
+#define MAL2_CFG_RPP_MASK	0x00c00000
+#define MAL2_CFG_RPP_10	   	0x00800000
+#define MAL2_CFG_RMBS_MASK	0x00300000
+#define MAL2_CFG_WPP_MASK	0x000c0000
+#define MAL2_CFG_WPP_10	   	0x00080000
+#define MAL2_CFG_WMBS_MASK	0x00030000
+#define MAL2_CFG_PLBLE	   	0x00008000
+#define MAL2_CFG_DEFAULT	(MAL2_CFG_RMBS_MASK | MAL2_CFG_WMBS_MASK | \
 				 MAL2_CFG_RPP_10 | MAL2_CFG_WPP_10)
 
 #define MAL_ESR			0x01
-#define   MAL_ESR_EVB		0x80000000
-#define   MAL_ESR_CIDT		0x40000000
-#define   MAL_ESR_CID_MASK	0x3e000000
-#define   MAL_ESR_CID_SHIFT	25
-#define   MAL_ESR_DE		0x00100000
-#define   MAL_ESR_OTE		0x00040000
-#define   MAL_ESR_OSE		0x00020000
-#define   MAL_ESR_PEIN		0x00010000
-#define   MAL_ESR_DEI		0x00000010
-#define   MAL_ESR_OTEI		0x00000004
-#define   MAL_ESR_OSEI		0x00000002
-#define   MAL_ESR_PBEI		0x00000001
+#define MAL_ESR_EVB		0x80000000
+#define MAL_ESR_CIDT		0x40000000
+#define MAL_ESR_CID_MASK	0x3e000000
+#define MAL_ESR_CID_SHIFT	25
+#define MAL_ESR_DE		0x00100000
+#define MAL_ESR_OTE		0x00040000
+#define MAL_ESR_OSE		0x00020000
+#define MAL_ESR_PEIN		0x00010000
+#define MAL_ESR_DEI		0x00000010
+#define MAL_ESR_OTEI		0x00000004
+#define MAL_ESR_OSEI		0x00000002
+#define MAL_ESR_PBEI		0x00000001
 
 /* MAL V1 ESR bits */
-#define   MAL1_ESR_ONE		0x00080000
-#define   MAL1_ESR_ONEI		0x00000008
+#define MAL1_ESR_ONE		0x00080000
+#define MAL1_ESR_ONEI		0x00000008
 
 /* MAL V2 ESR bits */
-#define   MAL2_ESR_PTE		0x00800000
-#define   MAL2_ESR_PRE		0x00400000
-#define   MAL2_ESR_PWE		0x00200000
-#define   MAL2_ESR_PTEI		0x00000080
-#define   MAL2_ESR_PREI		0x00000040
-#define   MAL2_ESR_PWEI		0x00000020
-
+#define MAL2_ESR_PTE		0x00800000
+#define MAL2_ESR_PRE		0x00400000
+#define MAL2_ESR_PWE		0x00200000
+#define MAL2_ESR_PTEI		0x00000080
+#define MAL2_ESR_PREI		0x00000040
+#define MAL2_ESR_PWEI		0x00000020
 
 #define MAL_IER			0x02
 /* MAL IER bits */
-#define   MAL_IER_DE		0x00000010
-#define   MAL_IER_OTE		0x00000004
-#define   MAL_IER_OE		0x00000002
-#define   MAL_IER_PE		0x00000001
+#define MAL_IER_DE		0x00000010
+#define MAL_IER_OTE		0x00000004
+#define MAL_IER_OE		0x00000002
+#define MAL_IER_PE		0x00000001
 
 /* PLB read/write/timeout errors */
-#define   MAL_IER_PTE		0x00000080
-#define   MAL_IER_PRE		0x00000040
-#define   MAL_IER_PWE		0x00000020
-
-#define   MAL_IER_SOC_EVENTS	(MAL_IER_PTE | MAL_IER_PRE | MAL_IER_PWE)
-#define   MAL_IER_EVENTS	(MAL_IER_SOC_EVENTS | MAL_IER_DE | \
-				 MAL_IER_OTE | MAL_IER_OE | MAL_IER_PE)
+#define MAL_IER_PTE		0x00000080
+#define MAL_IER_PRE		0x00000040
+#define MAL_IER_PWE		0x00000020
+
+#define MAL_IER_SOC_EVENTS	(MAL_IER_PTE | MAL_IER_PRE | MAL_IER_PWE)
+#define MAL_IER_EVENTS	    	(MAL_IER_SOC_EVENTS | MAL_IER_DE | MAL_IER_OTE | MAL_IER_OE | MAL_IER_PE)
 
 #define MAL_TXCASR		0x04
 #define MAL_TXCARR		0x05
 #define MAL_TXEOBISR		0x06
 #define MAL_TXDEIR		0x07
+#define MAL_TXBADDR		0x09
 #define MAL_RXCASR		0x10
 #define MAL_RXCARR		0x11
 #define MAL_RXEOBISR		0x12
 #define MAL_RXDEIR		0x13
+#define MAL_RXBADDR		0x15
 #define MAL_TXCTPR(n)		((n) + 0x20)
 #define MAL_RXCTPR(n)		((n) + 0x40)
 #define MAL_RCBS(n)		((n) + 0x60)
 
 /* In reality MAL can handle TX buffers up to 4095 bytes long,
- * but this isn't a good round number :) 		 --ebs
- */
+ * but this isn't a good round number :) --ebs */
 #define MAL_MAX_TX_SIZE		4080
 #define MAL_MAX_RX_SIZE		4080
 
-static inline int mal_rx_size(int len)
-{
+static inline int mal_rx_size(int len) {
 	len = (len + 0xf) & ~0xf;
 	return len > MAL_MAX_RX_SIZE ? MAL_MAX_RX_SIZE : len;
 }
 
-static inline int mal_tx_chunks(int len)
-{
-	return (len + MAL_MAX_TX_SIZE - 1) / MAL_MAX_TX_SIZE;
+/* Since MAL_MAX_TX_SIZE is close to 4K, this is a good-enough esimate */
+#define MAL_ESTIMATE_TX_CHUNKS(len) (len >> 12)
+static inline int mal_tx_chunks(int len) {
+    return DIV_ROUND_UP(len, MAL_MAX_TX_SIZE);
 }
 
 #define MAL_CHAN_MASK(n)	(0x80000000 >> (n))
@@ -155,7 +154,13 @@
 #define MAL_RX_CTRL_CM		0x2000
 #define MAL_RX_CTRL_LAST	0x1000
 #define MAL_RX_CTRL_FIRST	0x0800
+/* Optimize OR operations */ // ECO
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
 #define MAL_RX_CTRL_INTR	0x0400
+#else
+#define MAL_RX_CTRL_INTR	0x0
+#endif
+
 #define MAL_RX_CTRL_SINGLE	(MAL_RX_CTRL_LAST | MAL_RX_CTRL_FIRST)
 #define MAL_IS_SINGLE_RX(ctrl)	(((ctrl) & MAL_RX_CTRL_SINGLE) == MAL_RX_CTRL_SINGLE)
 
@@ -165,29 +170,112 @@
 #define MAL_TX_CTRL_LAST	0x1000
 #define MAL_TX_CTRL_INTR	0x0400
 
+#define MAL_DESC_MEM_SDRAM	0x0
+#define MAL_DESC_MEM_OCM	0x1
+
+#if defined(CONFIG_405EX)
+#define DCRN_SDR0_ICCRTX	0x430B /* Int coal Tx control register */
+#define DCRN_SDR0_ICCRRX	0x430C /* Int coal Rx control register */
+#define SDR0_ICC_FTHR0_SHIFT 	23
+#define SDR0_ICC_FLUSH0		22
+#define SDR0_ICC_FLUWI0		21
+#define SDR0_ICC_FTHR1_SHIFT 	12
+#define SDR0_ICC_FLUSH1		11
+#define SDR0_ICC_FLUWI1		10
+#define DCRN_SDR0_ICCTRTX0	0x430D /* Int coal Tx0 count threshold */
+#define DCRN_SDR0_ICCTRTX1	0x430E /* Int coal Tx1 count threshold */
+#define DCRN_SDR0_ICCTRRX0	0x430F /* Int coal Rx0 count threshold */
+#define DCRN_SDR0_ICCTRRX1	0x4310 /* Int coal Rx1 count threshold */
+#define DCRN_SDR0_ICTSRTX0	0x4307 /* Int coal Tx0 timer status*/
+#define DCRN_SDR0_ICTSRTX1	0x4308 /* Int coal Tx1 timer status*/
+#define DCRN_SDR0_ICTSRRX0	0x4309 /* Int coal Rx0 timer status*/
+#define DCRN_SDR0_ICTSRRX1	0x430A /* Int coal Rx1 timer status*/
+
+#elif defined(CONFIG_APM821xx)
+#define DCRN_SDR0_ICCRTX0	0x4410 /* Int coal Tx0 control register */
+#define DCRN_SDR0_ICCRRX0	0x4414 /* Int coal Rx0 control register */
+#define SDR0_ICC_FTHR_SHIFT	23
+#define SDR0_ICC_FLUSH		22
+#define SDR0_ICC_FLUWI		21
+#define DCRN_SDR0_ICCTRTX0	0x4418 /* Int coal Tx0 count threshold */
+#define DCRN_SDR0_ICCTRRX0	0x441C /* Int coal Rx0 count threshold */
+#define DCRN_SDR0_ICTSRTX0	0x4420 /* Int coal Tx0 timer status*/
+#define DCRN_SDR0_ICTSRRX0	0x4424 /* Int coal Rx0 timer status*/
+#define SDR0_PERCLK		0x4201
+
+#define DCRN_SDR0_MIRQ0		0x0260
+#define DCRN_SDR0_MIRQ1		0x0261
+#define DCRN_SDR0_MALTBL	0x0280 /* MAL Transmit Burst Length */
+#define DCRN_SDR0_MALRBL	0x02A0 /* MAL Receive Burst Length */
+#define DCRN_SDR0_MALTBS	0x02C0 /* MAL Transmit Bus Size */
+#define DCRN_SDR0_MALRBS	0x02E0 /* MAL Receive Bus Size */
+
+#elif defined(CONFIG_460EX) || defined(CONFIG_460GT) || defined(CONFIG_460SX)
+#define DCRN_SDR0_ICCRTX0	0x4410 /* Int coal Tx0 control register */
+#define DCRN_SDR0_ICCRTX1	0x4411 /* Int coal Tx1 control register */
+#define DCRN_SDR0_ICCRTX2	0x4412 /* Int coal Tx2 control register */
+#define DCRN_SDR0_ICCRTX3	0x4413 /* Int coal Tx3 control register */
+#define DCRN_SDR0_ICCRRX0	0x4414 /* Int coal Rx0 control register */
+#define DCRN_SDR0_ICCRRX1	0x4415 /* Int coal Rx1 control register */
+#define DCRN_SDR0_ICCRRX2	0x4416 /* Int coal Rx2 control register */
+#define DCRN_SDR0_ICCRRX3	0x4417 /* Int coal Rx3 control register */
+#define SDR0_ICC_FTHR_SHIFT	23
+#define SDR0_ICC_FLUSH		22
+#define SDR0_ICC_FLUWI		21
+#define DCRN_SDR0_ICCTRTX0	0x4418 /* Int coal Tx0 count threshold */
+#define DCRN_SDR0_ICCTRTX1	0x4419 /* Int coal Tx1 count threshold */
+#define DCRN_SDR0_ICCTRTX2	0x441A /* Int coal Tx2 count threshold */
+#define DCRN_SDR0_ICCTRTX3	0x441B /* Int coal Tx3 count threshold */
+#define DCRN_SDR0_ICCTRRX0	0x441C /* Int coal Rx0 count threshold */
+#define DCRN_SDR0_ICCTRRX1	0x441D /* Int coal Rx1 count threshold */
+#define DCRN_SDR0_ICCTRRX2	0x441E /* Int coal Rx2 count threshold */
+#define DCRN_SDR0_ICCTRRX3	0x441F /* Int coal Rx3 count threshold */
+#define DCRN_SDR0_ICTSRTX0	0x4420 /* Int coal Tx0 timer status*/
+#define DCRN_SDR0_ICTSRTX1	0x4421 /* Int coal Tx1 timer status*/
+#define DCRN_SDR0_ICTSRTX2	0x4422 /* Int coal Tx2 timer status*/
+#define DCRN_SDR0_ICTSRTX3	0x4423 /* Int coal Tx3 timer status*/
+#define DCRN_SDR0_ICTSRRX0	0x4424 /* Int coal Rx0 timer status*/
+#define DCRN_SDR0_ICTSRRX1	0x4425 /* Int coal Rx1 timer status*/
+#define DCRN_SDR0_ICTSRRX2	0x4426 /* Int coal Rx2 timer status*/
+#define DCRN_SDR0_ICTSRRX3	0x4427 /* Int coal Rx3 timer status*/
+#endif
+
+#define COAL_FRAME_MASK		0x1FF
+#define MAL_MAX_PHYS_CHANNELS	4
+
 struct mal_commac_ops {
 	void	(*poll_tx) (void *dev);
 	int	(*poll_rx) (void *dev, int budget);
 	int	(*peek_rx) (void *dev);
 	void	(*rxde) (void *dev);
 };
-
 struct mal_commac {
 	struct mal_commac_ops	*ops;
 	void			*dev;
+#if !defined (CONFIG_APM821xx)
 	struct list_head	poll_list;
+	struct list_head	list;
+#endif
 	long       		flags;
 #define MAL_COMMAC_RX_STOPPED		0
 #define MAL_COMMAC_POLL_DISABLED	1
 	u32			tx_chan_mask;
 	u32			rx_chan_mask;
-	struct list_head	list;
 };
 
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+struct mal_coales_param { /* Configuration parameters for the coalescing function */
+	int         tx_count;
+	int         tx_time;
+	int         rx_count;
+	int         rx_time;
+};
+#endif
+
 struct mal_instance {
 	int			version;
 	dcr_host_t		dcr_host;
-
+	int			desc_memory;	/* SDRAM or OCM */
 	int			num_tx_chans;	/* Number of TX channels */
 	int			num_rx_chans;	/* Number of RX channels */
 	int 			txeob_irq;	/* TX End Of Buffer IRQ  */
@@ -196,14 +284,40 @@
 	int			rxde_irq;	/* RX Descriptor Error IRQ */
 	int			serr_irq;	/* MAL System Error IRQ    */
 
-	struct list_head	poll_list;
+#if defined(CONFIG_IBM_EMAC_INTR_COALESCE)
+	int			txcoal0_irq;	/* COAL */
+	int			txcoal1_irq;	/* COAL */
+	int			txcoal2_irq;	/* COAL */
+	int			txcoal3_irq;	/* COAL */
+	int			rxcoal0_irq;	/* COAL */
+	int			rxcoal1_irq;	/* COAL */
+	int			rxcoal2_irq;	/* COAL */
+	int			rxcoal3_irq;	/* COAL */
+
+	struct mal_coales_param coales_param[4];
+	/* add copy of iccrtx and iccrrx registers to bypass the bug on the 440EPX pass1 
+	 * where these registers are write only */
+	u32     	enet_coales_iccrtx;
+	u32     	enet_coales_iccrrx;
+	struct timer_list mal_coal_timer;
+#endif
+
 	struct napi_struct	napi;
 
+#if defined (CONFIG_APM821xx)
+    struct mal_commac  *poll_commac;
+    struct mal_commac  *commac;
+    u32         poll_tx;
+    u32         poll_rx;
+#else
+	struct list_head	poll_list;
 	struct list_head	list;
+#endif
 	u32			tx_chan_mask;
 	u32			rx_chan_mask;
 
 	dma_addr_t		bd_dma;
+	phys_addr_t		bd_phys;
 	struct mal_descriptor	*bd_virt;
 
 	struct platform_device	*ofdev;
@@ -213,63 +327,57 @@
 	struct net_device	dummy_dev;
 
 	unsigned int features;
+	u32			net_alloc_offset;
+    u32         mal_cfg;    /* Cache mal CFG */
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+	int			txcoal_irq[MAL_MAX_PHYS_CHANNELS];	/* MAL TxCoalesce Error IRQ    */
+	int			rxcoal_irq[MAL_MAX_PHYS_CHANNELS];	/* MAL RxCoalesce IRQ    */
+	int			coalesce_disabled;	/* Coalesce disable flag    */
+#endif
 };
 
-static inline u32 get_mal_dcrn(struct mal_instance *mal, int reg)
-{
+static inline u32 get_mal_dcrn(struct mal_instance *mal, int reg) {
 	return dcr_read(mal->dcr_host, reg);
 }
 
-static inline void set_mal_dcrn(struct mal_instance *mal, int reg, u32 val)
-{
+static inline void set_mal_dcrn(struct mal_instance *mal, int reg, u32 val){
 	dcr_write(mal->dcr_host, reg, val);
 }
 
 /* Features of various MAL implementations */
 
 /* Set if you have interrupt coalescing and you have to clear the SDR
- * register for TXEOB and RXEOB interrupts to work
- */
+ * register for TXEOB and RXEOB interrupts to work */
 #define MAL_FTR_CLEAR_ICINTSTAT	0x00000001
 
-/* Set if your MAL has SERR, TXDE, and RXDE OR'd into a single UIC
- * interrupt
- */
+/* Set if your MAL has SERR, TXDE, and RXDE OR'd into a single UIC interrupt */
 #define MAL_FTR_COMMON_ERR_INT	0x00000002
 
 enum {
 	MAL_FTRS_ALWAYS = 0,
-
 	MAL_FTRS_POSSIBLE =
 #ifdef CONFIG_IBM_EMAC_MAL_CLR_ICINTSTAT
-		MAL_FTR_CLEAR_ICINTSTAT |
+	MAL_FTR_CLEAR_ICINTSTAT |
 #endif
 #ifdef CONFIG_IBM_EMAC_MAL_COMMON_ERR
-		MAL_FTR_COMMON_ERR_INT |
+	MAL_FTR_COMMON_ERR_INT |
 #endif
-		0,
+	0,
 };
 
-static inline int mal_has_feature(struct mal_instance *dev,
-		unsigned long feature)
-{
-	return (MAL_FTRS_ALWAYS & feature) ||
-		(MAL_FTRS_POSSIBLE & dev->features & feature);
+static inline int mal_has_feature(struct mal_instance *dev, unsigned long feature) {
+	return (MAL_FTRS_ALWAYS & feature) || (MAL_FTRS_POSSIBLE & dev->features & feature);
 }
 
 /* Register MAL devices */
 int mal_init(void);
 void mal_exit(void);
 
-int mal_register_commac(struct mal_instance *mal,
-			struct mal_commac *commac);
-void mal_unregister_commac(struct mal_instance *mal,
-			   struct mal_commac *commac);
+int mal_register_commac(struct mal_instance *mal, struct mal_commac *commac);
+void mal_unregister_commac(struct mal_instance *mal, struct mal_commac *commac);
 int mal_set_rcbs(struct mal_instance *mal, int channel, unsigned long size);
 
-/* Returns BD ring offset for a particular channel
-   (in 'struct mal_descriptor' elements)
-*/
+/* Returns BD ring offset for a particular channel (in 'struct mal_descriptor' elements)*/
 int mal_tx_bd_offset(struct mal_instance *mal, int channel);
 int mal_rx_bd_offset(struct mal_instance *mal, int channel);
 
@@ -280,6 +388,9 @@
 
 void mal_poll_disable(struct mal_instance *mal, struct mal_commac *commac);
 void mal_poll_enable(struct mal_instance *mal, struct mal_commac *commac);
+#ifdef CONFIG_IBM_EMAC_INTR_COALESCE
+void mal_enable_coal(struct mal_instance *mal);
+#endif
 
 /* Add/remove EMAC to/from MAL polling list */
 void mal_poll_add(struct mal_instance *mal, struct mal_commac *commac);
@@ -289,7 +400,6 @@
 struct mal_regs {
 	u32 tx_count;
 	u32 rx_count;
-
 	u32 cfg;
 	u32 esr;
 	u32 ier;
--- a/drivers/net/ethernet/ibm/emac/phy.c	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/phy.c	2019-03-11 18:19:26.000000000 +0100
@@ -24,6 +24,8 @@
 #include <linux/mii.h>
 #include <linux/ethtool.h>
 #include <linux/delay.h>
+//#include <linux/brcmphy.h>
+//#include <linux/mii.h>
 
 #include "emac.h"
 #include "phy.h"
@@ -31,66 +33,53 @@
 #define phy_read _phy_read
 #define phy_write _phy_write
 
-static inline int _phy_read(struct mii_phy *phy, int reg)
-{
+static inline int _phy_read(struct mii_phy *phy, int reg) {
 	return phy->mdio_read(phy->dev, phy->address, reg);
 }
 
-static inline void _phy_write(struct mii_phy *phy, int reg, int val)
-{
+static inline void _phy_write(struct mii_phy *phy, int reg, int val) {
 	phy->mdio_write(phy->dev, phy->address, reg, val);
 }
 
-static inline int gpcs_phy_read(struct mii_phy *phy, int reg)
-{
+static inline int gpcs_phy_read(struct mii_phy *phy, int reg) {
 	return phy->mdio_read(phy->dev, phy->gpcs_address, reg);
 }
 
-static inline void gpcs_phy_write(struct mii_phy *phy, int reg, int val)
-{
+static inline void gpcs_phy_write(struct mii_phy *phy, int reg, int val) {
 	phy->mdio_write(phy->dev, phy->gpcs_address, reg, val);
 }
 
-int emac_mii_reset_phy(struct mii_phy *phy)
-{
-	int val;
-	int limit = 10000;
+int emac_mii_reset_phy(struct mii_phy *phy) {
+	int val, limit = 10000;
 
-	val = phy_read(phy, MII_BMCR);
+	val = phy_read(phy, MII_BMCR) | BMCR_RESET;
 	val &= ~(BMCR_ISOLATE | BMCR_ANENABLE);
-	val |= BMCR_RESET;
 	phy_write(phy, MII_BMCR, val);
 
 	udelay(300);
 
 	while (--limit) {
 		val = phy_read(phy, MII_BMCR);
-		if (val >= 0 && (val & BMCR_RESET) == 0)
-			break;
+		if (val >= 0 && (val & BMCR_RESET) == 0) break;
 		udelay(10);
 	}
 	if ((val & BMCR_ISOLATE) && limit > 0)
 		phy_write(phy, MII_BMCR, val & ~BMCR_ISOLATE);
-
 	return limit <= 0;
 }
 
-int emac_mii_reset_gpcs(struct mii_phy *phy)
-{
-	int val;
-	int limit = 10000;
+int emac_mii_reset_gpcs(struct mii_phy *phy) {
+	int val, limit = 10000;
 
-	val = gpcs_phy_read(phy, MII_BMCR);
+	val = gpcs_phy_read(phy, MII_BMCR) | BMCR_RESET;
 	val &= ~(BMCR_ISOLATE | BMCR_ANENABLE);
-	val |= BMCR_RESET;
 	gpcs_phy_write(phy, MII_BMCR, val);
 
 	udelay(300);
 
 	while (--limit) {
 		val = gpcs_phy_read(phy, MII_BMCR);
-		if (val >= 0 && (val & BMCR_RESET) == 0)
-			break;
+		if (val >= 0 && (val & BMCR_RESET) == 0) break;
 		udelay(10);
 	}
 	if ((val & BMCR_ISOLATE) && limit > 0)
@@ -106,8 +95,7 @@
 	return limit <= 0;
 }
 
-static int genmii_setup_aneg(struct mii_phy *phy, u32 advertise)
-{
+static int genmii_setup_aneg(struct mii_phy *phy, u32 advertise) {
 	int ctl, adv;
 
 	phy->autoneg = AUTONEG_ENABLE;
@@ -117,8 +105,7 @@
 	phy->advertising = advertise;
 
 	ctl = phy_read(phy, MII_BMCR);
-	if (ctl < 0)
-		return ctl;
+	if (ctl < 0) return ctl;
 	ctl &= ~(BMCR_FULLDPLX | BMCR_SPEED100 | BMCR_SPEED1000 | BMCR_ANENABLE);
 
 	/* First clear the PHY */
@@ -126,47 +113,32 @@
 
 	/* Setup standard advertise */
 	adv = phy_read(phy, MII_ADVERTISE);
-	if (adv < 0)
-		return adv;
-	adv &= ~(ADVERTISE_ALL | ADVERTISE_100BASE4 | ADVERTISE_PAUSE_CAP |
-		 ADVERTISE_PAUSE_ASYM);
-	if (advertise & ADVERTISED_10baseT_Half)
-		adv |= ADVERTISE_10HALF;
-	if (advertise & ADVERTISED_10baseT_Full)
-		adv |= ADVERTISE_10FULL;
-	if (advertise & ADVERTISED_100baseT_Half)
-		adv |= ADVERTISE_100HALF;
-	if (advertise & ADVERTISED_100baseT_Full)
-		adv |= ADVERTISE_100FULL;
-	if (advertise & ADVERTISED_Pause)
-		adv |= ADVERTISE_PAUSE_CAP;
-	if (advertise & ADVERTISED_Asym_Pause)
-		adv |= ADVERTISE_PAUSE_ASYM;
+	if (adv < 0) return adv;
+	adv &= ~(ADVERTISE_ALL | ADVERTISE_100BASE4 | ADVERTISE_PAUSE_CAP | ADVERTISE_PAUSE_ASYM);
+	if (advertise & ADVERTISED_10baseT_Half)	adv |= ADVERTISE_10HALF;
+	if (advertise & ADVERTISED_10baseT_Full)	adv |= ADVERTISE_10FULL;
+	if (advertise & ADVERTISED_100baseT_Half)	adv |= ADVERTISE_100HALF;
+	if (advertise & ADVERTISED_100baseT_Full)	adv |= ADVERTISE_100FULL;
+	if (advertise & ADVERTISED_Pause)		adv |= ADVERTISE_PAUSE_CAP;
+	if (advertise & ADVERTISED_Asym_Pause)		adv |= ADVERTISE_PAUSE_ASYM;
 	phy_write(phy, MII_ADVERTISE, adv);
 
-	if (phy->features &
-	    (SUPPORTED_1000baseT_Full | SUPPORTED_1000baseT_Half)) {
+	if (phy->features & (SUPPORTED_1000baseT_Full | SUPPORTED_1000baseT_Half)) {
 		adv = phy_read(phy, MII_CTRL1000);
-		if (adv < 0)
-			return adv;
+		if (adv < 0) return adv;
 		adv &= ~(ADVERTISE_1000FULL | ADVERTISE_1000HALF);
-		if (advertise & ADVERTISED_1000baseT_Full)
-			adv |= ADVERTISE_1000FULL;
-		if (advertise & ADVERTISED_1000baseT_Half)
-			adv |= ADVERTISE_1000HALF;
+		if (advertise & ADVERTISED_1000baseT_Full)	adv |= ADVERTISE_1000FULL;
+		if (advertise & ADVERTISED_1000baseT_Half)	adv |= ADVERTISE_1000HALF;
 		phy_write(phy, MII_CTRL1000, adv);
 	}
 
 	/* Start/Restart aneg */
-	ctl = phy_read(phy, MII_BMCR);
-	ctl |= (BMCR_ANENABLE | BMCR_ANRESTART);
+	ctl = phy_read(phy, MII_BMCR) | BMCR_ANENABLE | BMCR_ANRESTART;
 	phy_write(phy, MII_BMCR, ctl);
-
 	return 0;
 }
 
-static int genmii_setup_forced(struct mii_phy *phy, int speed, int fd)
-{
+static int genmii_setup_forced(struct mii_phy *phy, int speed, int fd) {
 	int ctl;
 
 	phy->autoneg = AUTONEG_DISABLE;
@@ -175,8 +147,7 @@
 	phy->pause = phy->asym_pause = 0;
 
 	ctl = phy_read(phy, MII_BMCR);
-	if (ctl < 0)
-		return ctl;
+	if (ctl < 0) return ctl;
 	ctl &= ~(BMCR_FULLDPLX | BMCR_SPEED100 | BMCR_SPEED1000 | BMCR_ANENABLE);
 
 	/* First clear the PHY */
@@ -184,54 +155,38 @@
 
 	/* Select speed & duplex */
 	switch (speed) {
-	case SPEED_10:
-		break;
-	case SPEED_100:
-		ctl |= BMCR_SPEED100;
-		break;
-	case SPEED_1000:
-		ctl |= BMCR_SPEED1000;
-		break;
-	default:
-		return -EINVAL;
+		case SPEED_1000:	ctl |= BMCR_SPEED1000; break;
+		case SPEED_100:		ctl |= BMCR_SPEED100; break;
+		case SPEED_10:		break;
+		default:		return -EINVAL;
 	}
-	if (fd == DUPLEX_FULL)
-		ctl |= BMCR_FULLDPLX;
+	if (fd == DUPLEX_FULL) ctl |= BMCR_FULLDPLX;
 	phy_write(phy, MII_BMCR, ctl);
-
 	return 0;
 }
 
-static int genmii_poll_link(struct mii_phy *phy)
-{
+static int genmii_poll_link(struct mii_phy *phy) {
 	int status;
 
 	/* Clear latched value with dummy read */
 	phy_read(phy, MII_BMSR);
 	status = phy_read(phy, MII_BMSR);
-	if (status < 0 || (status & BMSR_LSTATUS) == 0)
-		return 0;
-	if (phy->autoneg == AUTONEG_ENABLE && !(status & BMSR_ANEGCOMPLETE))
-		return 0;
+	if (status < 0 || (status & BMSR_LSTATUS) == 0)	return 0;
+	if (phy->autoneg == AUTONEG_ENABLE && !(status & BMSR_ANEGCOMPLETE)) return 0;
 	return 1;
 }
 
-static int genmii_read_link(struct mii_phy *phy)
-{
+static int genmii_read_link(struct mii_phy *phy) {
 	if (phy->autoneg == AUTONEG_ENABLE) {
 		int glpa = 0;
 		int lpa = phy_read(phy, MII_LPA) & phy_read(phy, MII_ADVERTISE);
-		if (lpa < 0)
-			return lpa;
+		if (lpa < 0) return lpa;
 
 		if (phy->features &
 		    (SUPPORTED_1000baseT_Full | SUPPORTED_1000baseT_Half)) {
 			int adv = phy_read(phy, MII_CTRL1000);
 			glpa = phy_read(phy, MII_STAT1000);
-
-			if (glpa < 0 || adv < 0)
-				return adv;
-
+			if (glpa < 0 || adv < 0) return adv;
 			glpa &= adv << 2;
 		}
 
@@ -241,14 +196,11 @@
 
 		if (glpa & (LPA_1000FULL | LPA_1000HALF)) {
 			phy->speed = SPEED_1000;
-			if (glpa & LPA_1000FULL)
-				phy->duplex = DUPLEX_FULL;
+			if (glpa & LPA_1000FULL) phy->duplex = DUPLEX_FULL;
 		} else if (lpa & (LPA_100FULL | LPA_100HALF)) {
 			phy->speed = SPEED_100;
-			if (lpa & LPA_100FULL)
-				phy->duplex = DUPLEX_FULL;
-		} else if (lpa & LPA_10FULL)
-			phy->duplex = DUPLEX_FULL;
+			if (lpa & LPA_100FULL) phy->duplex = DUPLEX_FULL;
+		} else if (lpa & LPA_10FULL) phy->duplex = DUPLEX_FULL;
 
 		if (phy->duplex == DUPLEX_FULL) {
 			phy->pause = lpa & LPA_PAUSE_CAP ? 1 : 0;
@@ -256,19 +208,13 @@
 		}
 	} else {
 		int bmcr = phy_read(phy, MII_BMCR);
-		if (bmcr < 0)
-			return bmcr;
-
-		if (bmcr & BMCR_FULLDPLX)
-			phy->duplex = DUPLEX_FULL;
-		else
-			phy->duplex = DUPLEX_HALF;
-		if (bmcr & BMCR_SPEED1000)
-			phy->speed = SPEED_1000;
-		else if (bmcr & BMCR_SPEED100)
-			phy->speed = SPEED_100;
-		else
-			phy->speed = SPEED_10;
+		if (bmcr < 0) return bmcr;
+		if (bmcr & BMCR_FULLDPLX) phy->duplex = DUPLEX_FULL;
+		else phy->duplex = DUPLEX_HALF;
+
+		if (bmcr & BMCR_SPEED1000) phy->speed = SPEED_1000;
+		else if (bmcr & BMCR_SPEED100) phy->speed = SPEED_100;
+		else phy->speed = SPEED_10;
 
 		phy->pause = phy->asym_pause = 0;
 	}
@@ -292,51 +238,37 @@
 
 /* CIS8201 */
 #define MII_CIS8201_10BTCSR	0x16
-#define  TENBTCSR_ECHO_DISABLE	0x2000
+#define TENBTCSR_ECHO_DISABLE	0x2000
 #define MII_CIS8201_EPCR	0x17
-#define  EPCR_MODE_MASK		0x3000
-#define  EPCR_GMII_MODE		0x0000
-#define  EPCR_RGMII_MODE	0x1000
-#define  EPCR_TBI_MODE		0x2000
-#define  EPCR_RTBI_MODE		0x3000
+#define EPCR_MODE_MASK		0x3000
+#define EPCR_GMII_MODE		0x0000
+#define EPCR_RGMII_MODE	0x1000
+#define EPCR_TBI_MODE		0x2000
+#define EPCR_RTBI_MODE		0x3000
 #define MII_CIS8201_ACSR	0x1c
-#define  ACSR_PIN_PRIO_SELECT	0x0004
+#define ACSR_PIN_PRIO_SELECT	0x0004
 
-static int cis8201_init(struct mii_phy *phy)
-{
-	int epcr;
-
-	epcr = phy_read(phy, MII_CIS8201_EPCR);
-	if (epcr < 0)
-		return epcr;
+static int cis8201_init(struct mii_phy *phy) {
+	int epcr = phy_read(phy, MII_CIS8201_EPCR);
 
+	if (epcr < 0) return epcr;
 	epcr &= ~EPCR_MODE_MASK;
-
 	switch (phy->mode) {
-	case PHY_INTERFACE_MODE_TBI:
-		epcr |= EPCR_TBI_MODE;
-		break;
-	case PHY_INTERFACE_MODE_RTBI:
-		epcr |= EPCR_RTBI_MODE;
-		break;
-	case PHY_INTERFACE_MODE_GMII:
-		epcr |= EPCR_GMII_MODE;
-		break;
-	case PHY_INTERFACE_MODE_RGMII:
-	default:
-		epcr |= EPCR_RGMII_MODE;
+		case PHY_INTERFACE_MODE_TBI:	epcr |= EPCR_TBI_MODE; break;
+		case PHY_INTERFACE_MODE_RTBI:	epcr |= EPCR_RTBI_MODE;	break;
+		case PHY_INTERFACE_MODE_GMII:	epcr |= EPCR_GMII_MODE;	break;
+		case PHY_INTERFACE_MODE_RGMII:
+		default:			epcr |= EPCR_RGMII_MODE;
 	}
 
 	phy_write(phy, MII_CIS8201_EPCR, epcr);
 
 	/* MII regs override strap pins */
-	phy_write(phy, MII_CIS8201_ACSR,
-		  phy_read(phy, MII_CIS8201_ACSR) | ACSR_PIN_PRIO_SELECT);
+	phy_write(phy, MII_CIS8201_ACSR, phy_read(phy, MII_CIS8201_ACSR) | ACSR_PIN_PRIO_SELECT);
 
 	/* Disable TX_EN -> CRS echo mode, otherwise 10/HDX doesn't work */
 	phy_write(phy, MII_CIS8201_10BTCSR,
-		  phy_read(phy, MII_CIS8201_10BTCSR) | TENBTCSR_ECHO_DISABLE);
-
+		phy_read(phy, MII_CIS8201_10BTCSR) | TENBTCSR_ECHO_DISABLE);
 	return 0;
 }
 
@@ -356,15 +288,44 @@
 };
 
 static struct mii_phy_def bcm5248_phy_def = {
-
 	.phy_id		= 0x0143bc00,
 	.phy_id_mask	= 0x0ffffff0,
 	.name		= "BCM5248 10/100 SMII Ethernet",
 	.ops		= &generic_phy_ops
 };
+#ifdef CONFIG_APOLLO3G
+static int bcm54610_init(struct mii_phy *phy) {
+	int reg;
+	phy_write(phy, MII_NCONFIG, 0x2C00);	/* find out in what mode we are */
+	reg = phy_read(phy, MII_NCONFIG);
+	phy_write(phy, MII_NCONFIG, 0xAC8C);
+	phy_write(phy, MII_NCONFIG, 0x2C00);
+	//printk(KERN_INFO "%s: 0x%04x -> 0x%04x\n", __FUNCTION__, reg, phy_read(phy, MII_NCONFIG));
+
+	/* the RGMII interface is not half-duplex capable */
+	reg = phy_read(phy, MII_ADVERTISE);		// Autonegotiation advertisement register
+	phy_write(phy, MII_ADVERTISE, reg & ~(ADVERTISE_100HALF | ADVERTISE_10HALF)); //0x00a0	
+	reg = phy_read(phy, MII_CTRL1000);
+	phy_write(phy, MII_CTRL1000, reg & ~ADVERTISE_1000HALF); // 0x0100)
+	return 0;
+}
 
-static int m88e1111_init(struct mii_phy *phy)
-{
+static struct mii_phy_ops bcm54610_phy_ops = {
+	.init		= bcm54610_init,
+	.setup_aneg	= genmii_setup_aneg,
+	.setup_forced	= genmii_setup_forced,
+	.poll_link	= genmii_poll_link,
+	.read_link	= genmii_read_link
+};
+
+static struct mii_phy_def bcm54610_phy_def = {
+	.phy_id		= 0x0143BD63,
+	.phy_id_mask	= 0xffffffff,
+	.name		= "BCM54610 Gigabit Ethernet",
+	.ops		= &bcm54610_phy_ops
+};
+#endif
+static int m88e1111_init(struct mii_phy *phy) {
 	pr_debug("%s: Marvell 88E1111 Ethernet\n", __func__);
 	phy_write(phy, 0x14, 0x0ce3);
 	phy_write(phy, 0x18, 0x4101);
@@ -372,25 +333,18 @@
 	phy_write(phy, 0x04, 0x01e1);
 	phy_write(phy, 0x00, 0x9140);
 	phy_write(phy, 0x00, 0x1140);
-
 	return  0;
 }
 
-static int m88e1112_init(struct mii_phy *phy)
-{
-	/*
-	 * Marvell 88E1112 PHY needs to have the SGMII MAC
-	 * interace (page 2) properly configured to
-	 * communicate with the 460EX/GT GPCS interface.
-	 */
+static int m88e1112_init(struct mii_phy *phy) {
+	/* Marvell 88E1112 PHY needs to have the SGMII MAC interace (page 2) properly 
+	 * configured to communicate with the 460EX/GT GPCS interface */
 
 	u16 reg_short;
-
 	pr_debug("%s: Marvell 88E1112 Ethernet\n", __func__);
 
 	/* Set access to Page 2 */
 	phy_write(phy, 0x16, 0x0002);
-
 	phy_write(phy, 0x00, 0x0040); /* 1Gbps */
 	reg_short = (u16)(phy_read(phy, 0x1a));
 	reg_short |= 0x8000; /* bypass Auto-Negotiation */
@@ -403,8 +357,7 @@
 	return  0;
 }
 
-static int et1011c_init(struct mii_phy *phy)
-{
+static int et1011c_init(struct mii_phy *phy) {
 	u16 reg_short;
 
 	reg_short = (u16)(phy_read(phy, 0x16));
@@ -435,10 +388,6 @@
 	.ops		= &et1011c_phy_ops
 };
 
-
-
-
-
 static const struct mii_phy_ops m88e1111_phy_ops = {
 	.init		= m88e1111_init,
 	.setup_aneg	= genmii_setup_aneg,
@@ -448,7 +397,6 @@
 };
 
 static struct mii_phy_def m88e1111_phy_def = {
-
 	.phy_id		= 0x01410CC0,
 	.phy_id_mask	= 0x0ffffff0,
 	.name		= "Marvell 88E1111 Ethernet",
@@ -470,8 +418,7 @@
 	.ops		= &m88e1112_phy_ops,
 };
 
-static int ar8035_init(struct mii_phy *phy)
-{
+static int ar8035_init(struct mii_phy *phy){
 	phy_write(phy, 0x1d, 0x5); /* Address debug register 5 */
 	phy_write(phy, 0x1e, 0x2d47); /* Value copied from u-boot */
 	phy_write(phy, 0x1d, 0xb);    /* Address hib ctrl */
@@ -499,6 +446,9 @@
 	&et1011c_phy_def,
 	&cis8201_phy_def,
 	&bcm5248_phy_def,
+#ifdef CONFIG_APOLLO3G
+	&bcm54610_phy_def,
+#endif
 	&m88e1111_phy_def,
 	&m88e1112_phy_def,
 	&ar8035_phy_def,
@@ -506,8 +456,7 @@
 	NULL
 };
 
-int emac_mii_phy_probe(struct mii_phy *phy, int address)
-{
+int emac_mii_phy_probe(struct mii_phy *phy, int address) {
 	struct mii_phy_def *def;
 	int i;
 	u32 id;
@@ -520,17 +469,14 @@
 	phy->pause = phy->asym_pause = 0;
 
 	/* Take PHY out of isolate mode and reset it. */
-	if (emac_mii_reset_phy(phy))
-		return -ENODEV;
+	if (emac_mii_reset_phy(phy)) return -ENODEV;
 
 	/* Read ID and find matching entry */
 	id = (phy_read(phy, MII_PHYSID1) << 16) | phy_read(phy, MII_PHYSID2);
 	for (i = 0; (def = mii_phy_table[i]) != NULL; i++)
-		if ((id & def->phy_id_mask) == def->phy_id)
-			break;
+		if ((id & def->phy_id_mask) == def->phy_id)	break;
 	/* Should never be NULL (we have a generic entry), but... */
-	if (!def)
-		return -ENODEV;
+	if (!def) return -ENODEV;
 
 	phy->def = def;
 
@@ -538,29 +484,25 @@
 	phy->features = def->features;
 	if (!phy->features) {
 		u16 bmsr = phy_read(phy, MII_BMSR);
-		if (bmsr & BMSR_ANEGCAPABLE)
-			phy->features |= SUPPORTED_Autoneg;
-		if (bmsr & BMSR_10HALF)
-			phy->features |= SUPPORTED_10baseT_Half;
-		if (bmsr & BMSR_10FULL)
-			phy->features |= SUPPORTED_10baseT_Full;
-		if (bmsr & BMSR_100HALF)
-			phy->features |= SUPPORTED_100baseT_Half;
-		if (bmsr & BMSR_100FULL)
-			phy->features |= SUPPORTED_100baseT_Full;
+		if (bmsr & BMSR_ANEGCAPABLE)	phy->features |= SUPPORTED_Autoneg;
+		if (bmsr & BMSR_10HALF)		phy->features |= SUPPORTED_10baseT_Half;
+		if (bmsr & BMSR_10FULL)		phy->features |= SUPPORTED_10baseT_Full;
+		if (bmsr & BMSR_100HALF)	phy->features |= SUPPORTED_100baseT_Half;
+		if (bmsr & BMSR_100FULL)	phy->features |= SUPPORTED_100baseT_Full;
 		if (bmsr & BMSR_ESTATEN) {
 			u16 esr = phy_read(phy, MII_ESTATUS);
-			if (esr & ESTATUS_1000_TFULL)
-				phy->features |= SUPPORTED_1000baseT_Full;
-			if (esr & ESTATUS_1000_THALF)
-				phy->features |= SUPPORTED_1000baseT_Half;
+			if (esr & ESTATUS_1000_TFULL)	phy->features |= SUPPORTED_1000baseT_Full;
+			if (esr & ESTATUS_1000_THALF)	phy->features |= SUPPORTED_1000baseT_Half;
 		}
 		phy->features |= SUPPORTED_MII;
 	}
 
+#if (defined CONFIG_APM821xx)  /* RGMII does not support half-duplex */
+	phy->features &= ~(SUPPORTED_1000baseT_Half | SUPPORTED_100baseT_Half | SUPPORTED_10baseT_Half);
+#endif
+
 	/* Setup default advertising */
 	phy->advertising = phy->features;
-
 	return 0;
 }
 
--- a/drivers/net/ethernet/ibm/emac/phy.h	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/phy.h	2019-03-11 18:19:26.000000000 +0100
@@ -42,8 +42,7 @@
 struct mii_phy_def {
 	u32 phy_id;		/* Concatenated ID1 << 16 | ID2 */
 	u32 phy_id_mask;	/* Significant bits */
-	u32 features;		/* Ethtool SUPPORTED_* defines or
-				   0 for autodetect */
+	u32 features;		/* Ethtool SUPPORTED_* defines or 0 for autodetect */
 	int magic_aneg;		/* Autoneg does all speed test for us */
 	const char *name;
 	const struct mii_phy_ops *ops;
@@ -53,18 +52,14 @@
 struct mii_phy {
 	struct mii_phy_def *def;
 	u32 advertising;	/* Ethtool ADVERTISED_* defines */
-	u32 features;		/* Copied from mii_phy_def.features
-				   or determined automaticaly */
+	u32 features;		/* Copied from mii_phy_def.features or determined automaticaly */
 	int address;		/* PHY address */
 	int mode;		/* PHY mode */
 	int gpcs_address;	/* GPCS PHY address */
 
-	/* 1: autoneg enabled, 0: disabled */
-	int autoneg;
+	int autoneg;		/* 1: autoneg enabled, 0: disabled */
 
-	/* forced speed & duplex (no autoneg)
-	 * partner speed & duplex & pause (autoneg)
-	 */
+	/* forced speed & duplex (no autoneg) partner speed & duplex & pause (autoneg) */
 	int speed;
 	int duplex;
 	int pause;
@@ -73,13 +68,11 @@
 	/* Provided by host chip */
 	struct net_device *dev;
 	int (*mdio_read) (struct net_device * dev, int addr, int reg);
-	void (*mdio_write) (struct net_device * dev, int addr, int reg,
-			    int val);
+	void (*mdio_write) (struct net_device * dev, int addr, int reg, int val);
 };
 
 /* Pass in a struct mii_phy with dev, mdio_read and mdio_write
- * filled, the remaining fields will be filled on return
- */
+ * filled, the remaining fields will be filled on return */
 int emac_mii_phy_probe(struct mii_phy *phy, int address);
 int emac_mii_reset_phy(struct mii_phy *phy);
 int emac_mii_reset_gpcs(struct mii_phy *phy);
--- a/drivers/net/ethernet/ibm/emac/rgmii.c	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/rgmii.c	2019-03-11 18:19:26.000000000 +0100
@@ -50,8 +50,7 @@
 #define RGMII_SSR_1000(idx)	(0x4 << ((idx) * 8))
 
 /* RGMII bridge supports only GMII/TBI and RGMII/RTBI PHYs */
-static inline int rgmii_valid_mode(int phy_mode)
-{
+static inline int rgmii_valid_mode(int phy_mode) {
 	return  phy_interface_mode_is_rgmii(phy_mode) ||
 		phy_mode == PHY_INTERFACE_MODE_GMII ||
 		phy_mode == PHY_INTERFACE_MODE_MII ||
@@ -59,166 +58,117 @@
 		phy_mode == PHY_INTERFACE_MODE_RTBI;
 }
 
-static inline u32 rgmii_mode_mask(int mode, int input)
-{
+static inline u32 rgmii_mode_mask(int mode, int input) {
 	switch (mode) {
 	case PHY_INTERFACE_MODE_RGMII:
 	case PHY_INTERFACE_MODE_RGMII_ID:
 	case PHY_INTERFACE_MODE_RGMII_RXID:
-	case PHY_INTERFACE_MODE_RGMII_TXID:
-		return RGMII_FER_RGMII(input);
-	case PHY_INTERFACE_MODE_TBI:
-		return RGMII_FER_TBI(input);
-	case PHY_INTERFACE_MODE_GMII:
-		return RGMII_FER_GMII(input);
-	case PHY_INTERFACE_MODE_MII:
-		return RGMII_FER_MII(input);
-	case PHY_INTERFACE_MODE_RTBI:
-		return RGMII_FER_RTBI(input);
-	default:
-		BUG();
+	case PHY_INTERFACE_MODE_RGMII_TXID:	return RGMII_FER_RGMII(input);
+	case PHY_INTERFACE_MODE_TBI:		return RGMII_FER_TBI(input);
+	case PHY_INTERFACE_MODE_GMII:		return RGMII_FER_GMII(input);
+	case PHY_INTERFACE_MODE_MII:		return RGMII_FER_MII(input);
+	case PHY_INTERFACE_MODE_RTBI:		return RGMII_FER_RTBI(input);
+	default:				BUG();
 	}
 }
 
-int rgmii_attach(struct platform_device *ofdev, int input, int mode)
-{
+int rgmii_attach(struct platform_device *ofdev, int input, int mode) {
 	struct rgmii_instance *dev = platform_get_drvdata(ofdev);
 	struct rgmii_regs __iomem *p = dev->base;
 
 	RGMII_DBG(dev, "attach(%d)" NL, input);
-
 	/* Check if we need to attach to a RGMII */
 	if (input < 0 || !rgmii_valid_mode(mode)) {
-		printk(KERN_ERR "%pOF: unsupported settings !\n",
-		       ofdev->dev.of_node);
+		printk(KERN_ERR "%pOF: unsupported settings !\n", ofdev->dev.of_node);
 		return -ENODEV;
 	}
-
 	mutex_lock(&dev->lock);
-
 	/* Enable this input */
 	out_be32(&p->fer, in_be32(&p->fer) | rgmii_mode_mask(mode, input));
-
-	printk(KERN_NOTICE "%pOF: input %d in %s mode\n",
-	       ofdev->dev.of_node, input, phy_modes(mode));
-
+	printk(KERN_NOTICE "%pOF: input %d in %s mode\n", ofdev->dev.of_node, input, phy_modes(mode));
 	++dev->users;
-
 	mutex_unlock(&dev->lock);
-
 	return 0;
 }
 
-void rgmii_set_speed(struct platform_device *ofdev, int input, int speed)
-{
+void rgmii_set_speed(struct platform_device *ofdev, int input, int speed) {
 	struct rgmii_instance *dev = platform_get_drvdata(ofdev);
 	struct rgmii_regs __iomem *p = dev->base;
 	u32 ssr;
-
 	mutex_lock(&dev->lock);
-
 	ssr = in_be32(&p->ssr) & ~RGMII_SSR_MASK(input);
-
 	RGMII_DBG(dev, "speed(%d, %d)" NL, input, speed);
-
-	if (speed == SPEED_1000)
-		ssr |= RGMII_SSR_1000(input);
-	else if (speed == SPEED_100)
-		ssr |= RGMII_SSR_100(input);
-	else if (speed == SPEED_10)
-		ssr |= RGMII_SSR_10(input);
-
+	if (speed == SPEED_1000) 	ssr |= RGMII_SSR_1000(input);
+	else if (speed == SPEED_100) 	ssr |= RGMII_SSR_100(input);
+	else if (speed == SPEED_10) 	ssr |= RGMII_SSR_10(input);
 	out_be32(&p->ssr, ssr);
-
 	mutex_unlock(&dev->lock);
 }
 
-void rgmii_get_mdio(struct platform_device *ofdev, int input)
-{
+void rgmii_get_mdio(struct platform_device *ofdev, int input) {
 	struct rgmii_instance *dev = platform_get_drvdata(ofdev);
 	struct rgmii_regs __iomem *p = dev->base;
 	u32 fer;
 
 	RGMII_DBG2(dev, "get_mdio(%d)" NL, input);
-
-	if (!(dev->flags & EMAC_RGMII_FLAG_HAS_MDIO))
-		return;
-
+	if (!(dev->flags & EMAC_RGMII_FLAG_HAS_MDIO)) return;
 	mutex_lock(&dev->lock);
-
-	fer = in_be32(&p->fer);
-	fer |= 0x00080000u >> input;
+	fer = in_be32(&p->fer) | (0x00080000u >> input);
 	out_be32(&p->fer, fer);
 	(void)in_be32(&p->fer);
-
 	DBG2(dev, " fer = 0x%08x\n", fer);
 }
 
-void rgmii_put_mdio(struct platform_device *ofdev, int input)
-{
+void rgmii_put_mdio(struct platform_device *ofdev, int input) {
 	struct rgmii_instance *dev = platform_get_drvdata(ofdev);
 	struct rgmii_regs __iomem *p = dev->base;
 	u32 fer;
 
 	RGMII_DBG2(dev, "put_mdio(%d)" NL, input);
 
-	if (!(dev->flags & EMAC_RGMII_FLAG_HAS_MDIO))
-		return;
+	if (!(dev->flags & EMAC_RGMII_FLAG_HAS_MDIO)) return;
 
-	fer = in_be32(&p->fer);
-	fer &= ~(0x00080000u >> input);
+	fer = in_be32(&p->fer) & ~(0x00080000u >> input);
 	out_be32(&p->fer, fer);
 	(void)in_be32(&p->fer);
 
 	DBG2(dev, " fer = 0x%08x\n", fer);
-
 	mutex_unlock(&dev->lock);
 }
 
-void rgmii_detach(struct platform_device *ofdev, int input)
-{
+void rgmii_detach(struct platform_device *ofdev, int input) {
 	struct rgmii_instance *dev = platform_get_drvdata(ofdev);
 	struct rgmii_regs __iomem *p;
 
 	BUG_ON(!dev || dev->users == 0);
 	p = dev->base;
-
 	mutex_lock(&dev->lock);
-
 	RGMII_DBG(dev, "detach(%d)" NL, input);
-
 	/* Disable this input */
 	out_be32(&p->fer, in_be32(&p->fer) & ~RGMII_FER_MASK(input));
-
 	--dev->users;
-
 	mutex_unlock(&dev->lock);
 }
 
-int rgmii_get_regs_len(struct platform_device *ofdev)
-{
-	return sizeof(struct emac_ethtool_regs_subhdr) +
-		sizeof(struct rgmii_regs);
+int rgmii_get_regs_len(struct platform_device *ofdev) {
+	return sizeof(struct emac_ethtool_regs_subhdr) + sizeof(struct rgmii_regs);
 }
 
-void *rgmii_dump_regs(struct platform_device *ofdev, void *buf)
-{
+void *rgmii_dump_regs(struct platform_device *ofdev, void *buf) {
 	struct rgmii_instance *dev = platform_get_drvdata(ofdev);
 	struct emac_ethtool_regs_subhdr *hdr = buf;
 	struct rgmii_regs *regs = (struct rgmii_regs *)(hdr + 1);
 
 	hdr->version = 0;
-	hdr->index = 0; /* for now, are there chips with more than one
-			 * rgmii ? if yes, then we'll add a cell_index
-			 * like we do for emac
-			 */
+	hdr->index = 0; 
+	/* for now, are there chips with more than one rgmii ? if yes, then we'll add a cell_index
+	 * like we do for emac */
 	memcpy_fromio(regs, dev->base, sizeof(struct rgmii_regs));
 	return regs + 1;
 }
 
 
-static int rgmii_probe(struct platform_device *ofdev)
-{
+static int rgmii_probe(struct platform_device *ofdev) {
 	struct device_node *np = ofdev->dev.of_node;
 	struct rgmii_instance *dev;
 	struct resource regs;
@@ -226,9 +176,7 @@
 
 	rc = -ENOMEM;
 	dev = kzalloc(sizeof(struct rgmii_instance), GFP_KERNEL);
-	if (dev == NULL)
-		goto err_gone;
-
+	if (dev == NULL) goto err_gone;
 	mutex_init(&dev->lock);
 	dev->ofdev = ofdev;
 
@@ -239,35 +187,27 @@
 	}
 
 	rc = -ENOMEM;
-	dev->base = (struct rgmii_regs __iomem *)ioremap(regs.start,
-						 sizeof(struct rgmii_regs));
+	dev->base = (struct rgmii_regs __iomem *)ioremap(regs.start, sizeof(struct rgmii_regs));
 	if (dev->base == NULL) {
 		printk(KERN_ERR "%pOF: Can't map device registers!\n", np);
 		goto err_free;
 	}
 
 	/* Check for RGMII flags */
-	if (of_get_property(ofdev->dev.of_node, "has-mdio", NULL))
-		dev->flags |= EMAC_RGMII_FLAG_HAS_MDIO;
+	if (of_get_property(np, "has-mdio", NULL)) dev->flags |= EMAC_RGMII_FLAG_HAS_MDIO;
 
 	/* CAB lacks the right properties, fix this up */
-	if (of_device_is_compatible(ofdev->dev.of_node, "ibm,rgmii-axon"))
-		dev->flags |= EMAC_RGMII_FLAG_HAS_MDIO;
-
-	DBG2(dev, " Boot FER = 0x%08x, SSR = 0x%08x\n",
-	     in_be32(&dev->base->fer), in_be32(&dev->base->ssr));
+	if (of_device_is_compatible(np, "ibm,rgmii-axon")) dev->flags |= EMAC_RGMII_FLAG_HAS_MDIO;
+	DBG2(dev, " Boot FER = 0x%08x, SSR = 0x%08x\n", in_be32(&dev->base->fer), in_be32(&dev->base->ssr));
 
 	/* Disable all inputs by default */
 	out_be32(&dev->base->fer, 0);
 
-	printk(KERN_INFO
-	       "RGMII %pOF initialized with%s MDIO support\n",
-	       ofdev->dev.of_node,
+	printk(KERN_INFO "RGMII %pOF initialized with%s MDIO support\n", np,
 	       (dev->flags & EMAC_RGMII_FLAG_HAS_MDIO) ? "" : "out");
 
 	wmb();
 	platform_set_drvdata(ofdev, dev);
-
 	return 0;
 
  err_free:
@@ -276,26 +216,18 @@
 	return rc;
 }
 
-static int rgmii_remove(struct platform_device *ofdev)
-{
+static int rgmii_remove(struct platform_device *ofdev) {
 	struct rgmii_instance *dev = platform_get_drvdata(ofdev);
 
 	WARN_ON(dev->users != 0);
-
 	iounmap(dev->base);
 	kfree(dev);
-
 	return 0;
 }
 
-static const struct of_device_id rgmii_match[] =
-{
-	{
-		.compatible	= "ibm,rgmii",
-	},
-	{
-		.type		= "emac-rgmii",
-	},
+static const struct of_device_id rgmii_match[] = {
+	{	.compatible	= "ibm,rgmii",  },
+	{	.type		= "emac-rgmii",	},
 	{},
 };
 
@@ -308,12 +240,10 @@
 	.remove = rgmii_remove,
 };
 
-int __init rgmii_init(void)
-{
+int __init rgmii_init(void) {
 	return platform_driver_register(&rgmii_driver);
 }
 
-void rgmii_exit(void)
-{
+void rgmii_exit(void) {
 	platform_driver_unregister(&rgmii_driver);
 }
--- a/drivers/net/ethernet/ibm/emac/rgmii.h	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/rgmii.h	2019-03-11 18:19:26.000000000 +0100
@@ -77,6 +77,6 @@
 # define rgmii_set_speed(x,y,z)	do { } while(0)
 # define rgmii_get_regs_len(x)	0
 # define rgmii_dump_regs(x,buf)	(buf)
-#endif				/* !CONFIG_IBM_EMAC_RGMII */
+#endif	/* !CONFIG_IBM_EMAC_RGMII */
 
-#endif /* __IBM_NEWEMAC_RGMII_H */
+#endif	/* __IBM_NEWEMAC_RGMII_H */
--- a/drivers/net/ethernet/ibm/emac/tah.c	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/tah.c	2019-03-21 19:51:48.000000000 +0100
@@ -24,71 +24,87 @@
 #include "emac.h"
 #include "core.h"
 
-int tah_attach(struct platform_device *ofdev, int channel)
-{
+int tah_attach(struct platform_device *ofdev, int channel) {
 	struct tah_instance *dev = platform_get_drvdata(ofdev);
 
 	mutex_lock(&dev->lock);
 	/* Reset has been done at probe() time... nothing else to do for now */
 	++dev->users;
 	mutex_unlock(&dev->lock);
-
 	return 0;
 }
 
-void tah_detach(struct platform_device *ofdev, int channel)
-{
+void tah_detach(struct platform_device *ofdev, int channel) {
 	struct tah_instance *dev = platform_get_drvdata(ofdev);
-
 	mutex_lock(&dev->lock);
 	--dev->users;
 	mutex_unlock(&dev->lock);
 }
 
-void tah_reset(struct platform_device *ofdev)
-{
+void tah_reset(struct platform_device *ofdev) {
 	struct tah_instance *dev = platform_get_drvdata(ofdev);
 	struct tah_regs __iomem *p = dev->base;
 	int n;
+	u32 ssr_tmp[] = TAH_SS_DEFAULT;
 
 	/* Reset TAH */
 	out_be32(&p->mr, TAH_MR_SR);
 	n = 100;
-	while ((in_be32(&p->mr) & TAH_MR_SR) && n)
-		--n;
+	while ((in_be32(&p->mr) & TAH_MR_SR) && n) --n;
 
 	if (unlikely(!n))
 		printk(KERN_ERR "%pOF: reset timeout\n", ofdev->dev.of_node);
 
-	/* 10KB TAH TX FIFO accommodates the max MTU of 9000 */
-	out_be32(&p->mr,
-		 TAH_MR_CVR | TAH_MR_ST_768 | TAH_MR_TFS_10KB | TAH_MR_DTFP |
-		 TAH_MR_DIG);
+	/* 10KB TAH TX FIFO accommodates the max MTU of 9000 */ // ECO
+	out_be32(&p->mr, TAH_MR_CVR | TAH_MR_ST_256 | TAH_MR_TFS_10KB | TAH_MR_DTFP | TAH_MR_DIG);
+
+	/* Re-initialize SSRx values */ // (ECO)
+   	for (n = 0; n < TAH_NO_SSR; n++) tah_set_ssr(ofdev, n, ssr_tmp[n]);
 }
 
-int tah_get_regs_len(struct platform_device *ofdev)
-{
-	return sizeof(struct emac_ethtool_regs_subhdr) +
-		sizeof(struct tah_regs);
+int tah_get_regs_len(struct platform_device *ofdev) {
+	return sizeof(struct emac_ethtool_regs_subhdr) + sizeof(struct tah_regs);
 }
 
-void *tah_dump_regs(struct platform_device *ofdev, void *buf)
-{
+void *tah_dump_regs(struct platform_device *ofdev, void *buf) {
 	struct tah_instance *dev = platform_get_drvdata(ofdev);
 	struct emac_ethtool_regs_subhdr *hdr = buf;
 	struct tah_regs *regs = (struct tah_regs *)(hdr + 1);
 
 	hdr->version = 0;
-	hdr->index = 0; /* for now, are there chips with more than one
-			 * zmii ? if yes, then we'll add a cell_index
-			 * like we do for emac
-			 */
+	hdr->index = 0; 
+	/* for now, are there chips with more than one zmii ? if yes, then we'll add a cell_index
+	 * like we do for emac */
 	memcpy_fromio(regs, dev->base, sizeof(struct tah_regs));
 	return regs + 1;
 }
 
-static int tah_probe(struct platform_device *ofdev)
-{
+void tah_set_ssr(struct platform_device *ofdev, int index, int seg_size) {
+	struct tah_instance *dev = dev_get_drvdata(&ofdev->dev);
+	struct tah_regs __iomem *p = dev->base;
+
+	if ((index < 0) || (index >= TAH_NO_SSR)) return;
+	mutex_lock(&dev->lock);
+	// TAH segment size reg defines the number of half words */
+	out_be32(&p->ssr0 + index, SS_2_TAH_SSR(seg_size >> 1));
+	dev->ssr[index] = seg_size & 0x3ffe;
+
+	mutex_unlock(&dev->lock);
+}
+
+u32 tah_get_ssr(struct platform_device *ofdev, int index) {
+	struct tah_instance *dev = dev_get_drvdata(&ofdev->dev);
+	struct tah_regs __iomem *p = dev->base;
+	u32 ret = 0;
+	
+	if ((index < 0) || (index > 5)) return 0;
+    mutex_lock(&dev->lock);
+	ret = (in_be32(&p->ssr0 + index));
+	mutex_unlock(&dev->lock);
+	return ret;
+}
+
+static int tah_probe(struct platform_device *ofdev) {
 	struct device_node *np = ofdev->dev.of_node;
 	struct tah_instance *dev;
 	struct resource regs;
@@ -96,9 +112,7 @@
 
 	rc = -ENOMEM;
 	dev = kzalloc(sizeof(struct tah_instance), GFP_KERNEL);
-	if (dev == NULL)
-		goto err_gone;
-
+	if (dev == NULL) goto err_gone;
 	mutex_init(&dev->lock);
 	dev->ofdev = ofdev;
 
@@ -109,21 +123,17 @@
 	}
 
 	rc = -ENOMEM;
-	dev->base = (struct tah_regs __iomem *)ioremap(regs.start,
-					       sizeof(struct tah_regs));
+	dev->base = (struct tah_regs __iomem *)ioremap(regs.start, sizeof(struct tah_regs));
 	if (dev->base == NULL) {
 		printk(KERN_ERR "%pOF: Can't map device registers!\n", np);
 		goto err_free;
 	}
 
 	platform_set_drvdata(ofdev, dev);
-
 	/* Initialize TAH and enable IPv4 checksum verification, no TSO yet */
 	tah_reset(ofdev);
-
-	printk(KERN_INFO "TAH %pOF initialized\n", ofdev->dev.of_node);
+	printk(KERN_INFO "TAH %pOF initialized\n", np);
 	wmb();
-
 	return 0;
 
  err_free:
@@ -132,27 +142,18 @@
 	return rc;
 }
 
-static int tah_remove(struct platform_device *ofdev)
-{
+static int tah_remove(struct platform_device *ofdev) {
 	struct tah_instance *dev = platform_get_drvdata(ofdev);
-
+	//platform_set_drvdata(ofdev, NULL);	//ECO
 	WARN_ON(dev->users != 0);
-
 	iounmap(dev->base);
 	kfree(dev);
-
 	return 0;
 }
 
-static const struct of_device_id tah_match[] =
-{
-	{
-		.compatible	= "ibm,tah",
-	},
-	/* For backward compat with old DT */
-	{
-		.type		= "tah",
-	},
+static const struct of_device_id tah_match[] = {
+	{	.compatible	= "ibm,tah", },
+	{	.type		= "tah", },			/* For backward compat with old DT */
 	{},
 };
 
@@ -165,12 +166,10 @@
 	.remove = tah_remove,
 };
 
-int __init tah_init(void)
-{
+int __init tah_init(void) {
 	return platform_driver_register(&tah_driver);
 }
 
-void tah_exit(void)
-{
+void tah_exit(void) {
 	platform_driver_unregister(&tah_driver);
 }
--- a/drivers/net/ethernet/ibm/emac/tah.h	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/tah.h	2019-03-18 11:28:32.000000000 +0100
@@ -38,20 +38,25 @@
 
 
 /* TAH device */
+/* Default MTU values for common networks. Note that the first value may not correct as 
+ * we will use the device's current MTU for SSR0
+#define TAH_SS_DEFAULT_9K	{9000, 4080, 1500, 1006, 576, 68}
+#define TAH_SS_DEFAULT_4K	{4080, 1500, 1400, 1006, 576, 68}
+#define TAH_SS_DEFAULT		{1500, 1400, 1280, 1006, 576, 68}  */
+#define TAH_SS_DEFAULT_9K	{9000, 4080, 1500, 576, 256, 68}
+#define TAH_SS_DEFAULT_4K	{4080, 1500, 1400, 576, 256, 68}
+#define TAH_SS_DEFAULT		{1500, 1400, 1280, 576, 256, 68}
+#define TAH_NO_SSR	6
 struct tah_instance {
 	struct tah_regs __iomem		*base;
+	u32 			ssr[TAH_NO_SSR];	// Current setting for TAHx_SSRx
+	//u32 			ssr_order[TAH_NO_SSR];	// Indexes of ordered TAH_x_SSRx values, high to low
 
-	/* Only one EMAC whacks us at a time */
-	struct mutex			lock;
-
-	/* number of EMACs using this TAH */
-	int				users;
-
-	/* OF device instance */
-	struct platform_device		*ofdev;
+	struct mutex		lock;	// Only one EMAC whacks us at a time
+	int			users;	// number of EMACs using this TAH
+	struct platform_device	*ofdev;	// OF device instance
 };
 
-
 /* TAH engine */
 #define TAH_MR_CVR		0x80000000
 #define TAH_MR_SR		0x40000000
@@ -69,6 +74,8 @@
 #define TAH_MR_TFS_10KB		0x00a00000
 #define TAH_MR_DTFP		0x00100000
 #define TAH_MR_DIG		0x00080000
+#define TAH_SSR_2_SS(val)	(((val) >> 17) & 0x1fff)
+#define SS_2_TAH_SSR(s)		(((s) & 0x1fff) << 17) 		// s is number of half words
 
 #ifdef CONFIG_IBM_EMAC_TAH
 
@@ -79,6 +86,8 @@
 void tah_reset(struct platform_device *ofdev);
 int tah_get_regs_len(struct platform_device *ofdev);
 void *tah_dump_regs(struct platform_device *ofdev, void *buf);
+void tah_set_ssr(struct platform_device *ofdev, int index, int seg_size);
+u32 tah_get_ssr(struct platform_device *ofdev, int index);
 
 #else
 
@@ -90,6 +99,6 @@
 # define tah_get_regs_len(x)	0
 # define tah_dump_regs(x,buf)	(buf)
 
-#endif				/* !CONFIG_IBM_EMAC_TAH */
+#endif	/* !CONFIG_IBM_EMAC_TAH */
 
 #endif /* __IBM_NEWEMAC_TAH_H */
--- a/drivers/net/ethernet/ibm/emac/Kconfig	2019-04-03 06:26:31.000000000 +0200
+++ b/drivers/net/ethernet/ibm/emac/Kconfig	2019-03-18 10:14:40.000000000 +0100
@@ -28,17 +28,50 @@
 	depends on IBM_EMAC
 	default "256"
 
-config IBM_EMAC_RX_SKB_HEADROOM
-	int "Additional RX skb headroom (bytes)"
-	depends on IBM_EMAC
-	default "0"
+config IBM_EMAC_INTR_COALESCE
+	bool "Hardware Interrupt coalescing"
+	depends on IBM_EMAC && (460EX || 460GT || 405EX || 460SX || APM821xx)
+	default n
 	help
-	  Additional receive skb headroom. Note, that driver
-	  will always reserve at least 2 bytes to make IP header
-	  aligned, so usually there is no need to add any additional
-	  headroom.
+	  When selected the Ethernet interrupt coalescing is selected.
+
+config IBM_EMAC_TX_COAL_COUNT
+	int "TX Coalescence frame count (packets)"
+	depends on IBM_EMAC_INTR_COALESCE
+	default "16"
+
+config IBM_EMAC_TX_COAL_TIMER
+	int "TX Coalescence timer (clock ticks)"
+	depends on IBM_EMAC_INTR_COALESCE
+	default "1000000"
 
-	  If unsure, set to 0.
+config IBM_EMAC_RX_COAL_COUNT
+	int "RX Coalescence frame count (packets)"
+	depends on IBM_EMAC_INTR_COALESCE
+	default "1"
+
+config IBM_EMAC_RX_COAL_TIMER
+	int "RX Coalescence timer (clock ticks)"
+	depends on IBM_EMAC_INTR_COALESCE
+	default "1000000"
+	  
+config IBM_EMAC_MASK_CEXT
+	bool "Mask Carrier Extension signals"
+	depends on IBM_EMAC && APM821xx
+	default n
+	help
+	  During normal idle TX, continously send dummy packets to mask 
+	  the Carrier Extension signals. This creates a separate BD 
+	  specifically for this purpose.
+
+	  If unsure, set to N.
+
+config IBM_EMAC_SYSFS
+	bool "Sysfs support for IBM EMAC"
+	depends on IBM_EMAC
+	default n
+	help
+	  Export IBM EMAC parameters via /sys/class/net/eth* interface
 
 config IBM_EMAC_DEBUG
 	bool "Debugging"
